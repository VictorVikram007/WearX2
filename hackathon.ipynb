{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131381,"status":"ok","timestamp":1741800958578,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"},"user_tz":-330},"id":"Z57UgxMmMT9W","outputId":"e2d2d43c-16e0-4429-eca3-1b8a7363ce8f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:35:55] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.58      0.79      0.67      2030\n","           1       0.61      0.76      0.68      1997\n","           2       0.69      0.66      0.68      2021\n","           3       0.74      0.63      0.68      2032\n","           4       0.54      0.34      0.42      2022\n","           5       0.54      0.52      0.53      2087\n","\n","    accuracy                           0.62     12189\n","   macro avg       0.62      0.62      0.61     12189\n","weighted avg       0.62      0.62      0.61     12189\n","\n","Confusion Matrix:\n"," [[1613   91  106   15   20  185]\n"," [  21 1518  111  119  140   88]\n"," [ 138  180 1341  225   39   98]\n"," [  50  259  264 1272  113   74]\n"," [ 480  277   54   49  684  478]\n"," [ 468  161   73   32  270 1083]]\n","Optimized model and scaler saved successfully.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from imblearn.over_sampling import SMOTE\n","from xgboost import XGBClassifier\n","import joblib\n","\n","# Load dataset\n","df = pd.read_csv('/content/balanced_fall_detection_dataset.csv')\n","\n","df['Magnitude'] = np.sqrt(df['AccelerationX']**2 + df['AccelerationY']**2 + df['AccelerationZ']**2)\n","\n","label_encoder = LabelEncoder()\n","df['Label'] = label_encoder.fit_transform(df['Label'])\n","\n","X = df[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'Magnitude']]\n","y = df['Label']\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","smote = SMOTE(random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","param_dist = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [3, 5, 7],\n","    'learning_rate': [0.01, 0.05, 0.1],\n","    'subsample': [0.8, 1.0],\n","    'colsample_bytree': [0.8, 1.0]\n","}\n","\n","xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n","search = RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=10, scoring='f1_macro', cv=3, random_state=42, n_jobs=-1)\n","search.fit(X_train, y_train)\n","\n","best_model = search.best_estimator_\n","y_pred = best_model.predict(X_test)\n","\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","\n","joblib.dump(best_model, 'optimized_fall_detection_model.pkl')\n","joblib.dump(scaler, 'optimized_scaler.pkl')\n","\n","print(\"Optimized model and scaler saved successfully.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":330657,"status":"ok","timestamp":1741720945424,"user":{"displayName":"Vikram Balaji","userId":"13340630066862876081"},"user_tz":-330},"id":"6JvqPlTPo2Ax","outputId":"63a230a9-793c-4319-851c-cdd6c2b92da8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:22:02] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.73      0.79      0.76      2030\n","           1       0.67      0.76      0.71      1997\n","           2       0.72      0.68      0.70      2021\n","           3       0.75      0.71      0.73      2032\n","           4       0.54      0.47      0.50      2022\n","           5       0.56      0.58      0.57      2087\n","\n","    accuracy                           0.66     12189\n","   macro avg       0.66      0.66      0.66     12189\n","weighted avg       0.66      0.66      0.66     12189\n","\n","Confusion Matrix:\n"," [[1604   63   77   17   92  177]\n"," [   8 1513  102  103  175   96]\n"," [  91  121 1369  292   56   92]\n"," [  14  188  240 1442   75   73]\n"," [ 237  242   45   37  956  505]\n"," [ 233  137   56   24  431 1206]]\n","Optimized model and scaler saved successfully.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from imblearn.over_sampling import SMOTE\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","import joblib\n","\n","df = pd.read_csv('balanced_fall_detection_dataset.csv')\n","\n","df['Magnitude'] = np.sqrt(df['AccelerationX']**2 + df['AccelerationY']**2 + df['AccelerationZ']**2)\n","df['AccelerationJerk'] = np.sqrt(np.diff(df['AccelerationX'], prepend=0)**2 + np.diff(df['AccelerationY'], prepend=0)**2 + np.diff(df['AccelerationZ'], prepend=0)**2)\n","\n","label_encoder = LabelEncoder()\n","df['Label'] = label_encoder.fit_transform(df['Label'])\n","\n","X = df[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'Magnitude', 'AccelerationJerk']]\n","y = df['Label']\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","smote = SMOTE(random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","param_dist = {\n","    'n_estimators': [100, 200, 300, 500],\n","    'max_depth': [3, 5, 7, 9],\n","    'learning_rate': [0.001, 0.01, 0.05, 0.1],\n","    'subsample': [0.6, 0.8, 1.0],\n","    'colsample_bytree': [0.6, 0.8, 1.0]\n","}\n","\n","xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n","search = RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=10, scoring='f1_macro', cv=3, random_state=42, n_jobs=-1)\n","search.fit(X_train, y_train)\n","\n","best_model = search.best_estimator_\n","y_pred = best_model.predict(X_test)\n","\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","\n","joblib.dump(best_model, 'optimized_fall_detection_model.pkl')\n","joblib.dump(scaler, 'optimized_scaler.pkl')\n","\n","print(\"Optimized model and scaler saved successfully.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":616},"executionInfo":{"elapsed":5937,"status":"error","timestamp":1741882486474,"user":{"displayName":"Vikram Balaji","userId":"13340630066862876081"},"user_tz":-330},"id":"9Ezfpe4vzcXM","outputId":"1f770240-9d41-4dc1-87c5-12839ad64228"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.14.1)\n"]},{"ename":"KeyError","evalue":"\"['target'] not found in axis\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-b80528235ff3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Example:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/balanced_fall_detection_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['target'] not found in axis\""]}],"source":["!pip install optuna\n","!pip install lightgbm\n","import pandas as pd\n","import numpy as np\n","import os\n","import joblib\n","import optuna\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from imblearn.over_sampling import SMOTE\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n","from sklearn.impute import SimpleImputer\n","\n","# ✅ 1️⃣ LOAD DATA (Assuming X and y are loaded properly)\n","# Example:\n","data = pd.read_csv('/content/balanced_fall_detection_dataset.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","le = LabelEncoder()\n","y_encoded = le.fit_transform(y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y_encoded, test_size=0.15, random_state=42\n",")\n","\n","imputer = SimpleImputer(strategy='mean')\n","X_train_imputed = imputer.fit_transform(X_train)\n","X_test_imputed = imputer.transform(X_test)\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train_imputed)\n","X_test_scaled = scaler.transform(X_test_imputed)\n","\n","smote = SMOTE(random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n","\n","def objective(trial):\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 300, 800, step=100),\n","        'max_depth': trial.suggest_int('max_depth', 3, 10),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'gamma': trial.suggest_float('gamma', 0, 0.3),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 1, 6)\n","    }\n","    model = XGBClassifier(**params, random_state=42, eval_metric='mlogloss')\n","    model.fit(X_resampled, y_resampled)\n","    y_pred = model.predict(X_test_scaled)\n","    return classification_report(y_test, y_pred, output_dict=True)['accuracy']\n","\n","study = optuna.create_study(direction='maximize')\n","study.optimize(objective, n_trials=20)\n","best_params = study.best_params\n","\n","base_models = [\n","    ('rf', RandomForestClassifier(n_estimators=500, random_state=42)),\n","    ('xgb', XGBClassifier(**best_params, random_state=42, eval_metric='mlogloss')),\n","    ('lgbm', LGBMClassifier(n_estimators=500, random_state=42))\n","]\n","\n","stack_model = StackingClassifier(\n","    estimators=base_models,\n","    final_estimator=RandomForestClassifier(n_estimators=300)\n",")\n","stack_model.fit(X_resampled, y_resampled)\n","\n","y_pred = stack_model.predict(X_test_scaled)\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","\n","joblib.dump(stack_model, 'optimized_model.pkl')\n","joblib.dump(scaler, 'scaler.pkl')\n","joblib.dump(imputer, 'imputer.pkl')\n","joblib.dump(le, 'label_encoder.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":485725,"status":"ok","timestamp":1741801478768,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"},"user_tz":-330},"id":"I7yonWPTMMYI","outputId":"26216965-ce70-4404-eb47-105d35f6e27f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:44:20] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.73      0.79      0.76      2030\n","           1       0.65      0.76      0.70      1997\n","           2       0.70      0.66      0.68      2021\n","           3       0.73      0.71      0.72      2032\n","           4       0.53      0.45      0.49      2022\n","           5       0.56      0.55      0.55      2087\n","\n","    accuracy                           0.65     12189\n","   macro avg       0.65      0.65      0.65     12189\n","weighted avg       0.65      0.65      0.65     12189\n","\n","Confusion Matrix:\n"," [[1606   54   83   20  116  151]\n"," [   8 1517  114  120  148   90]\n"," [  90  130 1334  320   55   92]\n"," [  16  194  251 1446   61   64]\n"," [ 234  266   57   46  909  510]\n"," [ 241  169   66   36  426 1149]]\n","Optimized model and scaler saved successfully.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from imblearn.over_sampling import SMOTE\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.decomposition import PCA\n","import joblib\n","\n","# Load dataset\n","df = pd.read_csv('balanced_fall_detection_dataset.csv')\n","\n","# Feature engineering\n","df['Magnitude'] = np.sqrt(df['AccelerationX']**2 + df['AccelerationY']**2 + df['AccelerationZ']**2)\n","df['AccelerationJerk'] = np.sqrt(np.diff(df['AccelerationX'], prepend=0)**2 + np.diff(df['AccelerationY'], prepend=0)**2 + np.diff(df['AccelerationZ'], prepend=0)**2)\n","\n","# Encode labels\n","label_encoder = LabelEncoder()\n","df['Label'] = label_encoder.fit_transform(df['Label'])\n","\n","# Define features and target\n","X = df[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'Magnitude', 'AccelerationJerk']]\n","y = df['Label']\n","\n","# Scale features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Handle class imbalance\n","smote = SMOTE(random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# Create a pipeline with PCA for dimensionality reduction\n","pipeline = Pipeline([\n","    ('pca', PCA(n_components=0.95)),  # Retain 95% of variance\n","    ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'))\n","])\n","\n","# Hyperparameter tuning\n","param_dist = {\n","    'classifier__n_estimators': [100, 200, 300, 500],\n","    'classifier__max_depth': [3, 5, 7, 9],\n","    'classifier__learning_rate': [0.001, 0.01, 0.05, 0.1],\n","    'classifier__subsample': [0.6, 0.8, 1.0],\n","    'classifier__colsample_bytree': [0.6, 0.8, 1.0]\n","}\n","\n","search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=20, scoring='f1_macro', cv=3, random_state=42, n_jobs=-1)\n","search.fit(X_train, y_train)\n","\n","# Evaluate the model\n","best_model = search.best_estimator_\n","y_pred = best_model.predict(X_test)\n","\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","\n","# Save the model and scaler\n","joblib.dump(best_model, 'optimized_fall_detection_model.pkl')\n","joblib.dump(scaler, 'optimized_scaler.pkl')\n","\n","print(\"Optimized model and scaler saved successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"elapsed":113,"status":"error","timestamp":1741840344958,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"},"user_tz":-330},"id":"SWQdQgQ5wuCb","outputId":"da89109c-e3a2-461d-d813-59897af998dd"},"outputs":[{"ename":"NameError","evalue":"name 'y' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-4b792b74878e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# ✅ Encode labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0my_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# ✅ Train-Test Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"]}],"source":["# Install required libraries (Uncomment if not installed)\n","# !pip install optuna lightgbm catboost xgboost imbalanced-learn joblib pandas numpy scikit-learn\n","\n","import pandas as pd\n","import numpy as np\n","import optuna\n","import joblib\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler, LabelEncoder\n","from sklearn.impute import KNNImputer\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from imblearn.over_sampling import BorderlineSMOTE\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from catboost import CatBoostClassifier\n","from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","# ✅ 1️⃣ LOAD DATA (Replace with actual dataset)\n","# data = pd.read_csv('data.csv')\n","# X = data.drop('target', axis=1)\n","# y = data['target']\n","\n","# ✅ Encode labels\n","le = LabelEncoder()\n","y_encoded = le.fit_transform(y)\n","\n","# ✅ Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y_encoded, test_size=0.15, random_state=42\n",")\n","\n","# ✅ Feature Selection using XGBoost\n","xgb_temp = XGBClassifier(n_estimators=500, random_state=42)\n","xgb_temp.fit(X_train, y_train)\n","feature_importance = xgb_temp.feature_importances_\n","\n","# Keep only the top 20 most important features\n","important_features = X.columns[np.argsort(-feature_importance)[:20]]\n","X_train = X_train[important_features]\n","X_test = X_test[important_features]\n","\n","# ✅ Missing Value Handling using KNN Imputer\n","imputer = KNNImputer(n_neighbors=5)\n","X_train_imputed = imputer.fit_transform(X_train)\n","X_test_imputed = imputer.transform(X_test)\n","\n","# ✅ Robust Scaling (Better than StandardScaler)\n","scaler = RobustScaler()\n","X_train_scaled = scaler.fit_transform(X_train_imputed)\n","X_test_scaled = scaler.transform(X_test_imputed)\n","\n","# ✅ Apply PCA for Noise Reduction\n","pca = PCA(n_components=15)  # Reduce to 15 features\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","# ✅ Use Borderline-SMOTE for better data balancing\n","smote = BorderlineSMOTE(random_state=42, kind='borderline-1')\n","X_resampled, y_resampled = smote.fit_resample(X_train_pca, y_train)\n","\n","# ✅ Hyperparameter Tuning with Optuna (LGBM)\n","def lgbm_objective(trial):\n","    params = {\n","        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n","        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50)\n","    }\n","    model = LGBMClassifier(**params, random_state=42)\n","    model.fit(X_resampled, y_resampled)\n","    y_pred = model.predict(X_test_pca)\n","    return accuracy_score(y_test, y_pred)\n","\n","lgbm_study = optuna.create_study(direction='maximize')\n","lgbm_study.optimize(lgbm_objective, n_trials=300)  # Increased trials for better tuning\n","best_lgbm_params = lgbm_study.best_params\n","\n","# ✅ Hyperparameter Tuning with Optuna (XGBoost)\n","def xgb_objective(trial):\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n","        'max_depth': trial.suggest_int('max_depth', 3, 12),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'gamma': trial.suggest_float('gamma', 0, 0.3),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 1, 6)\n","    }\n","    model = XGBClassifier(**params, random_state=42, eval_metric='mlogloss')\n","    model.fit(X_resampled, y_resampled)\n","    y_pred = model.predict(X_test_pca)\n","    return accuracy_score(y_test, y_pred)\n","\n","xgb_study = optuna.create_study(direction='maximize')\n","xgb_study.optimize(xgb_objective, n_trials=300)\n","best_xgb_params = xgb_study.best_params\n","\n","# ✅ Train Final Models with Optimized Parameters\n","rf_model = RandomForestClassifier(n_estimators=500, random_state=42)\n","xgb_model = XGBClassifier(**best_xgb_params, random_state=42, eval_metric='mlogloss')\n","lgbm_model = LGBMClassifier(**best_lgbm_params, random_state=42)\n","cat_model = CatBoostClassifier(n_estimators=500, depth=8, learning_rate=0.03, verbose=0)\n","\n","# ✅ Stacking Classifier (Better than Voting Classifier)\n","meta_learner = LogisticRegression()\n","\n","stacking_clf = StackingClassifier(\n","    estimators=[\n","        ('rf', rf_model),\n","        ('xgb', xgb_model),\n","        ('lgbm', lgbm_model),\n","        ('cat', cat_model)\n","    ],\n","    final_estimator=meta_learner\n",")\n","\n","# ✅ Train Stacking Classifier\n","stacking_clf.fit(X_resampled, y_resampled)\n","\n","# ✅ Evaluate Model\n","y_pred = stacking_clf.predict(X_test_pca)\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","\n","# ✅ Save Model and Preprocessing Objects\n","joblib.dump(stacking_clf, 'final_stacking_model.pkl')\n","joblib.dump(scaler, 'scaler.pkl')\n","joblib.dump(imputer, 'imputer.pkl')\n","joblib.dump(pca, 'pca.pkl')\n","joblib.dump(le, 'label_encoder.pkl')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"m19lnKMQyK1P","outputId":"868bddff-f28d-4c0e-b090-0f1839f8d993"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2025-03-13 04:37:50,562] A new study created in memory with name: no-name-4db1d770-7bdc-4b3f-b005-421ab9a1c970\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002512 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:38:24,702] Trial 0 finished with value: 0.5818201706409976 and parameters: {'num_leaves': 87, 'learning_rate': 0.026964749121822354, 'n_estimators': 800, 'subsample': 0.731168357550096, 'colsample_bytree': 0.7727387002825166, 'min_child_samples': 25}. Best is trial 0 with value: 0.5818201706409976.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001778 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:38:30,646] Trial 1 finished with value: 0.6126668125136732 and parameters: {'num_leaves': 42, 'learning_rate': 0.07627804749240272, 'n_estimators': 200, 'subsample': 0.6118131025017626, 'colsample_bytree': 0.9830761906187663, 'min_child_samples': 38}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003134 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:39:32,214] Trial 2 finished with value: 0.5918836140888208 and parameters: {'num_leaves': 128, 'learning_rate': 0.007799731256735923, 'n_estimators': 1000, 'subsample': 0.9440002840391888, 'colsample_bytree': 0.802101785862844, 'min_child_samples': 11}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001620 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:40:13,774] Trial 3 finished with value: 0.5904616057755414 and parameters: {'num_leaves': 140, 'learning_rate': 0.010390451608247968, 'n_estimators': 700, 'subsample': 0.8437933513692112, 'colsample_bytree': 0.6975632328959656, 'min_child_samples': 24}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002818 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:40:43,895] Trial 4 finished with value: 0.5892583679719974 and parameters: {'num_leaves': 63, 'learning_rate': 0.022327231276071374, 'n_estimators': 800, 'subsample': 0.9630504122872338, 'colsample_bytree': 0.602929876938422, 'min_child_samples': 36}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001677 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:41:19,832] Trial 5 finished with value: 0.5882739006781886 and parameters: {'num_leaves': 92, 'learning_rate': 0.008676523915473544, 'n_estimators': 700, 'subsample': 0.9944921186146949, 'colsample_bytree': 0.632627178361284, 'min_child_samples': 49}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001634 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:41:50,619] Trial 6 finished with value: 0.5453948807700721 and parameters: {'num_leaves': 117, 'learning_rate': 0.09817179794656575, 'n_estimators': 700, 'subsample': 0.7612727475551307, 'colsample_bytree': 0.6417521616069725, 'min_child_samples': 23}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001703 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:42:19,431] Trial 7 finished with value: 0.5887114416976592 and parameters: {'num_leaves': 38, 'learning_rate': 0.018857990344611348, 'n_estimators': 1000, 'subsample': 0.7616047962019951, 'colsample_bytree': 0.6308716103856239, 'min_child_samples': 8}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002808 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:42:42,230] Trial 8 finished with value: 0.5875082038941151 and parameters: {'num_leaves': 83, 'learning_rate': 0.02481041252748676, 'n_estimators': 600, 'subsample': 0.9787067822264627, 'colsample_bytree': 0.6981693749929847, 'min_child_samples': 5}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001636 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:42:47,572] Trial 9 finished with value: 0.572194268212645 and parameters: {'num_leaves': 96, 'learning_rate': 0.02088116141874836, 'n_estimators': 100, 'subsample': 0.9821864140579852, 'colsample_bytree': 0.6723833880067759, 'min_child_samples': 21}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001785 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:42:52,065] Trial 10 finished with value: 0.6125574272588055 and parameters: {'num_leaves': 21, 'learning_rate': 0.0951917639152793, 'n_estimators': 200, 'subsample': 0.6054555552729707, 'colsample_bytree': 0.9982351943832333, 'min_child_samples': 39}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001729 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:42:56,632] Trial 11 finished with value: 0.6125574272588055 and parameters: {'num_leaves': 21, 'learning_rate': 0.09904673107380672, 'n_estimators': 200, 'subsample': 0.6087629131704385, 'colsample_bytree': 0.992224418171421, 'min_child_samples': 40}. Best is trial 1 with value: 0.6126668125136732.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003235 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:43:06,483] Trial 12 finished with value: 0.6139794355720849 and parameters: {'num_leaves': 46, 'learning_rate': 0.05446345315786458, 'n_estimators': 300, 'subsample': 0.6153945804432125, 'colsample_bytree': 0.9998884340272418, 'min_child_samples': 37}. Best is trial 12 with value: 0.6139794355720849.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001702 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:43:20,223] Trial 13 finished with value: 0.6126668125136732 and parameters: {'num_leaves': 55, 'learning_rate': 0.04952957752081735, 'n_estimators': 400, 'subsample': 0.6754870927186123, 'colsample_bytree': 0.9225311770430632, 'min_child_samples': 48}. Best is trial 12 with value: 0.6139794355720849.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001800 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:43:33,527] Trial 14 finished with value: 0.6123386567490702 and parameters: {'num_leaves': 53, 'learning_rate': 0.05247880864070392, 'n_estimators': 400, 'subsample': 0.6708462843295091, 'colsample_bytree': 0.9204294123409784, 'min_child_samples': 33}. Best is trial 12 with value: 0.6139794355720849.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001698 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:43:45,490] Trial 15 finished with value: 0.6141982060818202 and parameters: {'num_leaves': 40, 'learning_rate': 0.05666116991999779, 'n_estimators': 400, 'subsample': 0.6579212314877203, 'colsample_bytree': 0.9176842071760314, 'min_child_samples': 31}. Best is trial 15 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001720 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:44:01,088] Trial 16 finished with value: 0.6140888208269525 and parameters: {'num_leaves': 71, 'learning_rate': 0.0409126136802478, 'n_estimators': 400, 'subsample': 0.6770293182755988, 'colsample_bytree': 0.8988297235061387, 'min_child_samples': 31}. Best is trial 15 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003009 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:44:19,553] Trial 17 finished with value: 0.6131043535331437 and parameters: {'num_leaves': 71, 'learning_rate': 0.03614801822810898, 'n_estimators': 500, 'subsample': 0.8477713987076801, 'colsample_bytree': 0.8598709647764845, 'min_child_samples': 16}. Best is trial 15 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001786 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:44:45,764] Trial 18 finished with value: 0.6149639028658936 and parameters: {'num_leaves': 108, 'learning_rate': 0.014164908662738514, 'n_estimators': 500, 'subsample': 0.6787580416003458, 'colsample_bytree': 0.8927325204524057, 'min_child_samples': 28}. Best is trial 18 with value: 0.6149639028658936.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001746 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:45:12,653] Trial 19 finished with value: 0.6155108291402319 and parameters: {'num_leaves': 110, 'learning_rate': 0.012151591618683847, 'n_estimators': 500, 'subsample': 0.7162193413286994, 'colsample_bytree': 0.8386549939115717, 'min_child_samples': 29}. Best is trial 19 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001674 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:45:37,287] Trial 20 finished with value: 0.591227302559615 and parameters: {'num_leaves': 110, 'learning_rate': 0.013294780997751367, 'n_estimators': 500, 'subsample': 0.7166729530417317, 'colsample_bytree': 0.8199044301213413, 'min_child_samples': 19}. Best is trial 19 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001732 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:46:11,410] Trial 21 finished with value: 0.6157295996499672 and parameters: {'num_leaves': 102, 'learning_rate': 0.014387954189724373, 'n_estimators': 600, 'subsample': 0.6661902041202169, 'colsample_bytree': 0.8567691539079443, 'min_child_samples': 28}. Best is trial 21 with value: 0.6157295996499672.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001796 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:46:43,915] Trial 22 finished with value: 0.618245460511923 and parameters: {'num_leaves': 106, 'learning_rate': 0.005000284961227462, 'n_estimators': 600, 'subsample': 0.7121350003850856, 'colsample_bytree': 0.8519801618445605, 'min_child_samples': 28}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001674 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:47:16,947] Trial 23 finished with value: 0.5828046379348064 and parameters: {'num_leaves': 125, 'learning_rate': 0.005166634314861357, 'n_estimators': 600, 'subsample': 0.8083182942176325, 'colsample_bytree': 0.768103001770608, 'min_child_samples': 28}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001725 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:48:12,750] Trial 24 finished with value: 0.6175891489827171 and parameters: {'num_leaves': 147, 'learning_rate': 0.0052467101985515996, 'n_estimators': 800, 'subsample': 0.7204576149166084, 'colsample_bytree': 0.849863117704264, 'min_child_samples': 43}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001725 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:49:19,094] Trial 25 finished with value: 0.6169328374535112 and parameters: {'num_leaves': 148, 'learning_rate': 0.0051778107526840025, 'n_estimators': 900, 'subsample': 0.7608177144066207, 'colsample_bytree': 0.8580085419296168, 'min_child_samples': 44}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002847 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:50:20,994] Trial 26 finished with value: 0.5884926711879239 and parameters: {'num_leaves': 149, 'learning_rate': 0.005238793703269383, 'n_estimators': 900, 'subsample': 0.7946206726208928, 'colsample_bytree': 0.7494595114612282, 'min_child_samples': 43}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003286 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:51:24,132] Trial 27 finished with value: 0.614854517611026 and parameters: {'num_leaves': 139, 'learning_rate': 0.006625292705097622, 'n_estimators': 900, 'subsample': 0.7553335569694352, 'colsample_bytree': 0.8591289375942113, 'min_child_samples': 44}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002817 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:52:17,377] Trial 28 finished with value: 0.5892583679719974 and parameters: {'num_leaves': 150, 'learning_rate': 0.006620638524669444, 'n_estimators': 800, 'subsample': 0.8912595612922932, 'colsample_bytree': 0.8102415160310265, 'min_child_samples': 45}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001726 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:53:20,502] Trial 29 finished with value: 0.6134325092977466 and parameters: {'num_leaves': 135, 'learning_rate': 0.006434378576739683, 'n_estimators': 900, 'subsample': 0.7303558121796394, 'colsample_bytree': 0.9506597955534754, 'min_child_samples': 50}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001717 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:54:05,309] Trial 30 finished with value: 0.592430540363159 and parameters: {'num_leaves': 123, 'learning_rate': 0.009086582969967697, 'n_estimators': 800, 'subsample': 0.7952755296792468, 'colsample_bytree': 0.7689939339276506, 'min_child_samples': 42}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001701 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:54:44,718] Trial 31 finished with value: 0.6162765259243054 and parameters: {'num_leaves': 100, 'learning_rate': 0.005512962484286627, 'n_estimators': 700, 'subsample': 0.6439171724144421, 'colsample_bytree': 0.8589915956179578, 'min_child_samples': 34}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002846 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:55:18,711] Trial 32 finished with value: 0.6149639028658936 and parameters: {'num_leaves': 79, 'learning_rate': 0.0055494086818286366, 'n_estimators': 700, 'subsample': 0.6443128666786541, 'colsample_bytree': 0.8830205885038426, 'min_child_samples': 35}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001731 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:56:20,141] Trial 33 finished with value: 0.6150732881207613 and parameters: {'num_leaves': 134, 'learning_rate': 0.007019443148606725, 'n_estimators': 900, 'subsample': 0.6988210938799319, 'colsample_bytree': 0.9495496306091191, 'min_child_samples': 46}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001616 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:57:04,524] Trial 34 finished with value: 0.5900240647560708 and parameters: {'num_leaves': 118, 'learning_rate': 0.01044212265781184, 'n_estimators': 800, 'subsample': 0.7415081708210701, 'colsample_bytree': 0.8313035394869778, 'min_child_samples': 41}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001653 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:58:09,387] Trial 35 finished with value: 0.589695908991468 and parameters: {'num_leaves': 144, 'learning_rate': 0.007996039467584102, 'n_estimators': 1000, 'subsample': 0.6323464780019603, 'colsample_bytree': 0.7922145901817824, 'min_child_samples': 34}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:58:51,754] Trial 36 finished with value: 0.6174797637278495 and parameters: {'num_leaves': 131, 'learning_rate': 0.006130841931266714, 'n_estimators': 700, 'subsample': 0.7048811423575443, 'colsample_bytree': 0.876350449027674, 'min_child_samples': 25}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001731 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 04:59:32,526] Trial 37 finished with value: 0.6143075913366878 and parameters: {'num_leaves': 131, 'learning_rate': 0.009688553724296868, 'n_estimators': 700, 'subsample': 0.7001313183263825, 'colsample_bytree': 0.9576800563592078, 'min_child_samples': 14}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003022 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:00:21,326] Trial 38 finished with value: 0.5914460730693503 and parameters: {'num_leaves': 140, 'learning_rate': 0.007851287125865962, 'n_estimators': 800, 'subsample': 0.7821734845891549, 'colsample_bytree': 0.7399120830408712, 'min_child_samples': 23}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001696 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:01:21,422] Trial 39 finished with value: 0.5881645154233209 and parameters: {'num_leaves': 126, 'learning_rate': 0.0063063757881750785, 'n_estimators': 1000, 'subsample': 0.8247630511277353, 'colsample_bytree': 0.7921889117566189, 'min_child_samples': 25}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001692 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:01:51,869] Trial 40 finished with value: 0.6161671406694378 and parameters: {'num_leaves': 118, 'learning_rate': 0.017835209781729724, 'n_estimators': 600, 'subsample': 0.7737348825973579, 'colsample_bytree': 0.8724540490255981, 'min_child_samples': 19}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002865 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:02:30,260] Trial 41 finished with value: 0.6172609932181142 and parameters: {'num_leaves': 95, 'learning_rate': 0.005595425667560787, 'n_estimators': 700, 'subsample': 0.7003978365773428, 'colsample_bytree': 0.8401717730168178, 'min_child_samples': 25}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002698 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:03:09,710] Trial 42 finished with value: 0.5867425071100416 and parameters: {'num_leaves': 89, 'learning_rate': 0.0060012103535644085, 'n_estimators': 800, 'subsample': 0.6999179883160171, 'colsample_bytree': 0.8293579357992277, 'min_child_samples': 26}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001725 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:03:54,526] Trial 43 finished with value: 0.6171516079632465 and parameters: {'num_leaves': 145, 'learning_rate': 0.005013487081508033, 'n_estimators': 700, 'subsample': 0.7322269022001889, 'colsample_bytree': 0.8415045504423253, 'min_child_samples': 21}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001700 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:04:39,115] Trial 44 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 141, 'learning_rate': 0.00775456848022221, 'n_estimators': 700, 'subsample': 0.7371247266442295, 'colsample_bytree': 0.8402477026407682, 'min_child_samples': 21}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001739 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:05:09,577] Trial 45 finished with value: 0.6151826733756289 and parameters: {'num_leaves': 93, 'learning_rate': 0.007204030957048408, 'n_estimators': 600, 'subsample': 0.709735317053917, 'colsample_bytree': 0.9049666124371278, 'min_child_samples': 16}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001674 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:05:42,449] Trial 46 finished with value: 0.5846641872675563 and parameters: {'num_leaves': 84, 'learning_rate': 0.005035807682429048, 'n_estimators': 700, 'subsample': 0.7280445651023095, 'colsample_bytree': 0.7997186691979646, 'min_child_samples': 20}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001661 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:06:23,658] Trial 47 finished with value: 0.5876175891489828 and parameters: {'num_leaves': 130, 'learning_rate': 0.0059382367296731695, 'n_estimators': 700, 'subsample': 0.6852707776455214, 'colsample_bytree': 0.8124445251222917, 'min_child_samples': 23}. Best is trial 22 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001714 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:06:55,448] Trial 48 finished with value: 0.6184642310216583 and parameters: {'num_leaves': 114, 'learning_rate': 0.008621398370501722, 'n_estimators': 600, 'subsample': 0.7496354830857416, 'colsample_bytree': 0.8814664760983768, 'min_child_samples': 10}. Best is trial 48 with value: 0.6184642310216583.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002873 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:07:25,543] Trial 49 finished with value: 0.6141982060818202 and parameters: {'num_leaves': 103, 'learning_rate': 0.010346408459031866, 'n_estimators': 600, 'subsample': 0.7509816724182092, 'colsample_bytree': 0.8740570548629051, 'min_child_samples': 12}. Best is trial 48 with value: 0.6184642310216583.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002531 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:07:56,777] Trial 50 finished with value: 0.6168234521986437 and parameters: {'num_leaves': 116, 'learning_rate': 0.008343544323921535, 'n_estimators': 600, 'subsample': 0.7685762975306996, 'colsample_bytree': 0.9367370849674306, 'min_child_samples': 6}. Best is trial 48 with value: 0.6184642310216583.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:08:46,467] Trial 51 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 136, 'learning_rate': 0.005834712570226601, 'n_estimators': 800, 'subsample': 0.691427690506828, 'colsample_bytree': 0.8458826207358149, 'min_child_samples': 9}. Best is trial 51 with value: 0.6186830015313936.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001715 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:09:33,295] Trial 52 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 122, 'learning_rate': 0.0058484771467383154, 'n_estimators': 800, 'subsample': 0.6889526018471352, 'colsample_bytree': 0.8793035894176939, 'min_child_samples': 9}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001697 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:10:18,188] Trial 53 finished with value: 0.6162765259243054 and parameters: {'num_leaves': 121, 'learning_rate': 0.007215185251015256, 'n_estimators': 800, 'subsample': 0.6886176350745558, 'colsample_bytree': 0.8854215658916837, 'min_child_samples': 9}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003047 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:11:03,097] Trial 54 finished with value: 0.6176985342375848 and parameters: {'num_leaves': 112, 'learning_rate': 0.006061136149541831, 'n_estimators': 800, 'subsample': 0.7192491170202431, 'colsample_bytree': 0.920981145885121, 'min_child_samples': 9}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001730 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:11:18,992] Trial 55 finished with value: 0.618573616276526 and parameters: {'num_leaves': 112, 'learning_rate': 0.008874900411273168, 'n_estimators': 300, 'subsample': 0.6617382082576204, 'colsample_bytree': 0.9026293015472515, 'min_child_samples': 9}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001707 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:11:35,312] Trial 56 finished with value: 0.6173703784729818 and parameters: {'num_leaves': 114, 'learning_rate': 0.011654308050834422, 'n_estimators': 300, 'subsample': 0.6542728326233228, 'colsample_bytree': 0.9710190516393784, 'min_child_samples': 9}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001864 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:11:40,170] Trial 57 finished with value: 0.6166046816889084 and parameters: {'num_leaves': 104, 'learning_rate': 0.030147016244130114, 'n_estimators': 100, 'subsample': 0.6258022286272533, 'colsample_bytree': 0.9086012671237919, 'min_child_samples': 7}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:11:56,092] Trial 58 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 112, 'learning_rate': 0.00865512168403016, 'n_estimators': 300, 'subsample': 0.6614202737092661, 'colsample_bytree': 0.9299943330145453, 'min_child_samples': 12}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001836 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:12:12,309] Trial 59 finished with value: 0.6172609932181142 and parameters: {'num_leaves': 107, 'learning_rate': 0.008754377729449833, 'n_estimators': 300, 'subsample': 0.6626290286136017, 'colsample_bytree': 0.9296809719209691, 'min_child_samples': 11}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002957 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:12:21,918] Trial 60 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 99, 'learning_rate': 0.01762760196365646, 'n_estimators': 200, 'subsample': 0.6250980780085572, 'colsample_bytree': 0.894919490973094, 'min_child_samples': 5}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:12:38,597] Trial 61 finished with value: 0.6166046816889084 and parameters: {'num_leaves': 112, 'learning_rate': 0.00733377275684265, 'n_estimators': 300, 'subsample': 0.6840280136087349, 'colsample_bytree': 0.9149110006472801, 'min_child_samples': 11}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001809 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:12:58,827] Trial 62 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 107, 'learning_rate': 0.011613566507861476, 'n_estimators': 400, 'subsample': 0.6528227693893134, 'colsample_bytree': 0.942674192583958, 'min_child_samples': 13}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001733 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:13:20,975] Trial 63 finished with value: 0.6175891489827171 and parameters: {'num_leaves': 122, 'learning_rate': 0.011845753074781208, 'n_estimators': 400, 'subsample': 0.6558777694592247, 'colsample_bytree': 0.9646316212345487, 'min_child_samples': 13}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002003 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:13:41,987] Trial 64 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 106, 'learning_rate': 0.009570047607528701, 'n_estimators': 400, 'subsample': 0.6696649297519441, 'colsample_bytree': 0.9389940652406195, 'min_child_samples': 15}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001887 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:14:07,121] Trial 65 finished with value: 0.6160577554145701 and parameters: {'num_leaves': 120, 'learning_rate': 0.015232296337335693, 'n_estimators': 500, 'subsample': 0.6439768652602849, 'colsample_bytree': 0.8974639286798486, 'min_child_samples': 7}. Best is trial 52 with value: 0.6196674688252024.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001740 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:14:23,639] Trial 66 finished with value: 0.6202143950995406 and parameters: {'num_leaves': 126, 'learning_rate': 0.011000523430425051, 'n_estimators': 300, 'subsample': 0.6011613205163285, 'colsample_bytree': 0.9749223339719659, 'min_child_samples': 10}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001700 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:14:40,777] Trial 67 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 127, 'learning_rate': 0.01074803039941604, 'n_estimators': 300, 'subsample': 0.9260899117669759, 'colsample_bytree': 0.9683311825516321, 'min_child_samples': 17}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001730 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:14:52,559] Trial 68 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 135, 'learning_rate': 0.011000396367259357, 'n_estimators': 200, 'subsample': 0.8934727990665043, 'colsample_bytree': 0.9830074329543118, 'min_child_samples': 17}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001741 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:15:04,137] Trial 69 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 128, 'learning_rate': 0.01301464725768693, 'n_estimators': 200, 'subsample': 0.9427021100648406, 'colsample_bytree': 0.9850537443108164, 'min_child_samples': 10}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:15:15,667] Trial 70 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 127, 'learning_rate': 0.013120823040035274, 'n_estimators': 200, 'subsample': 0.9592760429836914, 'colsample_bytree': 0.98711888982223, 'min_child_samples': 7}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001712 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:15:27,185] Trial 71 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 127, 'learning_rate': 0.01309002385138931, 'n_estimators': 200, 'subsample': 0.9518139396545309, 'colsample_bytree': 0.9827825960063419, 'min_child_samples': 7}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001794 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:15:38,785] Trial 72 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 127, 'learning_rate': 0.015853519618153542, 'n_estimators': 200, 'subsample': 0.9382666000590407, 'colsample_bytree': 0.9828583519412846, 'min_child_samples': 7}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001747 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:15:44,141] Trial 73 finished with value: 0.6166046816889084 and parameters: {'num_leaves': 136, 'learning_rate': 0.01583311123098454, 'n_estimators': 100, 'subsample': 0.9603614460494281, 'colsample_bytree': 0.9933969516522329, 'min_child_samples': 7}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001723 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:15:55,528] Trial 74 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 128, 'learning_rate': 0.013139250894489677, 'n_estimators': 200, 'subsample': 0.9960134902752987, 'colsample_bytree': 0.9808693969623788, 'min_child_samples': 5}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001795 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:16:06,846] Trial 75 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 125, 'learning_rate': 0.013563780104992677, 'n_estimators': 200, 'subsample': 0.9929422554115748, 'colsample_bytree': 0.9723418986411775, 'min_child_samples': 5}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001776 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:16:12,821] Trial 76 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 125, 'learning_rate': 0.02138867106888605, 'n_estimators': 100, 'subsample': 0.9299242803612612, 'colsample_bytree': 0.9779734647585365, 'min_child_samples': 7}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003110 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:16:23,771] Trial 77 finished with value: 0.61977685408007 and parameters: {'num_leaves': 130, 'learning_rate': 0.01300754026359735, 'n_estimators': 200, 'subsample': 0.980693200144694, 'colsample_bytree': 0.9961316796078562, 'min_child_samples': 6}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002825 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:16:34,671] Trial 78 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 132, 'learning_rate': 0.024333541085555892, 'n_estimators': 200, 'subsample': 0.9671188140933648, 'colsample_bytree': 0.9959665603180508, 'min_child_samples': 13}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001748 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:16:40,927] Trial 79 finished with value: 0.6168234521986437 and parameters: {'num_leaves': 139, 'learning_rate': 0.01588435916789651, 'n_estimators': 100, 'subsample': 0.9423302143327634, 'colsample_bytree': 0.9521357618554964, 'min_child_samples': 11}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001889 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:16:51,707] Trial 80 finished with value: 0.6176985342375848 and parameters: {'num_leaves': 119, 'learning_rate': 0.019398622554680445, 'n_estimators': 200, 'subsample': 0.9729041756137765, 'colsample_bytree': 0.9596628166493529, 'min_child_samples': 8}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001727 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:17:02,371] Trial 81 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 124, 'learning_rate': 0.013086454089627008, 'n_estimators': 200, 'subsample': 0.9858243440692712, 'colsample_bytree': 0.9862657440045716, 'min_child_samples': 5}. Best is trial 66 with value: 0.6202143950995406.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002855 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:17:13,290] Trial 82 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 129, 'learning_rate': 0.014490415568409472, 'n_estimators': 200, 'subsample': 0.9546314826124812, 'colsample_bytree': 0.9739940307450785, 'min_child_samples': 6}. Best is trial 82 with value: 0.6203237803544083.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001797 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:17:24,589] Trial 83 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 129, 'learning_rate': 0.017012572147953493, 'n_estimators': 200, 'subsample': 0.6015207372535837, 'colsample_bytree': 0.997368997828278, 'min_child_samples': 8}. Best is trial 82 with value: 0.6203237803544083.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002458 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:17:36,471] Trial 84 finished with value: 0.61977685408007 and parameters: {'num_leaves': 142, 'learning_rate': 0.014623406537267567, 'n_estimators': 200, 'subsample': 0.9540416595559258, 'colsample_bytree': 0.9469961979381049, 'min_child_samples': 6}. Best is trial 82 with value: 0.6203237803544083.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001729 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:17:42,847] Trial 85 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 142, 'learning_rate': 0.014620711204671252, 'n_estimators': 100, 'subsample': 0.9003299919781833, 'colsample_bytree': 0.9464093833291986, 'min_child_samples': 10}. Best is trial 82 with value: 0.6203237803544083.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001740 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:17:59,441] Trial 86 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 133, 'learning_rate': 0.012163159375431741, 'n_estimators': 300, 'subsample': 0.9223384007210537, 'colsample_bytree': 0.9602892789741505, 'min_child_samples': 6}. Best is trial 82 with value: 0.6203237803544083.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001727 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:18:22,470] Trial 87 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 144, 'learning_rate': 0.012114457206517685, 'n_estimators': 400, 'subsample': 0.9313939342452554, 'colsample_bytree': 0.9605608050856557, 'min_child_samples': 6}. Best is trial 82 with value: 0.6203237803544083.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:18:39,267] Trial 88 finished with value: 0.6205425508641436 and parameters: {'num_leaves': 133, 'learning_rate': 0.009883203062464036, 'n_estimators': 300, 'subsample': 0.9195456902318152, 'colsample_bytree': 0.9381045376523915, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001769 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:18:49,638] Trial 89 finished with value: 0.6026033690658499 and parameters: {'num_leaves': 30, 'learning_rate': 0.009733543756438742, 'n_estimators': 300, 'subsample': 0.9157298170629344, 'colsample_bytree': 0.9322452411973613, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001720 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:19:06,250] Trial 90 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 132, 'learning_rate': 0.011432856226413057, 'n_estimators': 300, 'subsample': 0.8522312536702605, 'colsample_bytree': 0.9432534239916912, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001700 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:19:23,475] Trial 91 finished with value: 0.6204331656092759 and parameters: {'num_leaves': 138, 'learning_rate': 0.014356476218073042, 'n_estimators': 300, 'subsample': 0.8718926999414568, 'colsample_bytree': 0.9552922131536882, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001782 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:19:39,595] Trial 92 finished with value: 0.616714066943776 and parameters: {'num_leaves': 137, 'learning_rate': 0.01948987657152809, 'n_estimators': 300, 'subsample': 0.9547215867299704, 'colsample_bytree': 0.9711483159927643, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001776 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:20:03,017] Trial 93 finished with value: 0.6173703784729818 and parameters: {'num_leaves': 146, 'learning_rate': 0.012262819364164906, 'n_estimators': 400, 'subsample': 0.868083948840305, 'colsample_bytree': 0.9557348731339853, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001701 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:20:19,917] Trial 94 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 133, 'learning_rate': 0.010009394995207995, 'n_estimators': 300, 'subsample': 0.9130344507773546, 'colsample_bytree': 0.9426998319861094, 'min_child_samples': 10}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001901 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:20:36,829] Trial 95 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 138, 'learning_rate': 0.014199580552962105, 'n_estimators': 300, 'subsample': 0.9797204493015768, 'colsample_bytree': 0.9670061215732285, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002921 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:21:00,237] Trial 96 finished with value: 0.6175891489827171 and parameters: {'num_leaves': 143, 'learning_rate': 0.010962614096328833, 'n_estimators': 400, 'subsample': 0.9175510905979165, 'colsample_bytree': 0.9272468455687436, 'min_child_samples': 5}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001693 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:21:18,308] Trial 97 finished with value: 0.614854517611026 and parameters: {'num_leaves': 149, 'learning_rate': 0.016759012365147142, 'n_estimators': 300, 'subsample': 0.8821928773828307, 'colsample_bytree': 0.9147650227208438, 'min_child_samples': 12}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001781 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:21:32,749] Trial 98 finished with value: 0.6023845985561146 and parameters: {'num_leaves': 141, 'learning_rate': 0.06566410820639405, 'n_estimators': 300, 'subsample': 0.9726300766578476, 'colsample_bytree': 0.9524371342459084, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002846 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:21:53,495] Trial 99 finished with value: 0.6169328374535112 and parameters: {'num_leaves': 122, 'learning_rate': 0.015093001079141726, 'n_estimators': 400, 'subsample': 0.9557360230233917, 'colsample_bytree': 0.9909841131515557, 'min_child_samples': 9}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004330 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:22:04,197] Trial 100 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 116, 'learning_rate': 0.012551517264853509, 'n_estimators': 200, 'subsample': 0.8119477648082141, 'colsample_bytree': 0.9745189840329548, 'min_child_samples': 14}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001736 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:22:15,752] Trial 101 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 134, 'learning_rate': 0.014020644352320099, 'n_estimators': 200, 'subsample': 0.9379919419476707, 'colsample_bytree': 0.9611452644458479, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001648 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:22:26,439] Trial 102 finished with value: 0.5805075475825858 and parameters: {'num_leaves': 134, 'learning_rate': 0.014095464425544749, 'n_estimators': 200, 'subsample': 0.9078976675550763, 'colsample_bytree': 0.6107112861012253, 'min_child_samples': 5}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001750 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:22:32,435] Trial 103 finished with value: 0.6152920586304966 and parameters: {'num_leaves': 132, 'learning_rate': 0.014047895860558157, 'n_estimators': 100, 'subsample': 0.9472253024809079, 'colsample_bytree': 0.9586900634746603, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002970 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:22:41,606] Trial 104 finished with value: 0.611791730474732 and parameters: {'num_leaves': 80, 'learning_rate': 0.011149660977595023, 'n_estimators': 200, 'subsample': 0.9670053118392019, 'colsample_bytree': 0.9385882243348818, 'min_child_samples': 10}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001873 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:22:58,487] Trial 105 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 130, 'learning_rate': 0.018433911114740707, 'n_estimators': 300, 'subsample': 0.9219265128916684, 'colsample_bytree': 0.9625300200849092, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001832 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:23:14,179] Trial 106 finished with value: 0.586304966090571 and parameters: {'num_leaves': 140, 'learning_rate': 0.016555202028382177, 'n_estimators': 300, 'subsample': 0.6104337366133197, 'colsample_bytree': 0.685838517454103, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001726 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:23:19,498] Trial 107 finished with value: 0.6144169765915555 and parameters: {'num_leaves': 137, 'learning_rate': 0.009388966424704038, 'n_estimators': 100, 'subsample': 0.8821603323173167, 'colsample_bytree': 0.9905973834756859, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001730 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:23:30,918] Trial 108 finished with value: 0.618245460511923 and parameters: {'num_leaves': 123, 'learning_rate': 0.010318736762141484, 'n_estimators': 200, 'subsample': 0.938076635246548, 'colsample_bytree': 0.999727176190081, 'min_child_samples': 11}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002130 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:23:53,558] Trial 109 finished with value: 0.5894771384817327 and parameters: {'num_leaves': 135, 'learning_rate': 0.089852653345351, 'n_estimators': 500, 'subsample': 0.6170988102588656, 'colsample_bytree': 0.9758930283044261, 'min_child_samples': 9}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001715 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:24:05,197] Trial 110 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 129, 'learning_rate': 0.011566651433517813, 'n_estimators': 200, 'subsample': 0.96275258897288, 'colsample_bytree': 0.9467591382373632, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002070 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:24:16,285] Trial 111 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 119, 'learning_rate': 0.01570002355826005, 'n_estimators': 200, 'subsample': 0.937194228088819, 'colsample_bytree': 0.9775310771205525, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001741 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:24:27,297] Trial 112 finished with value: 0.618245460511923 and parameters: {'num_leaves': 120, 'learning_rate': 0.012540648720381681, 'n_estimators': 200, 'subsample': 0.9322231602600985, 'colsample_bytree': 0.9671689951039131, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001730 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:24:39,960] Trial 113 finished with value: 0.6138700503172172 and parameters: {'num_leaves': 71, 'learning_rate': 0.015394710594153978, 'n_estimators': 300, 'subsample': 0.947761059753804, 'colsample_bytree': 0.9771965284562385, 'min_child_samples': 5}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001726 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:24:48,205] Trial 114 finished with value: 0.6108072631809233 and parameters: {'num_leaves': 64, 'learning_rate': 0.013645893021392201, 'n_estimators': 200, 'subsample': 0.9886524240247043, 'colsample_bytree': 0.9236491320807259, 'min_child_samples': 10}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001729 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:24:59,442] Trial 115 finished with value: 0.6184642310216583 and parameters: {'num_leaves': 125, 'learning_rate': 0.014877916276565392, 'n_estimators': 200, 'subsample': 0.9733494374598458, 'colsample_bytree': 0.9516825989198181, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001830 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:25:22,604] Trial 116 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 131, 'learning_rate': 0.011761305078062398, 'n_estimators': 400, 'subsample': 0.9079353144602427, 'colsample_bytree': 0.9874818747842258, 'min_child_samples': 12}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001786 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:25:38,482] Trial 117 finished with value: 0.61977685408007 and parameters: {'num_leaves': 115, 'learning_rate': 0.012629842328009783, 'n_estimators': 300, 'subsample': 0.9564388855559388, 'colsample_bytree': 0.9352017643604824, 'min_child_samples': 9}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001721 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:25:53,902] Trial 118 finished with value: 0.618245460511923 and parameters: {'num_leaves': 115, 'learning_rate': 0.020239481552461473, 'n_estimators': 300, 'subsample': 0.8339511047790588, 'colsample_bytree': 0.9363300795309133, 'min_child_samples': 9}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002830 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:26:10,915] Trial 119 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 118, 'learning_rate': 0.010610045486363457, 'n_estimators': 300, 'subsample': 0.9267606775305746, 'colsample_bytree': 0.9077667918513006, 'min_child_samples': 13}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001820 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:26:25,719] Trial 120 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 110, 'learning_rate': 0.022687642126664697, 'n_estimators': 300, 'subsample': 0.9348192532770915, 'colsample_bytree': 0.9626436512334021, 'min_child_samples': 11}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001746 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:26:36,287] Trial 121 finished with value: 0.5778823014657624 and parameters: {'num_leaves': 122, 'learning_rate': 0.012328661604246598, 'n_estimators': 200, 'subsample': 0.9995705393803, 'colsample_bytree': 0.7206390937125257, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001808 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:26:41,566] Trial 122 finished with value: 0.6141982060818202 and parameters: {'num_leaves': 133, 'learning_rate': 0.013306851232793716, 'n_estimators': 100, 'subsample': 0.6341350867082818, 'colsample_bytree': 0.9484768029714123, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001787 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:26:53,098] Trial 123 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 127, 'learning_rate': 0.017914033770390553, 'n_estimators': 200, 'subsample': 0.9588570736436896, 'colsample_bytree': 0.9712935443096283, 'min_child_samples': 5}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001756 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:27:14,010] Trial 124 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 118, 'learning_rate': 0.014587877215504483, 'n_estimators': 400, 'subsample': 0.9458108615946692, 'colsample_bytree': 0.9584077446585401, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001699 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:27:31,982] Trial 125 finished with value: 0.6173703784729818 and parameters: {'num_leaves': 146, 'learning_rate': 0.012719394047464234, 'n_estimators': 300, 'subsample': 0.9828186686794259, 'colsample_bytree': 0.9786769104466438, 'min_child_samples': 31}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002231 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:27:43,823] Trial 126 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 138, 'learning_rate': 0.011321330957124894, 'n_estimators': 200, 'subsample': 0.9514010686111313, 'colsample_bytree': 0.9374729436969997, 'min_child_samples': 9}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002109 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:28:03,063] Trial 127 finished with value: 0.6161671406694378 and parameters: {'num_leaves': 124, 'learning_rate': 0.029940285337770652, 'n_estimators': 400, 'subsample': 0.9640010249607073, 'colsample_bytree': 0.9212402598561401, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002927 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:28:19,172] Trial 128 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 120, 'learning_rate': 0.01592401235752138, 'n_estimators': 300, 'subsample': 0.9745712788150137, 'colsample_bytree': 0.9888341548470774, 'min_child_samples': 10}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001694 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:28:30,938] Trial 129 finished with value: 0.6184642310216583 and parameters: {'num_leaves': 130, 'learning_rate': 0.013551903940809221, 'n_estimators': 200, 'subsample': 0.9213572520963154, 'colsample_bytree': 0.9664645140036164, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001734 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:28:46,135] Trial 130 finished with value: 0.6155108291402319 and parameters: {'num_leaves': 109, 'learning_rate': 0.016720721713567168, 'n_estimators': 300, 'subsample': 0.899654947084427, 'colsample_bytree': 0.9426565606808214, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001718 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:28:57,382] Trial 131 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 126, 'learning_rate': 0.015780013246987242, 'n_estimators': 200, 'subsample': 0.940914576267313, 'colsample_bytree': 0.9998989733719583, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001919 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:29:02,722] Trial 132 finished with value: 0.6169328374535112 and parameters: {'num_leaves': 135, 'learning_rate': 0.014721124519347713, 'n_estimators': 100, 'subsample': 0.9392831418059601, 'colsample_bytree': 0.9959602483464332, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001726 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:29:14,023] Trial 133 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 126, 'learning_rate': 0.013820206627232762, 'n_estimators': 200, 'subsample': 0.9561407936632619, 'colsample_bytree': 0.9815676578450687, 'min_child_samples': 5}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001727 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:29:25,548] Trial 134 finished with value: 0.618245460511923 and parameters: {'num_leaves': 129, 'learning_rate': 0.012338569010626863, 'n_estimators': 200, 'subsample': 0.9447682868538352, 'colsample_bytree': 0.953695038621645, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001730 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:29:37,632] Trial 135 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 142, 'learning_rate': 0.010480688139279175, 'n_estimators': 200, 'subsample': 0.9670393292061669, 'colsample_bytree': 0.9996579833987073, 'min_child_samples': 9}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001744 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:29:53,334] Trial 136 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 122, 'learning_rate': 0.017275503237630076, 'n_estimators': 300, 'subsample': 0.928001115459504, 'colsample_bytree': 0.9708809586584747, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001750 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:30:05,180] Trial 137 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 133, 'learning_rate': 0.01598573512014285, 'n_estimators': 200, 'subsample': 0.8622722923484258, 'colsample_bytree': 0.9286665604243088, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001820 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:30:11,523] Trial 138 finished with value: 0.6146357471012908 and parameters: {'num_leaves': 139, 'learning_rate': 0.009138377185445322, 'n_estimators': 100, 'subsample': 0.9063025095412499, 'colsample_bytree': 0.9881195669790508, 'min_child_samples': 5}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001762 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:30:26,712] Trial 139 finished with value: 0.6088383285933057 and parameters: {'num_leaves': 117, 'learning_rate': 0.04335306301081946, 'n_estimators': 300, 'subsample': 0.9528785750442749, 'colsample_bytree': 0.9780466637609019, 'min_child_samples': 37}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001722 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:31:24,092] Trial 140 finished with value: 0.612994968278276 and parameters: {'num_leaves': 125, 'learning_rate': 0.011798526758816331, 'n_estimators': 1000, 'subsample': 0.9795274109989704, 'colsample_bytree': 0.9621639131925885, 'min_child_samples': 9}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002873 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:31:35,512] Trial 141 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 127, 'learning_rate': 0.01535678295492737, 'n_estimators': 200, 'subsample': 0.9374468829462661, 'colsample_bytree': 0.9832639728878974, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001701 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:31:47,160] Trial 142 finished with value: 0.6204331656092759 and parameters: {'num_leaves': 130, 'learning_rate': 0.018789398077418716, 'n_estimators': 200, 'subsample': 0.9388795880996695, 'colsample_bytree': 0.9894427021340372, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001712 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:31:58,967] Trial 143 finished with value: 0.6175891489827171 and parameters: {'num_leaves': 136, 'learning_rate': 0.019340036007549563, 'n_estimators': 200, 'subsample': 0.9207138564906092, 'colsample_bytree': 0.9913757042219482, 'min_child_samples': 10}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001890 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:32:10,568] Trial 144 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 131, 'learning_rate': 0.0186169719605029, 'n_estimators': 200, 'subsample': 0.9605584673635085, 'colsample_bytree': 0.975726262506836, 'min_child_samples': 6}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001767 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:32:22,156] Trial 145 finished with value: 0.618573616276526 and parameters: {'num_leaves': 128, 'learning_rate': 0.014218129205983842, 'n_estimators': 200, 'subsample': 0.9480749784703245, 'colsample_bytree': 0.9539310829134572, 'min_child_samples': 11}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001871 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:32:38,862] Trial 146 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 134, 'learning_rate': 0.013001186738980706, 'n_estimators': 300, 'subsample': 0.9354407690252657, 'colsample_bytree': 0.9994156108343166, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001698 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:32:49,771] Trial 147 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 113, 'learning_rate': 0.01778326961345853, 'n_estimators': 200, 'subsample': 0.9255597192342635, 'colsample_bytree': 0.9665358301321811, 'min_child_samples': 7}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003098 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:33:15,762] Trial 148 finished with value: 0.614854517611026 and parameters: {'num_leaves': 123, 'learning_rate': 0.014802923170449395, 'n_estimators': 500, 'subsample': 0.9686037058069857, 'colsample_bytree': 0.8667391256786015, 'min_child_samples': 5}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001708 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:33:21,808] Trial 149 finished with value: 0.6164952964340407 and parameters: {'num_leaves': 120, 'learning_rate': 0.016759893913860093, 'n_estimators': 100, 'subsample': 0.6489183144658168, 'colsample_bytree': 0.9459639550128888, 'min_child_samples': 9}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001794 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:33:32,690] Trial 150 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 131, 'learning_rate': 0.020731560320289177, 'n_estimators': 200, 'subsample': 0.8894115324347068, 'colsample_bytree': 0.9734848862586628, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002849 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:33:43,546] Trial 151 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 130, 'learning_rate': 0.02095903204991117, 'n_estimators': 200, 'subsample': 0.9124968788071655, 'colsample_bytree': 0.9873290064315111, 'min_child_samples': 8}. Best is trial 88 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002890 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:33:55,373] Trial 152 finished with value: 0.6208707066287464 and parameters: {'num_leaves': 139, 'learning_rate': 0.012720598647674946, 'n_estimators': 200, 'subsample': 0.8901460283948105, 'colsample_bytree': 0.9726169631302614, 'min_child_samples': 6}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001824 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:34:06,901] Trial 153 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 138, 'learning_rate': 0.024049476713136925, 'n_estimators': 200, 'subsample': 0.8679607767545906, 'colsample_bytree': 0.9585729825429462, 'min_child_samples': 6}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002002 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:34:24,914] Trial 154 finished with value: 0.620105009844673 and parameters: {'num_leaves': 144, 'learning_rate': 0.01002412834302653, 'n_estimators': 300, 'subsample': 0.8831204157042606, 'colsample_bytree': 0.9693372289336504, 'min_child_samples': 8}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001740 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:34:42,698] Trial 155 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 148, 'learning_rate': 0.008150784770271523, 'n_estimators': 300, 'subsample': 0.8860204676104602, 'colsample_bytree': 0.9726546627843446, 'min_child_samples': 8}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001636 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:34:58,090] Trial 156 finished with value: 0.587836359658718 and parameters: {'num_leaves': 144, 'learning_rate': 0.022388521927222186, 'n_estimators': 300, 'subsample': 0.8717187064481839, 'colsample_bytree': 0.652843685673222, 'min_child_samples': 6}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001809 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:35:10,083] Trial 157 finished with value: 0.6202143950995406 and parameters: {'num_leaves': 142, 'learning_rate': 0.011161120262766078, 'n_estimators': 200, 'subsample': 0.8904773850541978, 'colsample_bytree': 0.9764735707793635, 'min_child_samples': 5}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001766 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:35:28,202] Trial 158 finished with value: 0.6173703784729818 and parameters: {'num_leaves': 150, 'learning_rate': 0.011006369851535346, 'n_estimators': 300, 'subsample': 0.840652718380293, 'colsample_bytree': 0.9624809331540496, 'min_child_samples': 5}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001841 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:35:33,612] Trial 159 finished with value: 0.614854517611026 and parameters: {'num_leaves': 140, 'learning_rate': 0.00998565086377268, 'n_estimators': 100, 'subsample': 0.898149741726746, 'colsample_bytree': 0.9819382191648373, 'min_child_samples': 6}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001840 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:36:24,070] Trial 160 finished with value: 0.614854517611026 and parameters: {'num_leaves': 143, 'learning_rate': 0.012389085328336408, 'n_estimators': 900, 'subsample': 0.8772940865296117, 'colsample_bytree': 0.966823088635194, 'min_child_samples': 5}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001843 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:36:35,994] Trial 161 finished with value: 0.618573616276526 and parameters: {'num_leaves': 141, 'learning_rate': 0.013478073141935722, 'n_estimators': 200, 'subsample': 0.8899937249351875, 'colsample_bytree': 0.9756306798988691, 'min_child_samples': 7}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001922 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:36:47,767] Trial 162 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 136, 'learning_rate': 0.009907707753743876, 'n_estimators': 200, 'subsample': 0.8606039753715489, 'colsample_bytree': 0.9537285712478903, 'min_child_samples': 7}. Best is trial 152 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001768 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:36:59,651] Trial 163 finished with value: 0.6217457886676876 and parameters: {'num_leaves': 146, 'learning_rate': 0.014249669568912725, 'n_estimators': 200, 'subsample': 0.8911334644145417, 'colsample_bytree': 0.9928240257678652, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001787 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:37:11,534] Trial 164 finished with value: 0.621308247648217 and parameters: {'num_leaves': 144, 'learning_rate': 0.015719022129874888, 'n_estimators': 200, 'subsample': 0.9026024740583637, 'colsample_bytree': 0.994378998282979, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002169 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:37:23,386] Trial 165 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 141, 'learning_rate': 0.014143367500857816, 'n_estimators': 200, 'subsample': 0.9044289080446093, 'colsample_bytree': 0.9959983518716797, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001642 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:37:39,560] Trial 166 finished with value: 0.5861955808357033 and parameters: {'num_leaves': 146, 'learning_rate': 0.01186744811777997, 'n_estimators': 300, 'subsample': 0.8954536756921341, 'colsample_bytree': 0.7783934812245414, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001769 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:37:51,531] Trial 167 finished with value: 0.6204331656092759 and parameters: {'num_leaves': 146, 'learning_rate': 0.010804480528798842, 'n_estimators': 200, 'subsample': 0.8739785580585226, 'colsample_bytree': 0.9929745564431015, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001790 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:38:08,996] Trial 168 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 147, 'learning_rate': 0.010854487749494431, 'n_estimators': 300, 'subsample': 0.8799266685574334, 'colsample_bytree': 0.9918494163573456, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001832 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:38:21,031] Trial 169 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 144, 'learning_rate': 0.009585001921552786, 'n_estimators': 200, 'subsample': 0.8735961267911088, 'colsample_bytree': 0.9911976846671161, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001790 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:38:33,115] Trial 170 finished with value: 0.620105009844673 and parameters: {'num_leaves': 147, 'learning_rate': 0.011181866590883242, 'n_estimators': 200, 'subsample': 0.8550840357121815, 'colsample_bytree': 0.9998314699659313, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001820 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:38:45,370] Trial 171 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 149, 'learning_rate': 0.01123780446133273, 'n_estimators': 200, 'subsample': 0.8876168205007982, 'colsample_bytree': 0.9999419562539775, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001996 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:38:57,338] Trial 172 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 145, 'learning_rate': 0.012554909312292525, 'n_estimators': 200, 'subsample': 0.8605725292522087, 'colsample_bytree': 0.9845496248750701, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001771 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:39:09,377] Trial 173 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 145, 'learning_rate': 0.010231417336596063, 'n_estimators': 200, 'subsample': 0.8490404313810479, 'colsample_bytree': 0.9850913719056017, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001700 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:39:21,461] Trial 174 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 150, 'learning_rate': 0.01272949819974688, 'n_estimators': 200, 'subsample': 0.8625610449484447, 'colsample_bytree': 0.9923303094892891, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:39:33,549] Trial 175 finished with value: 0.6202143950995406 and parameters: {'num_leaves': 147, 'learning_rate': 0.01185150543642026, 'n_estimators': 200, 'subsample': 0.852493290238718, 'colsample_bytree': 0.9845893286316579, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001710 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:39:45,621] Trial 176 finished with value: 0.620105009844673 and parameters: {'num_leaves': 147, 'learning_rate': 0.010702989620372153, 'n_estimators': 200, 'subsample': 0.8541246719271517, 'colsample_bytree': 0.9844301900812705, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001838 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:39:51,355] Trial 177 finished with value: 0.6149639028658936 and parameters: {'num_leaves': 147, 'learning_rate': 0.010612964301287489, 'n_estimators': 100, 'subsample': 0.8551361756271586, 'colsample_bytree': 0.9839426231045455, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002953 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:40:03,028] Trial 178 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 145, 'learning_rate': 0.009054210168090842, 'n_estimators': 200, 'subsample': 0.8416896114095737, 'colsample_bytree': 0.9794365381556764, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002924 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:40:15,105] Trial 179 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 147, 'learning_rate': 0.011599695847119021, 'n_estimators': 200, 'subsample': 0.8565931974729851, 'colsample_bytree': 0.9919487468422001, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001825 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:40:27,109] Trial 180 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 143, 'learning_rate': 0.010952192866116369, 'n_estimators': 200, 'subsample': 0.8680111586144306, 'colsample_bytree': 0.9875649437861472, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001935 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:40:38,898] Trial 181 finished with value: 0.61977685408007 and parameters: {'num_leaves': 139, 'learning_rate': 0.01201274499507966, 'n_estimators': 200, 'subsample': 0.8308508771686117, 'colsample_bytree': 0.9999834700076959, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001756 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:40:50,862] Trial 182 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 142, 'learning_rate': 0.010344238732291353, 'n_estimators': 200, 'subsample': 0.8781781819658324, 'colsample_bytree': 0.9814765527116376, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001807 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:41:03,121] Trial 183 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 150, 'learning_rate': 0.009489451684171209, 'n_estimators': 200, 'subsample': 0.8950827399555109, 'colsample_bytree': 0.9702423412842711, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001726 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:41:15,077] Trial 184 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 145, 'learning_rate': 0.01119194137687801, 'n_estimators': 200, 'subsample': 0.8745438598519474, 'colsample_bytree': 0.9920641531194814, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001760 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:41:27,060] Trial 185 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 146, 'learning_rate': 0.01130600591327602, 'n_estimators': 200, 'subsample': 0.8651533484519441, 'colsample_bytree': 0.9902553914528825, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001718 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:41:39,149] Trial 186 finished with value: 0.618245460511923 and parameters: {'num_leaves': 148, 'learning_rate': 0.01184495848758374, 'n_estimators': 200, 'subsample': 0.8508600441226761, 'colsample_bytree': 0.9992856853784707, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001794 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:41:51,154] Trial 187 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 144, 'learning_rate': 0.010014237218316489, 'n_estimators': 200, 'subsample': 0.8745924036856282, 'colsample_bytree': 0.9811929566295691, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001776 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:41:56,537] Trial 188 finished with value: 0.6152920586304966 and parameters: {'num_leaves': 141, 'learning_rate': 0.013255878514765024, 'n_estimators': 100, 'subsample': 0.8830885589083177, 'colsample_bytree': 0.9735805620911905, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001747 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:42:08,327] Trial 189 finished with value: 0.6175891489827171 and parameters: {'num_leaves': 138, 'learning_rate': 0.008676898996936354, 'n_estimators': 200, 'subsample': 0.8222538461753854, 'colsample_bytree': 0.9927024726583983, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002889 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:42:20,247] Trial 190 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 144, 'learning_rate': 0.010709346620873013, 'n_estimators': 200, 'subsample': 0.8998673014684565, 'colsample_bytree': 0.9854252867066001, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001720 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:42:31,980] Trial 191 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 141, 'learning_rate': 0.014767345848139926, 'n_estimators': 200, 'subsample': 0.8731895140665866, 'colsample_bytree': 0.9685632250567181, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001770 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:42:43,737] Trial 192 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 137, 'learning_rate': 0.012252725752935213, 'n_estimators': 200, 'subsample': 0.9126861892876548, 'colsample_bytree': 0.9778737054283791, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001708 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:42:55,433] Trial 193 finished with value: 0.618573616276526 and parameters: {'num_leaves': 137, 'learning_rate': 0.012189630435591497, 'n_estimators': 200, 'subsample': 0.9111895206308888, 'colsample_bytree': 0.978316365697248, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001689 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:43:07,576] Trial 194 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 147, 'learning_rate': 0.013236755255111433, 'n_estimators': 200, 'subsample': 0.8890299628403714, 'colsample_bytree': 0.9874070539461862, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:43:19,838] Trial 195 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 147, 'learning_rate': 0.01142688274817899, 'n_estimators': 200, 'subsample': 0.8878295501557465, 'colsample_bytree': 0.9851496379442304, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001848 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:43:31,898] Trial 196 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 145, 'learning_rate': 0.012796771090396623, 'n_estimators': 200, 'subsample': 0.9035300209085645, 'colsample_bytree': 0.9746082189163192, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001777 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:43:49,353] Trial 197 finished with value: 0.6174797637278495 and parameters: {'num_leaves': 150, 'learning_rate': 0.013623535135661561, 'n_estimators': 300, 'subsample': 0.9159187351784103, 'colsample_bytree': 0.9914394196417985, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:44:01,220] Trial 198 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 139, 'learning_rate': 0.010536182814027724, 'n_estimators': 200, 'subsample': 0.890735051521978, 'colsample_bytree': 0.9682525997090525, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001708 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:44:07,644] Trial 199 finished with value: 0.6151826733756289 and parameters: {'num_leaves': 143, 'learning_rate': 0.009583840135987738, 'n_estimators': 100, 'subsample': 0.8796231704053151, 'colsample_bytree': 0.9851781704673739, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001852 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:44:19,583] Trial 200 finished with value: 0.61977685408007 and parameters: {'num_leaves': 148, 'learning_rate': 0.012226638216814991, 'n_estimators': 200, 'subsample': 0.8577353217806124, 'colsample_bytree': 0.9796883319095661, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001703 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:44:31,147] Trial 201 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 145, 'learning_rate': 0.012981622421653027, 'n_estimators': 200, 'subsample': 0.8466584664206153, 'colsample_bytree': 0.9950124700590316, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003093 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:44:42,635] Trial 202 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 141, 'learning_rate': 0.0111480202394806, 'n_estimators': 200, 'subsample': 0.8972940502948936, 'colsample_bytree': 0.9939534825118563, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003015 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:44:50,593] Trial 203 finished with value: 0.6067600087508204 and parameters: {'num_leaves': 46, 'learning_rate': 0.013678683178860816, 'n_estimators': 200, 'subsample': 0.8660867117612767, 'colsample_bytree': 0.9994019547112838, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001715 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:45:02,354] Trial 204 finished with value: 0.6202143950995406 and parameters: {'num_leaves': 134, 'learning_rate': 0.012056265878636828, 'n_estimators': 200, 'subsample': 0.9114148387889551, 'colsample_bytree': 0.9762909690530005, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001860 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:45:14,304] Trial 205 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 139, 'learning_rate': 0.011420223635915341, 'n_estimators': 200, 'subsample': 0.9130188738145785, 'colsample_bytree': 0.9657592178016752, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001691 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:45:31,577] Trial 206 finished with value: 0.6205425508641436 and parameters: {'num_leaves': 135, 'learning_rate': 0.01140054174050341, 'n_estimators': 300, 'subsample': 0.9173228838977592, 'colsample_bytree': 0.9646590957192652, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001822 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:45:49,671] Trial 207 finished with value: 0.6202143950995406 and parameters: {'num_leaves': 134, 'learning_rate': 0.010102917294445119, 'n_estimators': 300, 'subsample': 0.9251461022629255, 'colsample_bytree': 0.9608798682256104, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001752 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:46:07,080] Trial 208 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 135, 'learning_rate': 0.010342160118734378, 'n_estimators': 300, 'subsample': 0.9192925585901988, 'colsample_bytree': 0.959080624972361, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003055 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:46:24,697] Trial 209 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 135, 'learning_rate': 0.009306875258824615, 'n_estimators': 300, 'subsample': 0.9237258125928814, 'colsample_bytree': 0.9642675684379761, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002922 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:46:42,043] Trial 210 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 134, 'learning_rate': 0.01022754850989527, 'n_estimators': 300, 'subsample': 0.9276071101815903, 'colsample_bytree': 0.9694701485511009, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001883 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:46:59,090] Trial 211 finished with value: 0.6211988623933494 and parameters: {'num_leaves': 133, 'learning_rate': 0.01005354351475645, 'n_estimators': 300, 'subsample': 0.9069386854416281, 'colsample_bytree': 0.9666535038699058, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001710 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:47:16,630] Trial 212 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 134, 'learning_rate': 0.009994280955851326, 'n_estimators': 300, 'subsample': 0.9070403624198836, 'colsample_bytree': 0.9551981906890191, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001844 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:47:33,409] Trial 213 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 132, 'learning_rate': 0.01056088427003388, 'n_estimators': 300, 'subsample': 0.9154176682717093, 'colsample_bytree': 0.9667224499838825, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001755 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:47:50,713] Trial 214 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 138, 'learning_rate': 0.009848437877327504, 'n_estimators': 300, 'subsample': 0.9301907144576821, 'colsample_bytree': 0.9703021502495973, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001817 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:48:08,867] Trial 215 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 133, 'learning_rate': 0.008961569599838834, 'n_estimators': 300, 'subsample': 0.8989318678855664, 'colsample_bytree': 0.9576987910467439, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001713 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:48:26,072] Trial 216 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 139, 'learning_rate': 0.00907475566950323, 'n_estimators': 300, 'subsample': 0.9045788990890148, 'colsample_bytree': 0.9616364303925655, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001738 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:48:43,800] Trial 217 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 133, 'learning_rate': 0.007632284302689343, 'n_estimators': 300, 'subsample': 0.9086668995648008, 'colsample_bytree': 0.9502345029311475, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002960 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:49:01,265] Trial 218 finished with value: 0.620105009844673 and parameters: {'num_leaves': 136, 'learning_rate': 0.008995034072964599, 'n_estimators': 300, 'subsample': 0.8998487715960399, 'colsample_bytree': 0.9725360736365806, 'min_child_samples': 12}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001769 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:49:18,711] Trial 219 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 140, 'learning_rate': 0.008275845630731884, 'n_estimators': 300, 'subsample': 0.9152192718404046, 'colsample_bytree': 0.9655406015912932, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001755 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:49:36,927] Trial 220 finished with value: 0.621308247648217 and parameters: {'num_leaves': 140, 'learning_rate': 0.007876338501087626, 'n_estimators': 300, 'subsample': 0.9178626044373949, 'colsample_bytree': 0.9562928748177907, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001739 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:49:54,463] Trial 221 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 140, 'learning_rate': 0.00844489706859399, 'n_estimators': 300, 'subsample': 0.9167881854493352, 'colsample_bytree': 0.9597928318335465, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001753 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:50:12,611] Trial 222 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 137, 'learning_rate': 0.008422945212022833, 'n_estimators': 300, 'subsample': 0.9249397758381489, 'colsample_bytree': 0.9662774981159804, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002242 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:50:30,088] Trial 223 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 134, 'learning_rate': 0.007785208572657494, 'n_estimators': 300, 'subsample': 0.8946054641025764, 'colsample_bytree': 0.9522462787889916, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001733 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:50:48,020] Trial 224 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 142, 'learning_rate': 0.009522659682149472, 'n_estimators': 300, 'subsample': 0.7848705411118142, 'colsample_bytree': 0.9754715793415101, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001734 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:51:07,043] Trial 225 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 142, 'learning_rate': 0.007337984376359246, 'n_estimators': 300, 'subsample': 0.9098838046527622, 'colsample_bytree': 0.9561231844103346, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001714 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:51:24,406] Trial 226 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 139, 'learning_rate': 0.008134364593114915, 'n_estimators': 300, 'subsample': 0.928071113615514, 'colsample_bytree': 0.9731830850301231, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001752 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:51:42,069] Trial 227 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 131, 'learning_rate': 0.009527027881248193, 'n_estimators': 300, 'subsample': 0.7928728707421663, 'colsample_bytree': 0.9661438673836407, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:51:59,140] Trial 228 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 131, 'learning_rate': 0.006942727758658916, 'n_estimators': 300, 'subsample': 0.9195122532364558, 'colsample_bytree': 0.9464805631890577, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001823 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:52:16,039] Trial 229 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 132, 'learning_rate': 0.008769542937925718, 'n_estimators': 300, 'subsample': 0.7819591228648185, 'colsample_bytree': 0.9621882467635364, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001794 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:52:39,375] Trial 230 finished with value: 0.6175891489827171 and parameters: {'num_leaves': 129, 'learning_rate': 0.009117682492690469, 'n_estimators': 400, 'subsample': 0.7972388527347702, 'colsample_bytree': 0.9734816173852966, 'min_child_samples': 12}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:52:57,014] Trial 231 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 136, 'learning_rate': 0.009610946737941915, 'n_estimators': 300, 'subsample': 0.792403587600965, 'colsample_bytree': 0.9662257983410416, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003016 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:53:14,676] Trial 232 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 142, 'learning_rate': 0.009964624940392987, 'n_estimators': 300, 'subsample': 0.9042045792486301, 'colsample_bytree': 0.9759338152924598, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001704 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:53:31,778] Trial 233 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 134, 'learning_rate': 0.009424906096273422, 'n_estimators': 300, 'subsample': 0.9127762032687203, 'colsample_bytree': 0.9587375186280205, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001703 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:53:50,078] Trial 234 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 139, 'learning_rate': 0.008581874871243348, 'n_estimators': 300, 'subsample': 0.809622106914028, 'colsample_bytree': 0.9667465968927286, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:54:07,336] Trial 235 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 136, 'learning_rate': 0.01030939881335121, 'n_estimators': 300, 'subsample': 0.8960398637014583, 'colsample_bytree': 0.9773992248680835, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001723 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:54:24,682] Trial 236 finished with value: 0.6204331656092759 and parameters: {'num_leaves': 132, 'learning_rate': 0.007968279679813214, 'n_estimators': 300, 'subsample': 0.9029048677281868, 'colsample_bytree': 0.9547623682316765, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002976 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:54:42,662] Trial 237 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 130, 'learning_rate': 0.00783685573140816, 'n_estimators': 300, 'subsample': 0.7653227154068516, 'colsample_bytree': 0.9503337828412758, 'min_child_samples': 48}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001751 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:55:05,882] Trial 238 finished with value: 0.61977685408007 and parameters: {'num_leaves': 132, 'learning_rate': 0.007980076855126373, 'n_estimators': 400, 'subsample': 0.7834143615725249, 'colsample_bytree': 0.943894940149241, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001829 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:55:20,439] Trial 239 finished with value: 0.6123386567490702 and parameters: {'num_leaves': 87, 'learning_rate': 0.0068819390073037315, 'n_estimators': 300, 'subsample': 0.9221574444061554, 'colsample_bytree': 0.9531526080969844, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001712 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:55:37,168] Trial 240 finished with value: 0.6206519361190111 and parameters: {'num_leaves': 133, 'learning_rate': 0.0116225892827825, 'n_estimators': 300, 'subsample': 0.9067349975046388, 'colsample_bytree': 0.9578695353325718, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001756 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:55:54,529] Trial 241 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 133, 'learning_rate': 0.01086585332565881, 'n_estimators': 300, 'subsample': 0.9043366338202584, 'colsample_bytree': 0.9632052409453031, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001792 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:56:11,512] Trial 242 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 129, 'learning_rate': 0.011547688180690614, 'n_estimators': 300, 'subsample': 0.9137603426978201, 'colsample_bytree': 0.9567278024350995, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:56:28,691] Trial 243 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 136, 'learning_rate': 0.008597472259268327, 'n_estimators': 300, 'subsample': 0.901550584817059, 'colsample_bytree': 0.9790807056124387, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002857 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:56:46,524] Trial 244 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 133, 'learning_rate': 0.009055452433209231, 'n_estimators': 300, 'subsample': 0.9322573001671586, 'colsample_bytree': 0.9697759843116562, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001722 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:57:03,546] Trial 245 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 137, 'learning_rate': 0.011678519736820023, 'n_estimators': 300, 'subsample': 0.9095894797842351, 'colsample_bytree': 0.9621851364020477, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001689 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:57:21,052] Trial 246 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 139, 'learning_rate': 0.00961082789739168, 'n_estimators': 300, 'subsample': 0.8938024333600404, 'colsample_bytree': 0.9418818463667057, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003006 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:57:38,793] Trial 247 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 130, 'learning_rate': 0.007551767729137049, 'n_estimators': 300, 'subsample': 0.9015523605022961, 'colsample_bytree': 0.9799297016010142, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001797 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:57:55,960] Trial 248 finished with value: 0.620105009844673 and parameters: {'num_leaves': 141, 'learning_rate': 0.010860797483240715, 'n_estimators': 300, 'subsample': 0.9241654791792931, 'colsample_bytree': 0.9715338357950474, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001708 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:58:13,613] Trial 249 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 135, 'learning_rate': 0.011684016936529172, 'n_estimators': 300, 'subsample': 0.8857247607870802, 'colsample_bytree': 0.9525090148967087, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001743 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:58:30,683] Trial 250 finished with value: 0.6204331656092759 and parameters: {'num_leaves': 127, 'learning_rate': 0.008262107912848265, 'n_estimators': 300, 'subsample': 0.9137019050715465, 'colsample_bytree': 0.9632272559781528, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001713 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:58:53,973] Trial 251 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 127, 'learning_rate': 0.007454812237364382, 'n_estimators': 400, 'subsample': 0.9140339121547295, 'colsample_bytree': 0.9825738837143771, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002001 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:59:11,159] Trial 252 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 128, 'learning_rate': 0.008209047188158729, 'n_estimators': 300, 'subsample': 0.9072938383123464, 'colsample_bytree': 0.9739771585920528, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001992 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:59:28,808] Trial 253 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 131, 'learning_rate': 0.008016800979133982, 'n_estimators': 300, 'subsample': 0.8940069144996576, 'colsample_bytree': 0.9680547417538454, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001720 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:59:45,944] Trial 254 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 142, 'learning_rate': 0.01240756156676682, 'n_estimators': 300, 'subsample': 0.9179207487748074, 'colsample_bytree': 0.9562281629980809, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002827 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 05:59:55,714] Trial 255 finished with value: 0.5750382848392037 and parameters: {'num_leaves': 28, 'learning_rate': 0.00836438729526878, 'n_estimators': 300, 'subsample': 0.9021529145903021, 'colsample_bytree': 0.7391828448243103, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001758 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:00:13,630] Trial 256 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 138, 'learning_rate': 0.009464646307895543, 'n_estimators': 300, 'subsample': 0.892676302732533, 'colsample_bytree': 0.9833647574050173, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001761 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:00:32,122] Trial 257 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 132, 'learning_rate': 0.01106945918076694, 'n_estimators': 300, 'subsample': 0.9107854330968052, 'colsample_bytree': 0.9647877293402576, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002028 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:00:50,010] Trial 258 finished with value: 0.6131043535331437 and parameters: {'num_leaves': 60, 'learning_rate': 0.008731247247857347, 'n_estimators': 400, 'subsample': 0.8815140954337971, 'colsample_bytree': 0.9465588044932081, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001778 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:01:11,707] Trial 259 finished with value: 0.6176985342375848 and parameters: {'num_leaves': 143, 'learning_rate': 0.011987523412175571, 'n_estimators': 300, 'subsample': 0.9170118457933394, 'colsample_bytree': 0.9748888104852856, 'min_child_samples': 40}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001725 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:01:23,548] Trial 260 finished with value: 0.6171516079632465 and parameters: {'num_leaves': 140, 'learning_rate': 0.010307590877608748, 'n_estimators': 200, 'subsample': 0.9329670356990692, 'colsample_bytree': 0.9844753166134231, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001728 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:01:34,861] Trial 261 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 128, 'learning_rate': 0.014423252908648898, 'n_estimators': 200, 'subsample': 0.7875933324512951, 'colsample_bytree': 0.9584828644533575, 'min_child_samples': 12}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003046 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:01:52,943] Trial 262 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 137, 'learning_rate': 0.012521849716286414, 'n_estimators': 300, 'subsample': 0.8999532517328664, 'colsample_bytree': 0.9701063221556847, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001773 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:02:05,007] Trial 263 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 134, 'learning_rate': 0.011394646447960732, 'n_estimators': 200, 'subsample': 0.816617752277008, 'colsample_bytree': 0.9380991155691486, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001732 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:02:17,453] Trial 264 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 143, 'learning_rate': 0.008946532009585846, 'n_estimators': 200, 'subsample': 0.8852115592732415, 'colsample_bytree': 0.9794517420058179, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:02:35,102] Trial 265 finished with value: 0.618245460511923 and parameters: {'num_leaves': 130, 'learning_rate': 0.01370464205743206, 'n_estimators': 300, 'subsample': 0.9095622562652002, 'colsample_bytree': 0.9505654676514119, 'min_child_samples': 26}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001776 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:02:53,645] Trial 266 finished with value: 0.620105009844673 and parameters: {'num_leaves': 140, 'learning_rate': 0.010771429203629952, 'n_estimators': 300, 'subsample': 0.9193518347356828, 'colsample_bytree': 0.9654027923442491, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001839 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:03:04,115] Trial 267 finished with value: 0.572194268212645 and parameters: {'num_leaves': 125, 'learning_rate': 0.009812595140598693, 'n_estimators': 200, 'subsample': 0.8051350521768874, 'colsample_bytree': 0.8244728738725978, 'min_child_samples': 32}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002891 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:03:15,728] Trial 268 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 138, 'learning_rate': 0.012965670486069187, 'n_estimators': 200, 'subsample': 0.7454946547275888, 'colsample_bytree': 0.9768309422095115, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002840 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:03:33,390] Trial 269 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 132, 'learning_rate': 0.01180793286185945, 'n_estimators': 300, 'subsample': 0.7768722142986905, 'colsample_bytree': 0.98672546551637, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001771 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:03:50,306] Trial 270 finished with value: 0.6211988623933494 and parameters: {'num_leaves': 135, 'learning_rate': 0.015182064512198131, 'n_estimators': 300, 'subsample': 0.9310392499964455, 'colsample_bytree': 0.9599950064443791, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001962 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:04:08,487] Trial 271 finished with value: 0.6168234521986437 and parameters: {'num_leaves': 144, 'learning_rate': 0.01539262254515124, 'n_estimators': 300, 'subsample': 0.9436703898825052, 'colsample_bytree': 0.9489167580356858, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001713 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:04:25,428] Trial 272 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 136, 'learning_rate': 0.01480099323906637, 'n_estimators': 300, 'subsample': 0.9316900264575948, 'colsample_bytree': 0.9604726119982336, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001695 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:04:42,553] Trial 273 finished with value: 0.6169328374535112 and parameters: {'num_leaves': 141, 'learning_rate': 0.016318274355589597, 'n_estimators': 300, 'subsample': 0.9279713634823901, 'colsample_bytree': 0.9552042492235235, 'min_child_samples': 12}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001791 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:05:06,352] Trial 274 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 129, 'learning_rate': 0.008068291474572569, 'n_estimators': 400, 'subsample': 0.890634024961142, 'colsample_bytree': 0.9660330168550452, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001758 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:05:24,851] Trial 275 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 145, 'learning_rate': 0.007340430796651217, 'n_estimators': 300, 'subsample': 0.9384808813121893, 'colsample_bytree': 0.9357820927563312, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001823 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:05:40,683] Trial 276 finished with value: 0.6132137387880113 and parameters: {'num_leaves': 99, 'learning_rate': 0.0065284359880888095, 'n_estimators': 300, 'subsample': 0.8689682003171634, 'colsample_bytree': 0.9695828169694407, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001710 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:05:58,261] Trial 277 finished with value: 0.61977685408007 and parameters: {'num_leaves': 136, 'learning_rate': 0.009279258750778051, 'n_estimators': 300, 'subsample': 0.8977039861930719, 'colsample_bytree': 0.9889587525132769, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001814 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:06:21,028] Trial 278 finished with value: 0.6145263618464231 and parameters: {'num_leaves': 139, 'learning_rate': 0.017204993186115767, 'n_estimators': 400, 'subsample': 0.9208090232354342, 'colsample_bytree': 0.9585182652859371, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001883 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:06:37,939] Trial 279 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 131, 'learning_rate': 0.013993658735733732, 'n_estimators': 300, 'subsample': 0.9035556722524983, 'colsample_bytree': 0.9463643690656942, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002995 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:06:54,770] Trial 280 finished with value: 0.618245460511923 and parameters: {'num_leaves': 126, 'learning_rate': 0.015245713402440499, 'n_estimators': 300, 'subsample': 0.8784557985000736, 'colsample_bytree': 0.9726214453777069, 'min_child_samples': 7}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:07:12,323] Trial 281 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 142, 'learning_rate': 0.010340111647012111, 'n_estimators': 300, 'subsample': 0.9248866837126292, 'colsample_bytree': 0.9837436167028922, 'min_child_samples': 5}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001707 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:07:30,133] Trial 282 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 134, 'learning_rate': 0.008433383483805863, 'n_estimators': 300, 'subsample': 0.8874557885810925, 'colsample_bytree': 0.9633015296084747, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003098 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:07:42,082] Trial 283 finished with value: 0.618573616276526 and parameters: {'num_leaves': 145, 'learning_rate': 0.00940039578195343, 'n_estimators': 200, 'subsample': 0.7230344850686117, 'colsample_bytree': 0.9746916093558681, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002881 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:07:53,875] Trial 284 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 137, 'learning_rate': 0.010931148148353971, 'n_estimators': 200, 'subsample': 0.9143003327832732, 'colsample_bytree': 0.9550880840845978, 'min_child_samples': 19}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001721 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:08:12,922] Trial 285 finished with value: 0.618245460511923 and parameters: {'num_leaves': 150, 'learning_rate': 0.010030503385457308, 'n_estimators': 300, 'subsample': 0.9042141108024954, 'colsample_bytree': 0.9893655505755474, 'min_child_samples': 6}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001712 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:08:30,296] Trial 286 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 132, 'learning_rate': 0.008930273765940579, 'n_estimators': 300, 'subsample': 0.8946308062658713, 'colsample_bytree': 0.9659082782431238, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001695 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:08:48,370] Trial 287 finished with value: 0.6207613213738788 and parameters: {'num_leaves': 140, 'learning_rate': 0.012874955160565473, 'n_estimators': 300, 'subsample': 0.9333631630224943, 'colsample_bytree': 0.9799961107246248, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001960 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:09:05,507] Trial 288 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 139, 'learning_rate': 0.013228194717813446, 'n_estimators': 300, 'subsample': 0.9313576286887647, 'colsample_bytree': 0.945316884711212, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001758 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:09:22,619] Trial 289 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 135, 'learning_rate': 0.013931667547275125, 'n_estimators': 300, 'subsample': 0.9467411946515554, 'colsample_bytree': 0.9776140311500326, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001903 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:09:41,279] Trial 290 finished with value: 0.6205425508641436 and parameters: {'num_leaves': 140, 'learning_rate': 0.0077454929167171315, 'n_estimators': 300, 'subsample': 0.9378599152303835, 'colsample_bytree': 0.9632495199954255, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001707 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:10:05,573] Trial 291 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 138, 'learning_rate': 0.007094111299188914, 'n_estimators': 400, 'subsample': 0.9292511242108769, 'colsample_bytree': 0.9548155542151484, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002253 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:10:23,437] Trial 292 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 135, 'learning_rate': 0.007580094676075498, 'n_estimators': 300, 'subsample': 0.9430084127637627, 'colsample_bytree': 0.9611266675076571, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001714 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:10:37,374] Trial 293 finished with value: 0.610588492671188 and parameters: {'num_leaves': 75, 'learning_rate': 0.007819957867323797, 'n_estimators': 300, 'subsample': 0.9427415192258425, 'colsample_bytree': 0.9423484525876438, 'min_child_samples': 13}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001766 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:10:55,424] Trial 294 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 129, 'learning_rate': 0.008196527535401135, 'n_estimators': 300, 'subsample': 0.9340598696229298, 'colsample_bytree': 0.967621081166745, 'min_child_samples': 10}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:11:12,598] Trial 295 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 140, 'learning_rate': 0.016414099414266508, 'n_estimators': 300, 'subsample': 0.9220167081561089, 'colsample_bytree': 0.9530287791066543, 'min_child_samples': 9}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001722 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:11:29,289] Trial 296 finished with value: 0.6123386567490702 and parameters: {'num_leaves': 133, 'learning_rate': 0.027156590460953207, 'n_estimators': 300, 'subsample': 0.9367570973786455, 'colsample_bytree': 0.9615629211231538, 'min_child_samples': 29}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002851 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:11:47,219] Trial 297 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 127, 'learning_rate': 0.008697734107257543, 'n_estimators': 300, 'subsample': 0.8024227945745975, 'colsample_bytree': 0.9721989491025373, 'min_child_samples': 12}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001741 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:12:09,417] Trial 298 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 137, 'learning_rate': 0.014513188325944454, 'n_estimators': 400, 'subsample': 0.9161193301351968, 'colsample_bytree': 0.9489502154854677, 'min_child_samples': 8}. Best is trial 163 with value: 0.6217457886676876.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002942 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 06:12:27,920] Trial 299 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 141, 'learning_rate': 0.007040074251931813, 'n_estimators': 300, 'subsample': 0.9524862747444465, 'colsample_bytree': 0.9807910148544535, 'min_child_samples': 11}. Best is trial 163 with value: 0.6217457886676876.\n","[I 2025-03-13 06:12:27,924] A new study created in memory with name: no-name-97c0d455-e493-4a3c-aead-3cccceef8156\n","[I 2025-03-13 06:12:55,620] Trial 0 finished with value: 0.600415663968497 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.017418569473918087, 'subsample': 0.8879430298002999, 'colsample_bytree': 0.7029981147965911, 'gamma': 0.2914200179079851, 'min_child_weight': 3}. Best is trial 0 with value: 0.600415663968497.\n","[I 2025-03-13 06:13:17,155] Trial 1 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.019983880650561756, 'subsample': 0.7295343367647167, 'colsample_bytree': 0.880706376983437, 'gamma': 0.1046100155148141, 'min_child_weight': 3}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:13:28,547] Trial 2 finished with value: 0.5973528768322031 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.019385836899034732, 'subsample': 0.64527934306361, 'colsample_bytree': 0.7802513452555377, 'gamma': 0.23033986258252415, 'min_child_weight': 1}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:14:02,697] Trial 3 finished with value: 0.5977904178516736 and parameters: {'n_estimators': 500, 'max_depth': 11, 'learning_rate': 0.020330516171222463, 'subsample': 0.741950116999597, 'colsample_bytree': 0.8753229260904889, 'gamma': 0.23006674871663776, 'min_child_weight': 1}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:14:15,464] Trial 4 finished with value: 0.5965871800481295 and parameters: {'n_estimators': 200, 'max_depth': 11, 'learning_rate': 0.05095622738951463, 'subsample': 0.7957064498896909, 'colsample_bytree': 0.696844903885759, 'gamma': 0.10662290593213906, 'min_child_weight': 5}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:14:25,379] Trial 5 finished with value: 0.5831327936994093 and parameters: {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.05627564805028222, 'subsample': 0.6442339274203807, 'colsample_bytree': 0.8079878081750074, 'gamma': 0.26588951439120123, 'min_child_weight': 2}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:15:11,356] Trial 6 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 900, 'max_depth': 9, 'learning_rate': 0.013291691836814636, 'subsample': 0.8358887586730214, 'colsample_bytree': 0.8420385873406424, 'gamma': 0.061236799681883766, 'min_child_weight': 5}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:15:41,228] Trial 7 finished with value: 0.6041347626339969 and parameters: {'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.05065416710351992, 'subsample': 0.6367236831377014, 'colsample_bytree': 0.9710638469483454, 'gamma': 0.004620337768845684, 'min_child_weight': 6}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:15:58,077] Trial 8 finished with value: 0.5793043097790418 and parameters: {'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.01583266033128085, 'subsample': 0.9840322177947669, 'colsample_bytree': 0.957278142794814, 'gamma': 0.09300630843882453, 'min_child_weight': 5}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:16:14,459] Trial 9 finished with value: 0.5904616057755414 and parameters: {'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.030200896910400774, 'subsample': 0.8124352686685934, 'colsample_bytree': 0.7448671261133019, 'gamma': 0.0011290417155891519, 'min_child_weight': 2}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:16:18,018] Trial 10 finished with value: 0.5037190986654999 and parameters: {'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.006118134708647001, 'subsample': 0.7161962638839079, 'colsample_bytree': 0.6369736351399551, 'gamma': 0.1688781144473508, 'min_child_weight': 3}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:17:10,329] Trial 11 finished with value: 0.6032596805950557 and parameters: {'n_estimators': 1000, 'max_depth': 9, 'learning_rate': 0.008659694412596135, 'subsample': 0.8759132793832931, 'colsample_bytree': 0.8820541769407145, 'gamma': 0.07088266762625266, 'min_child_weight': 4}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:17:43,494] Trial 12 finished with value: 0.603478451104791 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.010380321487920499, 'subsample': 0.737813603093457, 'colsample_bytree': 0.864979484828388, 'gamma': 0.14488402495295483, 'min_child_weight': 4}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:18:19,122] Trial 13 finished with value: 0.5783198424852329 and parameters: {'n_estimators': 800, 'max_depth': 9, 'learning_rate': 0.09962707634911339, 'subsample': 0.8845896675160368, 'colsample_bytree': 0.8274832405502791, 'gamma': 0.04518520334435935, 'min_child_weight': 6}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:18:49,943] Trial 14 finished with value: 0.5970247210676001 and parameters: {'n_estimators': 400, 'max_depth': 12, 'learning_rate': 0.009475829944399772, 'subsample': 0.815732220308476, 'colsample_bytree': 0.92855530671551, 'gamma': 0.14154737592505845, 'min_child_weight': 5}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:19:24,051] Trial 15 finished with value: 0.5998687376941588 and parameters: {'n_estimators': 800, 'max_depth': 8, 'learning_rate': 0.031889647932397264, 'subsample': 0.9586719234233652, 'colsample_bytree': 0.9119272218407615, 'gamma': 0.045111562165586525, 'min_child_weight': 4}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:19:49,382] Trial 16 finished with value: 0.5980091883614089 and parameters: {'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.013179919313485089, 'subsample': 0.6938982067181946, 'colsample_bytree': 0.8388455096065922, 'gamma': 0.1894372630623039, 'min_child_weight': 3}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:20:14,214] Trial 17 finished with value: 0.5986654998906148 and parameters: {'n_estimators': 400, 'max_depth': 10, 'learning_rate': 0.029000844491794155, 'subsample': 0.7830412447618723, 'colsample_bytree': 0.9965808637306479, 'gamma': 0.11848488860268966, 'min_child_weight': 2}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:20:56,204] Trial 18 finished with value: 0.600415663968497 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.005599130993835969, 'subsample': 0.8418654673842169, 'colsample_bytree': 0.7694505152531318, 'gamma': 0.05992373755896317, 'min_child_weight': 5}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:21:17,276] Trial 19 finished with value: 0.5970247210676001 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01266127979802149, 'subsample': 0.9327439110431505, 'colsample_bytree': 0.9221989906180992, 'gamma': 0.08740352385323198, 'min_child_weight': 6}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:21:28,537] Trial 20 finished with value: 0.5945088602056443 and parameters: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.02591920886331286, 'subsample': 0.677073902296142, 'colsample_bytree': 0.7270556270518691, 'gamma': 0.0303721568573079, 'min_child_weight': 4}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:21:58,343] Trial 21 finished with value: 0.6032596805950557 and parameters: {'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.04078776660782631, 'subsample': 0.6057754751243114, 'colsample_bytree': 0.9819144581756712, 'gamma': 0.009098686772558186, 'min_child_weight': 6}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:22:27,266] Trial 22 finished with value: 0.6000875082038941 and parameters: {'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.08459706383956467, 'subsample': 0.7542562605100622, 'colsample_bytree': 0.9438040796358312, 'gamma': 0.0265344618400611, 'min_child_weight': 6}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:23:02,086] Trial 23 finished with value: 0.5977904178516736 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.06504538064086303, 'subsample': 0.6120592472877417, 'colsample_bytree': 0.8962117238237396, 'gamma': 0.07196511746092185, 'min_child_weight': 5}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:23:24,175] Trial 24 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 700, 'max_depth': 5, 'learning_rate': 0.03962570942968125, 'subsample': 0.6607398231708965, 'colsample_bytree': 0.8540858482496209, 'gamma': 0.02446599650703739, 'min_child_weight': 6}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:24:03,005] Trial 25 finished with value: 0.589367753226865 and parameters: {'n_estimators': 700, 'max_depth': 10, 'learning_rate': 0.035396635850979435, 'subsample': 0.6925831296371637, 'colsample_bytree': 0.8474366849882239, 'gamma': 0.1205411041237193, 'min_child_weight': 3}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:24:21,014] Trial 26 finished with value: 0.5895865237366003 and parameters: {'n_estimators': 700, 'max_depth': 4, 'learning_rate': 0.024549850477468566, 'subsample': 0.8537865818185064, 'colsample_bytree': 0.8106809846764697, 'gamma': 0.08388457637454107, 'min_child_weight': 5}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:24:44,213] Trial 27 finished with value: 0.5578647998249836 and parameters: {'n_estimators': 900, 'max_depth': 3, 'learning_rate': 0.007742665786347971, 'subsample': 0.6791986933563936, 'colsample_bytree': 0.8557374341471132, 'gamma': 0.04982539921371838, 'min_child_weight': 4}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:25:06,690] Trial 28 finished with value: 0.5951651717348502 and parameters: {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.014149780206451695, 'subsample': 0.7125527922108115, 'colsample_bytree': 0.7836937269671544, 'gamma': 0.02793046040311685, 'min_child_weight': 6}. Best is trial 1 with value: 0.6045723036534675.\n","[I 2025-03-13 06:25:35,022] Trial 29 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 700, 'max_depth': 7, 'learning_rate': 0.017221722989476215, 'subsample': 0.7688511100857214, 'colsample_bytree': 0.8927020551286889, 'gamma': 0.10666261880782113, 'min_child_weight': 2}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:25:56,013] Trial 30 finished with value: 0.6020564427915117 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01712154860085853, 'subsample': 0.7728449280531431, 'colsample_bytree': 0.9021110502637318, 'gamma': 0.1700715552497402, 'min_child_weight': 2}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:26:32,144] Trial 31 finished with value: 0.6000875082038941 and parameters: {'n_estimators': 700, 'max_depth': 9, 'learning_rate': 0.02246976049111039, 'subsample': 0.831730078515687, 'colsample_bytree': 0.826726364717595, 'gamma': 0.10803783405724396, 'min_child_weight': 2}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:27:00,395] Trial 32 finished with value: 0.6033690658499234 and parameters: {'n_estimators': 600, 'max_depth': 8, 'learning_rate': 0.011297001717182363, 'subsample': 0.7686350445404154, 'colsample_bytree': 0.8831568426571275, 'gamma': 0.06627128910715799, 'min_child_weight': 1}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:27:40,535] Trial 33 finished with value: 0.5980091883614089 and parameters: {'n_estimators': 700, 'max_depth': 10, 'learning_rate': 0.018871842520583368, 'subsample': 0.7260708798017458, 'colsample_bytree': 0.8623158124221869, 'gamma': 0.13262181441726234, 'min_child_weight': 3}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:28:08,941] Trial 34 finished with value: 0.6030409100853205 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.04002804983468888, 'subsample': 0.6605897516857699, 'colsample_bytree': 0.7927334119905397, 'gamma': 0.09435991736707244, 'min_child_weight': 1}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:28:34,317] Trial 35 finished with value: 0.601947057536644 and parameters: {'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.021376597012934154, 'subsample': 0.9056745981372913, 'colsample_bytree': 0.9324147793071647, 'gamma': 0.16176369322518663, 'min_child_weight': 3}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:28:43,586] Trial 36 finished with value: 0.5820389411507328 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01816613467825474, 'subsample': 0.797324870384806, 'colsample_bytree': 0.8901219210098246, 'gamma': 0.10575497329507685, 'min_child_weight': 2}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:29:38,439] Trial 37 finished with value: 0.5959308685189236 and parameters: {'n_estimators': 800, 'max_depth': 11, 'learning_rate': 0.015538828624868406, 'subsample': 0.7535564347413267, 'colsample_bytree': 0.8210275825383944, 'gamma': 0.22449813472045116, 'min_child_weight': 1}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:30:11,970] Trial 38 finished with value: 0.5999781229490265 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.0444657270786553, 'subsample': 0.6384943953251964, 'colsample_bytree': 0.6750845244105261, 'gamma': 0.271492181579446, 'min_child_weight': 3}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:30:40,117] Trial 39 finished with value: 0.5987748851454824 and parameters: {'n_estimators': 600, 'max_depth': 8, 'learning_rate': 0.007618807191937036, 'subsample': 0.8634774686384235, 'colsample_bytree': 0.7624879325248975, 'gamma': 0.017468308583611314, 'min_child_weight': 5}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:30:49,341] Trial 40 finished with value: 0.5369722161452636 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.06118811496939638, 'subsample': 0.814539439343222, 'colsample_bytree': 0.6015179331037648, 'gamma': 0.20062358064177202, 'min_child_weight': 2}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:31:16,681] Trial 41 finished with value: 0.5962590242835266 and parameters: {'n_estimators': 1000, 'max_depth': 4, 'learning_rate': 0.02561343048006203, 'subsample': 0.6296465931623108, 'colsample_bytree': 0.9686558052866379, 'gamma': 0.0002156285501226131, 'min_child_weight': 6}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:31:48,222] Trial 42 finished with value: 0.6041347626339969 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.05043348736625834, 'subsample': 0.6562232836196793, 'colsample_bytree': 0.9591832681338798, 'gamma': 0.03664711997573626, 'min_child_weight': 6}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:32:02,643] Trial 43 finished with value: 0.595821483264056 and parameters: {'n_estimators': 600, 'max_depth': 3, 'learning_rate': 0.08018586336610888, 'subsample': 0.6250259975065461, 'colsample_bytree': 0.872481416137588, 'gamma': 0.056977821434362697, 'min_child_weight': 5}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:32:29,500] Trial 44 finished with value: 0.6000875082038941 and parameters: {'n_estimators': 1000, 'max_depth': 4, 'learning_rate': 0.03415956820805527, 'subsample': 0.7039730819901551, 'colsample_bytree': 0.8423712235960698, 'gamma': 0.07829493934577947, 'min_child_weight': 6}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:32:42,273] Trial 45 finished with value: 0.5833515642091446 and parameters: {'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.015261896588180827, 'subsample': 0.7328562001269027, 'colsample_bytree': 0.9082056084804307, 'gamma': 0.01708975653355576, 'min_child_weight': 5}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:33:15,315] Trial 46 finished with value: 0.6044629183985999 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.011151120291024772, 'subsample': 0.658433971093457, 'colsample_bytree': 0.9393516356173298, 'gamma': 0.04316590704845898, 'min_child_weight': 6}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:33:46,850] Trial 47 finished with value: 0.6044629183985999 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.011606199054035602, 'subsample': 0.6726746716055466, 'colsample_bytree': 0.9457689638792821, 'gamma': 0.0418633185990331, 'min_child_weight': 4}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:34:19,634] Trial 48 finished with value: 0.6028221395755852 and parameters: {'n_estimators': 700, 'max_depth': 8, 'learning_rate': 0.00943298476269838, 'subsample': 0.7881981404869497, 'colsample_bytree': 0.8692204164326557, 'gamma': 0.10101212404861772, 'min_child_weight': 6}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:34:53,601] Trial 49 finished with value: 0.6020564427915117 and parameters: {'n_estimators': 700, 'max_depth': 9, 'learning_rate': 0.019615102522249562, 'subsample': 0.907175634383246, 'colsample_bytree': 0.9197566039639944, 'gamma': 0.11655521104407031, 'min_child_weight': 4}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:35:22,062] Trial 50 finished with value: 0.6003062787136294 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.013275226477552948, 'subsample': 0.7496019198089128, 'colsample_bytree': 0.8091687883413297, 'gamma': 0.1285740582891829, 'min_child_weight': 5}. Best is trial 29 with value: 0.6047910741632028.\n","[I 2025-03-13 06:35:53,495] Trial 51 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.012571840957634528, 'subsample': 0.6689214128968254, 'colsample_bytree': 0.9463550961519824, 'gamma': 0.04098903705018032, 'min_child_weight': 4}. Best is trial 51 with value: 0.6058849267118792.\n","[I 2025-03-13 06:36:30,731] Trial 52 finished with value: 0.605338000437541 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011345652610313846, 'subsample': 0.6549611780956036, 'colsample_bytree': 0.9331957083676238, 'gamma': 0.05500962578429311, 'min_child_weight': 4}. Best is trial 51 with value: 0.6058849267118792.\n","[I 2025-03-13 06:37:10,957] Trial 53 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.016823547412477997, 'subsample': 0.6938491615529498, 'colsample_bytree': 0.8996060982179522, 'gamma': 0.057462150985010986, 'min_child_weight': 4}. Best is trial 53 with value: 0.6061036972216145.\n","[I 2025-03-13 06:37:52,076] Trial 54 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.01648486408931253, 'subsample': 0.6984845497508104, 'colsample_bytree': 0.9831755764340292, 'gamma': 0.05672310706973944, 'min_child_weight': 4}. Best is trial 53 with value: 0.6061036972216145.\n","[I 2025-03-13 06:38:31,903] Trial 55 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.01677801245869474, 'subsample': 0.689902701905905, 'colsample_bytree': 0.989251460247985, 'gamma': 0.0543921799973468, 'min_child_weight': 4}. Best is trial 53 with value: 0.6061036972216145.\n","[I 2025-03-13 06:39:11,663] Trial 56 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.016767972730889896, 'subsample': 0.7012595371208735, 'colsample_bytree': 0.9999770259123621, 'gamma': 0.05461838726580013, 'min_child_weight': 4}. Best is trial 53 with value: 0.6061036972216145.\n","[I 2025-03-13 06:39:51,846] Trial 57 finished with value: 0.605338000437541 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.015135121607285227, 'subsample': 0.6930671900254824, 'colsample_bytree': 0.9970353874107896, 'gamma': 0.058144233092052154, 'min_child_weight': 4}. Best is trial 53 with value: 0.6061036972216145.\n","[I 2025-03-13 06:40:32,148] Trial 58 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.01470609882257143, 'subsample': 0.6852511257806168, 'colsample_bytree': 0.99928404590899, 'gamma': 0.05198258989993302, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:41:18,925] Trial 59 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.009770251186489108, 'subsample': 0.7218070906624438, 'colsample_bytree': 0.9839597518867094, 'gamma': 0.07208244705809244, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:42:04,853] Trial 60 finished with value: 0.6046816889083352 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.009508101272854888, 'subsample': 0.7206177378885477, 'colsample_bytree': 0.9859654933734493, 'gamma': 0.0755642197230918, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:42:50,084] Trial 61 finished with value: 0.6044629183985999 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.008048050241318546, 'subsample': 0.6821026040210494, 'colsample_bytree': 0.9613140213683903, 'gamma': 0.06827780377294153, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:43:37,022] Trial 62 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 900, 'max_depth': 9, 'learning_rate': 0.012056493155164383, 'subsample': 0.6461383264580546, 'colsample_bytree': 0.9757653523756785, 'gamma': 0.050109435871211264, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:44:23,625] Trial 63 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 900, 'max_depth': 9, 'learning_rate': 0.012610652040152128, 'subsample': 0.7140472427275758, 'colsample_bytree': 0.9755906518956008, 'gamma': 0.03445823892405972, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:45:24,246] Trial 64 finished with value: 0.6030409100853205 and parameters: {'n_estimators': 1000, 'max_depth': 10, 'learning_rate': 0.006408259630930457, 'subsample': 0.6727975497786336, 'colsample_bytree': 0.9992766616092511, 'gamma': 0.014336613752474598, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:46:10,979] Trial 65 finished with value: 0.6049004594180705 and parameters: {'n_estimators': 900, 'max_depth': 9, 'learning_rate': 0.01401812155885854, 'subsample': 0.7074450232006768, 'colsample_bytree': 0.9486996837389708, 'gamma': 0.045106096420279115, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:47:03,349] Trial 66 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 1000, 'max_depth': 9, 'learning_rate': 0.010015841492298519, 'subsample': 0.6430838135950592, 'colsample_bytree': 0.9704196812041099, 'gamma': 0.08110624031957961, 'min_child_weight': 3}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:47:45,667] Trial 67 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.008740493756842712, 'subsample': 0.620956166490551, 'colsample_bytree': 0.9853017824073876, 'gamma': 0.09240880723648295, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:48:33,342] Trial 68 finished with value: 0.603478451104791 and parameters: {'n_estimators': 800, 'max_depth': 10, 'learning_rate': 0.006658448262020379, 'subsample': 0.6164645381309871, 'colsample_bytree': 0.954247341311278, 'gamma': 0.09268006145827554, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:49:17,204] Trial 69 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.008676138092587911, 'subsample': 0.607078891180742, 'colsample_bytree': 0.972075626183258, 'gamma': 0.06845293800980971, 'min_child_weight': 3}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:50:11,619] Trial 70 finished with value: 0.6027127543207176 and parameters: {'n_estimators': 1000, 'max_depth': 9, 'learning_rate': 0.005163335817593652, 'subsample': 0.6465063389155573, 'colsample_bytree': 0.9880517425071198, 'gamma': 0.035916562368832144, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:50:52,961] Trial 71 finished with value: 0.605338000437541 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.012205157116166383, 'subsample': 0.6864647347634876, 'colsample_bytree': 0.9880682625549003, 'gamma': 0.050874250036534736, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:51:30,148] Trial 72 finished with value: 0.6035878363596587 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.010439709408980369, 'subsample': 0.670238042607025, 'colsample_bytree': 0.9637865639490939, 'gamma': 0.02478773852027806, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:52:10,944] Trial 73 finished with value: 0.6007438197331 and parameters: {'n_estimators': 800, 'max_depth': 9, 'learning_rate': 0.02193014290393908, 'subsample': 0.625196367806295, 'colsample_bytree': 0.9980265917245915, 'gamma': 0.06719647726203766, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:52:51,365] Trial 74 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.01410358478744462, 'subsample': 0.6871512506262925, 'colsample_bytree': 0.9829756747053605, 'gamma': 0.08779421265901924, 'min_child_weight': 5}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:53:34,953] Trial 75 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.014527673375934955, 'subsample': 0.7404416035516552, 'colsample_bytree': 0.9802229043836079, 'gamma': 0.08929851528842993, 'min_child_weight': 5}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:54:08,781] Trial 76 finished with value: 0.5997593524392912 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.008718376448757264, 'subsample': 0.6471470001398123, 'colsample_bytree': 0.9196415358817723, 'gamma': 0.0831841109970255, 'min_child_weight': 5}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:55:00,120] Trial 77 finished with value: 0.6036972216145263 and parameters: {'n_estimators': 1000, 'max_depth': 9, 'learning_rate': 0.013501748929386827, 'subsample': 0.669802789189142, 'colsample_bytree': 0.9516797692213034, 'gamma': 0.0980818181469913, 'min_child_weight': 3}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:55:39,844] Trial 78 finished with value: 0.6043535331437322 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.018310557797185628, 'subsample': 0.7248150044344412, 'colsample_bytree': 0.9649692353954535, 'gamma': 0.06350007957170994, 'min_child_weight': 4}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:56:21,047] Trial 79 finished with value: 0.6052286151826733 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.010377592176342196, 'subsample': 0.6330231780218257, 'colsample_bytree': 0.977011699973969, 'gamma': 0.07412734707617372, 'min_child_weight': 5}. Best is trial 58 with value: 0.6064318529862175.\n","[I 2025-03-13 06:57:03,752] Trial 80 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011837080219034821, 'subsample': 0.7020896200859535, 'colsample_bytree': 0.938349515919922, 'gamma': 0.04924092287337109, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 06:57:45,426] Trial 81 finished with value: 0.599212426164953 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.007011872480565702, 'subsample': 0.704923427438433, 'colsample_bytree': 0.935830701696734, 'gamma': 0.047560323879567205, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 06:58:26,468] Trial 82 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01250920976866591, 'subsample': 0.6812213649066893, 'colsample_bytree': 0.9610169724017761, 'gamma': 0.03767758153655615, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 06:59:03,581] Trial 83 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01594703189743422, 'subsample': 0.6642146854555271, 'colsample_bytree': 0.9552466797845426, 'gamma': 0.007096856837424344, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 06:59:40,453] Trial 84 finished with value: 0.605338000437541 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.020458996397235377, 'subsample': 0.6636663944891599, 'colsample_bytree': 0.9439663761295696, 'gamma': 0.009136523700440408, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 06:59:43,886] Trial 85 finished with value: 0.5794136950339094 and parameters: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.015769587515739408, 'subsample': 0.6193449698594282, 'colsample_bytree': 0.9273775551699721, 'gamma': 0.029270095324273472, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:00:18,088] Trial 86 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.014009681923686582, 'subsample': 0.6481199286687205, 'colsample_bytree': 0.9041773320551146, 'gamma': 0.016543132127822317, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:00:59,625] Trial 87 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.011653898447927778, 'subsample': 0.6982662675451727, 'colsample_bytree': 0.951664988681459, 'gamma': 0.02287244072648886, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:01:32,506] Trial 88 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.02371886157702212, 'subsample': 0.6329845272693503, 'colsample_bytree': 0.9740228237955587, 'gamma': 0.06150621906981496, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:02:00,673] Trial 89 finished with value: 0.6043535331437322 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.024574628333919813, 'subsample': 0.6372290434028515, 'colsample_bytree': 0.9135151021420755, 'gamma': 0.005408792277259244, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:02:27,516] Trial 90 finished with value: 0.6024939838109823 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.02356664522338556, 'subsample': 0.9900279429618484, 'colsample_bytree': 0.9706240006465575, 'gamma': 0.06277277227586604, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:03:03,483] Trial 91 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.029155120053379502, 'subsample': 0.6660063027900442, 'colsample_bytree': 0.9930040777250028, 'gamma': 0.051139906672601834, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:03:39,431] Trial 92 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.02735107641028985, 'subsample': 0.601332992892267, 'colsample_bytree': 0.9767978284617642, 'gamma': 0.043591196172886794, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:04:19,634] Trial 93 finished with value: 0.6046816889083352 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.01994755558394987, 'subsample': 0.6808325545299507, 'colsample_bytree': 0.9550540746548489, 'gamma': 0.03186262336886808, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:05:16,508] Trial 94 finished with value: 0.5892583679719974 and parameters: {'n_estimators': 800, 'max_depth': 12, 'learning_rate': 0.018026420730295152, 'subsample': 0.6541006319634115, 'colsample_bytree': 0.9403707220377728, 'gamma': 0.11306357845968443, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:05:51,028] Trial 95 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.016517337126394614, 'subsample': 0.6301748508214905, 'colsample_bytree': 0.9997185468918297, 'gamma': 0.08478446345680914, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:06:24,438] Trial 96 finished with value: 0.6028221395755852 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.010850803281295186, 'subsample': 0.630390653850071, 'colsample_bytree': 0.9258372964263379, 'gamma': 0.08770923979767437, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:07:01,135] Trial 97 finished with value: 0.6042441478888646 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.015981558743246985, 'subsample': 0.612879310847776, 'colsample_bytree': 0.9806421555660544, 'gamma': 0.07745595938228231, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:07:33,014] Trial 98 finished with value: 0.6035878363596587 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.015107068290644054, 'subsample': 0.6383170467598736, 'colsample_bytree': 0.9646983236134438, 'gamma': 0.06137050079163032, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:08:14,262] Trial 99 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013233934049540779, 'subsample': 0.649736751469234, 'colsample_bytree': 0.7007337490942396, 'gamma': 0.041396497107143056, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:08:21,577] Trial 100 finished with value: 0.5880551301684533 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.012119641791203175, 'subsample': 0.6210875427788636, 'colsample_bytree': 0.9909505239405685, 'gamma': 0.09472932968894156, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:09:01,848] Trial 101 finished with value: 0.6052286151826733 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.017382188500481385, 'subsample': 0.6628219384292113, 'colsample_bytree': 0.9998297635385008, 'gamma': 0.051811847273891834, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:09:43,472] Trial 102 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.014648031013224387, 'subsample': 0.6902557620588968, 'colsample_bytree': 0.9723792247446026, 'gamma': 0.06279799135664593, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:10:24,286] Trial 103 finished with value: 0.603806606869394 and parameters: {'n_estimators': 800, 'max_depth': 9, 'learning_rate': 0.01623873824036144, 'subsample': 0.677844774147377, 'colsample_bytree': 0.9907214682557406, 'gamma': 0.0839257195613804, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:11:04,134] Trial 104 finished with value: 0.602275213301247 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.02122061931640617, 'subsample': 0.7030528459608878, 'colsample_bytree': 0.9565101705930814, 'gamma': 0.05776349999258532, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:11:44,297] Trial 105 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01367289241582603, 'subsample': 0.7130061611075085, 'colsample_bytree': 0.9824188809517849, 'gamma': 0.07224846920736916, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:12:20,525] Trial 106 finished with value: 0.6039159921242616 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.019038815393344274, 'subsample': 0.7102125446521482, 'colsample_bytree': 0.9425783858213673, 'gamma': 0.07028792022872873, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:13:00,977] Trial 107 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013270152520648759, 'subsample': 0.6401260336809941, 'colsample_bytree': 0.7174810225027126, 'gamma': 0.29996124336452246, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:13:43,309] Trial 108 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011888967492461678, 'subsample': 0.6580270671050509, 'colsample_bytree': 0.9783239884329744, 'gamma': 0.12467925853695973, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:14:24,381] Trial 109 finished with value: 0.6046816889083352 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010779643606690278, 'subsample': 0.6258474917428566, 'colsample_bytree': 0.9668631079322983, 'gamma': 0.12416340930332455, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:15:02,142] Trial 110 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01246461457848947, 'subsample': 0.6543114552674377, 'colsample_bytree': 0.6696898593779891, 'gamma': 0.03795027432406111, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:15:39,621] Trial 111 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01169649894732804, 'subsample': 0.6575165651827545, 'colsample_bytree': 0.6755751531432568, 'gamma': 0.1333555769154553, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:16:16,003] Trial 112 finished with value: 0.6027127543207176 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.009135496534579953, 'subsample': 0.6553308531545291, 'colsample_bytree': 0.677856860071342, 'gamma': 0.14587681382489226, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:16:43,346] Trial 113 finished with value: 0.5347845110479107 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.011864324769502159, 'subsample': 0.6725527498496459, 'colsample_bytree': 0.6569803366006507, 'gamma': 0.16041699965555653, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:17:26,888] Trial 114 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.012927140034292232, 'subsample': 0.6625900753384788, 'colsample_bytree': 0.6831224482704396, 'gamma': 0.11066693608571704, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:18:03,792] Trial 115 finished with value: 0.6028221395755852 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.009994253356551634, 'subsample': 0.6894761195083698, 'colsample_bytree': 0.7535434794379545, 'gamma': 0.13826292904360013, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:18:37,127] Trial 116 finished with value: 0.6006344344782323 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.011581782426020067, 'subsample': 0.6504431646911999, 'colsample_bytree': 0.6694030691277584, 'gamma': 0.15572400964974994, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:19:21,348] Trial 117 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.008036299335751004, 'subsample': 0.6831806303427388, 'colsample_bytree': 0.9464295684324253, 'gamma': 0.17346917023185918, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:19:52,018] Trial 118 finished with value: 0.5323780354408226 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.014708365660228723, 'subsample': 0.6742449942762364, 'colsample_bytree': 0.6454078778785826, 'gamma': 0.13000639006239442, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:20:27,069] Trial 119 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012532998085303355, 'subsample': 0.6665640886113601, 'colsample_bytree': 0.711400277181674, 'gamma': 0.13891782295925814, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:21:00,512] Trial 120 finished with value: 0.5315029534018815 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011051677423379121, 'subsample': 0.6421173653550383, 'colsample_bytree': 0.6153163706155056, 'gamma': 0.1344255057483445, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:21:34,263] Trial 121 finished with value: 0.5323780354408226 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01215372788766679, 'subsample': 0.6575697550711818, 'colsample_bytree': 0.661603023571638, 'gamma': 0.14904801583331873, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:22:18,103] Trial 122 finished with value: 0.5290964777947933 and parameters: {'n_estimators': 900, 'max_depth': 11, 'learning_rate': 0.013997193207906766, 'subsample': 0.666076114150419, 'colsample_bytree': 0.6343855241348053, 'gamma': 0.036576847914275235, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:22:58,137] Trial 123 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.012803227412611654, 'subsample': 0.6345909349106932, 'colsample_bytree': 0.6920278096312519, 'gamma': 0.10312733827946803, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:23:40,663] Trial 124 finished with value: 0.6049004594180705 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010318682668570727, 'subsample': 0.6933426016220999, 'colsample_bytree': 0.7296621945508612, 'gamma': 0.022278020003714448, 'min_child_weight': 2}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:24:17,272] Trial 125 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 800, 'max_depth': 8, 'learning_rate': 0.01262381103935914, 'subsample': 0.6765159918113444, 'colsample_bytree': 0.7861678139699564, 'gamma': 0.12541334309776495, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:24:57,583] Trial 126 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.015243917108942963, 'subsample': 0.6086103594183659, 'colsample_bytree': 0.7113075862963278, 'gamma': 0.12369669837108527, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:25:30,755] Trial 127 finished with value: 0.6031502953401882 and parameters: {'n_estimators': 700, 'max_depth': 8, 'learning_rate': 0.009087249175802871, 'subsample': 0.6453434685328074, 'colsample_bytree': 0.792696491612307, 'gamma': 0.13509887043113056, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:26:04,222] Trial 128 finished with value: 0.530080945088602 and parameters: {'n_estimators': 800, 'max_depth': 8, 'learning_rate': 0.01108137850367586, 'subsample': 0.6851495055564835, 'colsample_bytree': 0.6614513713557744, 'gamma': 0.11561442182871234, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:26:54,333] Trial 129 finished with value: 0.6040253773791293 and parameters: {'n_estimators': 1000, 'max_depth': 9, 'learning_rate': 0.013902572001472338, 'subsample': 0.6561117235336896, 'colsample_bytree': 0.6928484620153692, 'gamma': 0.14123525171719037, 'min_child_weight': 5}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:27:35,645] Trial 130 finished with value: 0.605338000437541 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.012181843311176528, 'subsample': 0.676426995951596, 'colsample_bytree': 0.7718043538497963, 'gamma': 0.12654519064518077, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:28:09,103] Trial 131 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.012729722405511285, 'subsample': 0.6666146278291888, 'colsample_bytree': 0.7377558410333824, 'gamma': 0.04724188108804301, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:28:41,242] Trial 132 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.012840112986941073, 'subsample': 0.6640217784431076, 'colsample_bytree': 0.7440088050312457, 'gamma': 0.11957219633541963, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:29:13,420] Trial 133 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.01285459965312749, 'subsample': 0.6669564126659976, 'colsample_bytree': 0.7424271834821765, 'gamma': 0.11995584750279595, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:29:38,900] Trial 134 finished with value: 0.5945088602056443 and parameters: {'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.011457080230731638, 'subsample': 0.6512825819833452, 'colsample_bytree': 0.7342906227618067, 'gamma': 0.15396296776376525, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:30:12,756] Trial 135 finished with value: 0.6028221395755852 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.010416476241896028, 'subsample': 0.6175897883604062, 'colsample_bytree': 0.7173093424755103, 'gamma': 0.046914749812325365, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:30:44,192] Trial 136 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.012131962605965803, 'subsample': 0.6612503510199299, 'colsample_bytree': 0.7541600497533936, 'gamma': 0.13986765238799337, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:31:13,556] Trial 137 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 700, 'max_depth': 7, 'learning_rate': 0.014578354173692187, 'subsample': 0.6432520787103172, 'colsample_bytree': 0.8195014567440404, 'gamma': 0.10666466389919613, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:31:44,505] Trial 138 finished with value: 0.6021658280463793 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.01338316892118961, 'subsample': 0.9606164160137889, 'colsample_bytree': 0.710266897480996, 'gamma': 0.13094723810664574, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:32:18,049] Trial 139 finished with value: 0.601947057536644 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.009560086786717884, 'subsample': 0.676081363048452, 'colsample_bytree': 0.7703065276365932, 'gamma': 0.029684676799153296, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:32:50,092] Trial 140 finished with value: 0.5913366878144826 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.007389751715397732, 'subsample': 0.6972268725785808, 'colsample_bytree': 0.7465409312862865, 'gamma': 0.050969215622438516, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:33:29,899] Trial 141 finished with value: 0.6052286151826733 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.015899540121103357, 'subsample': 0.6706633761994105, 'colsample_bytree': 0.783623007670647, 'gamma': 0.1125363489928897, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:34:10,953] Trial 142 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.014588795306809562, 'subsample': 0.685715239214117, 'colsample_bytree': 0.7610456223599306, 'gamma': 0.12039803802751142, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:34:45,604] Trial 143 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01269785600459199, 'subsample': 0.6564855859855669, 'colsample_bytree': 0.9761483316369661, 'gamma': 0.2551911258138593, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:35:19,263] Trial 144 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.011514576389698473, 'subsample': 0.6585657190190167, 'colsample_bytree': 0.6832551967503144, 'gamma': 0.038750479422335865, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:35:55,584] Trial 145 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012845503082126319, 'subsample': 0.6346259883475049, 'colsample_bytree': 0.9749136857069555, 'gamma': 0.19122955045348877, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:36:31,301] Trial 146 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.017634669413082955, 'subsample': 0.6522746866235117, 'colsample_bytree': 0.7221596748031684, 'gamma': 0.14439137209678127, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:37:11,477] Trial 147 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01068895567503423, 'subsample': 0.6248125302243321, 'colsample_bytree': 0.7394836093478927, 'gamma': 0.24066597075057, 'min_child_weight': 3}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:37:39,899] Trial 148 finished with value: 0.5996499671844235 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.01343549617584246, 'subsample': 0.6658899639410103, 'colsample_bytree': 0.9607388564288688, 'gamma': 0.05777712135321731, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:38:08,396] Trial 149 finished with value: 0.6026033690658499 and parameters: {'n_estimators': 700, 'max_depth': 7, 'learning_rate': 0.011846618403339134, 'subsample': 0.6442463369421976, 'colsample_bytree': 0.9823910650324474, 'gamma': 0.04531363514329331, 'min_child_weight': 4}. Best is trial 80 with value: 0.6067600087508204.\n","[I 2025-03-13 07:38:45,366] Trial 150 finished with value: 0.6069787792605557 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012431413631357687, 'subsample': 0.6797535523748256, 'colsample_bytree': 0.8045939611659851, 'gamma': 0.21615693426586527, 'min_child_weight': 3}. Best is trial 150 with value: 0.6069787792605557.\n","[I 2025-03-13 07:39:22,004] Trial 151 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012402594105057046, 'subsample': 0.6786861363631037, 'colsample_bytree': 0.7004969908121514, 'gamma': 0.24771723440046906, 'min_child_weight': 3}. Best is trial 150 with value: 0.6069787792605557.\n","[I 2025-03-13 07:39:58,908] Trial 152 finished with value: 0.6068693940056881 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012414718026489125, 'subsample': 0.6796809436350622, 'colsample_bytree': 0.8332977702424537, 'gamma': 0.2592810877782945, 'min_child_weight': 3}. Best is trial 150 with value: 0.6069787792605557.\n","[I 2025-03-13 07:40:36,199] Trial 153 finished with value: 0.6070881645154234 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012886521356580184, 'subsample': 0.6778810018519765, 'colsample_bytree': 0.7938267065302317, 'gamma': 0.25969064252332946, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:41:13,332] Trial 154 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011164165161578888, 'subsample': 0.6967202427283526, 'colsample_bytree': 0.8345065309527707, 'gamma': 0.25946392424184556, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:41:50,688] Trial 155 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013512046857900322, 'subsample': 0.6803315137704721, 'colsample_bytree': 0.7991590305503147, 'gamma': 0.2581654000813371, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:42:28,149] Trial 156 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.015342794871527407, 'subsample': 0.6723454883863244, 'colsample_bytree': 0.7094360257057608, 'gamma': 0.245755489205917, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:43:05,646] Trial 157 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.014064619955973407, 'subsample': 0.6879883746880141, 'colsample_bytree': 0.699520211952799, 'gamma': 0.2813556678137423, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:43:42,937] Trial 158 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012547476040892881, 'subsample': 0.806637211621967, 'colsample_bytree': 0.7000850079246298, 'gamma': 0.2788173907774056, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:44:19,457] Trial 159 finished with value: 0.6018376722817764 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.011635215502570644, 'subsample': 0.6621837790887823, 'colsample_bytree': 0.6920188531707421, 'gamma': 0.2715306716999972, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:44:55,139] Trial 160 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.014199289220067489, 'subsample': 0.6864935975827937, 'colsample_bytree': 0.8515938589171459, 'gamma': 0.21323555268598463, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:45:31,566] Trial 161 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012965240093113551, 'subsample': 0.7024703361752492, 'colsample_bytree': 0.8010957077545541, 'gamma': 0.28176581454883215, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:46:08,413] Trial 162 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013053131546143291, 'subsample': 0.7040972969016646, 'colsample_bytree': 0.6709191998812126, 'gamma': 0.2800696337476831, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:46:45,456] Trial 163 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.010872637049251267, 'subsample': 0.7182827333640496, 'colsample_bytree': 0.8154541826239525, 'gamma': 0.2473872513672799, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:47:25,489] Trial 164 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012202915560927808, 'subsample': 0.6820695678532765, 'colsample_bytree': 0.8011875656481575, 'gamma': 0.2817896693345422, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:48:01,224] Trial 165 finished with value: 0.6052286151826733 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.02650051654171471, 'subsample': 0.6681731617772113, 'colsample_bytree': 0.6825880397350107, 'gamma': 0.25938311606059594, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:48:22,556] Trial 166 finished with value: 0.6009625902428353 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.014066156031089285, 'subsample': 0.6929859893910323, 'colsample_bytree': 0.8243729520480028, 'gamma': 0.29258027202910547, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:49:00,278] Trial 167 finished with value: 0.6041347626339969 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01022679313490605, 'subsample': 0.6774682232403593, 'colsample_bytree': 0.7766271283857145, 'gamma': 0.2542537137008067, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:49:36,727] Trial 168 finished with value: 0.601947057536644 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.012681264050573255, 'subsample': 0.6594316270162559, 'colsample_bytree': 0.808636779786547, 'gamma': 0.2749328551411728, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:50:13,516] Trial 169 finished with value: 0.605338000437541 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011242372628809274, 'subsample': 0.7067863475637096, 'colsample_bytree': 0.7017189561545406, 'gamma': 0.2882783096885777, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:50:45,024] Trial 170 finished with value: 0.6015095165171734 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.013538169656605094, 'subsample': 0.7331615088452805, 'colsample_bytree': 0.8602263071936984, 'gamma': 0.26689397014069477, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:51:24,947] Trial 171 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012197582080071908, 'subsample': 0.6824318682004784, 'colsample_bytree': 0.8799677277688109, 'gamma': 0.28603662768925325, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:52:05,370] Trial 172 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011943733694645386, 'subsample': 0.8266045899547455, 'colsample_bytree': 0.883244341814778, 'gamma': 0.2305293175765591, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:52:40,919] Trial 173 finished with value: 0.5335812732443667 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01160236220565189, 'subsample': 0.6686232782826451, 'colsample_bytree': 0.6488191432191969, 'gamma': 0.23213048813321194, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:53:20,341] Trial 174 finished with value: 0.6046816889083352 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014948764507462011, 'subsample': 0.8358206804269391, 'colsample_bytree': 0.7945963746865604, 'gamma': 0.250327256354549, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:53:56,677] Trial 175 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013015301091614456, 'subsample': 0.6905639225458803, 'colsample_bytree': 0.723735961958128, 'gamma': 0.2375491053876812, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:54:33,437] Trial 176 finished with value: 0.6018376722817764 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.009915642164459651, 'subsample': 0.8467510834860215, 'colsample_bytree': 0.727089929594318, 'gamma': 0.22130212872148347, 'min_child_weight': 3}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:55:13,195] Trial 177 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011966437106135383, 'subsample': 0.8252829172049757, 'colsample_bytree': 0.7204932630117875, 'gamma': 0.24003469256417534, 'min_child_weight': 2}. Best is trial 153 with value: 0.6070881645154234.\n","[I 2025-03-13 07:55:49,823] Trial 178 finished with value: 0.607197549770291 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013855766575381599, 'subsample': 0.6571738863013076, 'colsample_bytree': 0.8340049952635855, 'gamma': 0.23721013929831872, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 07:56:26,352] Trial 179 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013216871866426071, 'subsample': 0.7796379432189536, 'colsample_bytree': 0.8302186287970833, 'gamma': 0.2353501484931679, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 07:57:06,384] Trial 180 finished with value: 0.6049004594180705 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010972227873749974, 'subsample': 0.7835085471691376, 'colsample_bytree': 0.8330142964779995, 'gamma': 0.23001846181601385, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 07:57:42,512] Trial 181 finished with value: 0.6041347626339969 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01301432663722675, 'subsample': 0.8645853258559977, 'colsample_bytree': 0.8474136670042494, 'gamma': 0.23931403535196105, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 07:58:18,279] Trial 182 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.023361422983449818, 'subsample': 0.6535181532034903, 'colsample_bytree': 0.8318942783487818, 'gamma': 0.23454900418990693, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 07:58:53,294] Trial 183 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.0240113777381157, 'subsample': 0.8252628757933743, 'colsample_bytree': 0.8442890679387123, 'gamma': 0.23278013291082542, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 07:59:28,645] Trial 184 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.023610560936829665, 'subsample': 0.6530070365923104, 'colsample_bytree': 0.8307677746766237, 'gamma': 0.24281868741086762, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 08:00:02,854] Trial 185 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.021340043905708487, 'subsample': 0.6705117034116492, 'colsample_bytree': 0.8397727929422004, 'gamma': 0.21877610762683708, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 08:00:38,291] Trial 186 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.022863523103716588, 'subsample': 0.8044675670333307, 'colsample_bytree': 0.8652369315751344, 'gamma': 0.20969552705354152, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 08:01:14,879] Trial 187 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013725618857744032, 'subsample': 0.6622199878891568, 'colsample_bytree': 0.8249638280900383, 'gamma': 0.22774192996554352, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 08:01:55,002] Trial 188 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011926704441543649, 'subsample': 0.6509328026626199, 'colsample_bytree': 0.8182740601645869, 'gamma': 0.26372651256277857, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 08:02:34,091] Trial 189 finished with value: 0.5994311966746882 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.03190969598960886, 'subsample': 0.7648829734179586, 'colsample_bytree': 0.8165113195684249, 'gamma': 0.2362074550520087, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 08:03:15,941] Trial 190 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01149892534126064, 'subsample': 0.7948945659644648, 'colsample_bytree': 0.8363305214844878, 'gamma': 0.26354758760190616, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 08:03:56,059] Trial 191 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01248274342903554, 'subsample': 0.6496381780099161, 'colsample_bytree': 0.8123101750337894, 'gamma': 0.2244252905400757, 'min_child_weight': 3}. Best is trial 178 with value: 0.607197549770291.\n","[I 2025-03-13 08:04:36,910] Trial 192 finished with value: 0.6073069350251586 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01194703004362391, 'subsample': 0.6419136400897885, 'colsample_bytree': 0.8244551794890843, 'gamma': 0.2660069302446137, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:05:16,108] Trial 193 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.02532483502789093, 'subsample': 0.6381186559882549, 'colsample_bytree': 0.8217205138981805, 'gamma': 0.2500676072403301, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:05:56,235] Trial 194 finished with value: 0.6040253773791293 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010769053115977756, 'subsample': 0.9038801190352453, 'colsample_bytree': 0.8308233475484582, 'gamma': 0.2356598272398769, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:06:21,441] Trial 195 finished with value: 0.6000875082038941 and parameters: {'n_estimators': 600, 'max_depth': 7, 'learning_rate': 0.011757013792021572, 'subsample': 0.6441904460499259, 'colsample_bytree': 0.8108451652843759, 'gamma': 0.26359420893699553, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:06:58,805] Trial 196 finished with value: 0.601947057536644 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.012088804247487926, 'subsample': 0.6517443039734234, 'colsample_bytree': 0.8257839570796899, 'gamma': 0.2512376445677972, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:07:39,060] Trial 197 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014630648260724521, 'subsample': 0.6362759776706389, 'colsample_bytree': 0.8402287437097744, 'gamma': 0.24491978656657232, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:08:15,853] Trial 198 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013577528043042441, 'subsample': 0.6757188023833112, 'colsample_bytree': 0.8863296891878424, 'gamma': 0.2680691381414341, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:08:56,567] Trial 199 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013522905396357665, 'subsample': 0.6762362055174457, 'colsample_bytree': 0.802857017381665, 'gamma': 0.2680296680306512, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:09:33,103] Trial 200 finished with value: 0.6042441478888646 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.010612857067878649, 'subsample': 0.673102610548129, 'colsample_bytree': 0.851814557134904, 'gamma': 0.263328032646122, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:10:09,892] Trial 201 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01332512731534818, 'subsample': 0.6566717140719617, 'colsample_bytree': 0.8956527491461447, 'gamma': 0.2725164369721766, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:10:46,930] Trial 202 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012401745756814221, 'subsample': 0.6641948265200485, 'colsample_bytree': 0.7081789690017556, 'gamma': 0.2556767173882568, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:11:24,253] Trial 203 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012187603748359076, 'subsample': 0.6653444714331791, 'colsample_bytree': 0.7083161007608196, 'gamma': 0.25537350717658114, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:12:01,522] Trial 204 finished with value: 0.605338000437541 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01137475207235407, 'subsample': 0.6795957561113555, 'colsample_bytree': 0.8714855003507511, 'gamma': 0.24335468942974428, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:12:38,779] Trial 205 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01230011176927646, 'subsample': 0.6909693235456015, 'colsample_bytree': 0.8898246718145633, 'gamma': 0.23708818598322215, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:13:19,202] Trial 206 finished with value: 0.605338000437541 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011323191638639558, 'subsample': 0.6481085968826816, 'colsample_bytree': 0.7151691996283541, 'gamma': 0.2595701611348335, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:13:52,393] Trial 207 finished with value: 0.6014001312623058 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.012987643821458452, 'subsample': 0.6586882724101528, 'colsample_bytree': 0.9131097975418105, 'gamma': 0.24708139408707172, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:14:29,469] Trial 208 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013838230289316359, 'subsample': 0.6295293499312928, 'colsample_bytree': 0.8188834268041053, 'gamma': 0.1799649490000435, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:15:09,078] Trial 209 finished with value: 0.602275213301247 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.028292342991609848, 'subsample': 0.6713131626552504, 'colsample_bytree': 0.6927721570371973, 'gamma': 0.2513802299998527, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:15:46,212] Trial 210 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012020666503243052, 'subsample': 0.6851072405436945, 'colsample_bytree': 0.7291536755737762, 'gamma': 0.21516023897445902, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:16:23,561] Trial 211 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.014186552350763704, 'subsample': 0.64019634524574, 'colsample_bytree': 0.8144332172210299, 'gamma': 0.1763207411727642, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:17:00,395] Trial 212 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013371582587855507, 'subsample': 0.630973183299802, 'colsample_bytree': 0.8211745847196886, 'gamma': 0.20431284864653224, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:17:36,242] Trial 213 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.0128975921965719, 'subsample': 0.6568754808835774, 'colsample_bytree': 0.8279115918232055, 'gamma': 0.19013946268207071, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:18:12,393] Trial 214 finished with value: 0.607197549770291 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.014400618122973158, 'subsample': 0.6266021554942102, 'colsample_bytree': 0.8418043585079835, 'gamma': 0.26696484934103426, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:18:49,407] Trial 215 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.015310285767745405, 'subsample': 0.6463060401643425, 'colsample_bytree': 0.841154497882785, 'gamma': 0.270657432499218, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:19:26,289] Trial 216 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012429259890311694, 'subsample': 0.6653824803522114, 'colsample_bytree': 0.8341341734096974, 'gamma': 0.26528597238734775, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:20:03,460] Trial 217 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012429111498889063, 'subsample': 0.6633434631205062, 'colsample_bytree': 0.8359578506539014, 'gamma': 0.2595174791580405, 'min_child_weight': 2}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:20:40,786] Trial 218 finished with value: 0.603806606869394 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011018256171317064, 'subsample': 0.6515706738598701, 'colsample_bytree': 0.8573503084109495, 'gamma': 0.22399643614465123, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:21:21,192] Trial 219 finished with value: 0.6055567709472763 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010413013539016499, 'subsample': 0.6662907774258422, 'colsample_bytree': 0.8508468310447693, 'gamma': 0.255756856066019, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:21:58,191] Trial 220 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011665688679590735, 'subsample': 0.6392212695542129, 'colsample_bytree': 0.8308413023365983, 'gamma': 0.22928472650299248, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:22:35,158] Trial 221 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01242785893886444, 'subsample': 0.6718889443102555, 'colsample_bytree': 0.8054640042336936, 'gamma': 0.2662363967403842, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:23:11,966] Trial 222 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.014288600813659161, 'subsample': 0.6743124521362818, 'colsample_bytree': 0.8415003194298132, 'gamma': 0.26554358443079207, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:23:48,551] Trial 223 finished with value: 0.6070881645154234 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013239052575005858, 'subsample': 0.6811925352409198, 'colsample_bytree': 0.6683301474881237, 'gamma': 0.2749351566420556, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:24:25,229] Trial 224 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011841790658892732, 'subsample': 0.6917453450448139, 'colsample_bytree': 0.675110302169918, 'gamma': 0.27402011646893126, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:24:57,337] Trial 225 finished with value: 0.533800043754102 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012879108596472243, 'subsample': 0.6829925663463138, 'colsample_bytree': 0.6652169012138729, 'gamma': 0.24251335678267621, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:25:32,760] Trial 226 finished with value: 0.5352220520673813 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.022381348829257842, 'subsample': 0.6537703151876982, 'colsample_bytree': 0.6470665954405116, 'gamma': 0.275195782472811, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:26:09,733] Trial 227 finished with value: 0.605338000437541 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011353832495795827, 'subsample': 0.6629091806788635, 'colsample_bytree': 0.8260092527582655, 'gamma': 0.25017727962343256, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:26:43,249] Trial 228 finished with value: 0.5340188142638372 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012478009321705803, 'subsample': 0.6970781510273107, 'colsample_bytree': 0.6545827681346621, 'gamma': 0.2597882401822369, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:26:50,472] Trial 229 finished with value: 0.5889302122073944 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.013140354033191345, 'subsample': 0.854727172049184, 'colsample_bytree': 0.6790440888173476, 'gamma': 0.25210533577353617, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:27:27,287] Trial 230 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011897540325586977, 'subsample': 0.6790175611380845, 'colsample_bytree': 0.6693997112662615, 'gamma': 0.2327730455403122, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:28:04,671] Trial 231 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011832493099053655, 'subsample': 0.6829505833017679, 'colsample_bytree': 0.6831810225853979, 'gamma': 0.23409703200600146, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:28:41,957] Trial 232 finished with value: 0.6073069350251586 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012448265204608372, 'subsample': 0.6694750410845219, 'colsample_bytree': 0.6696857081674918, 'gamma': 0.2288651060749557, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:29:18,854] Trial 233 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011113330087743048, 'subsample': 0.748926549314029, 'colsample_bytree': 0.6680064295925319, 'gamma': 0.23032116387620494, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:29:35,317] Trial 234 finished with value: 0.594290089695909 and parameters: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.011864321488379395, 'subsample': 0.6802241490620194, 'colsample_bytree': 0.6853296202470857, 'gamma': 0.23357074713608555, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:30:08,509] Trial 235 finished with value: 0.5342375847735725 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013601240347926822, 'subsample': 0.8203534997676921, 'colsample_bytree': 0.6619316526317184, 'gamma': 0.22437664172406363, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:30:41,674] Trial 236 finished with value: 0.602275213301247 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.014648344446140315, 'subsample': 0.6584255829777189, 'colsample_bytree': 0.6672848564347508, 'gamma': 0.23773834293647825, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:31:17,276] Trial 237 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01240654733700407, 'subsample': 0.6741048488875553, 'colsample_bytree': 0.6707853078235757, 'gamma': 0.22732209135101156, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:31:50,415] Trial 238 finished with value: 0.531940494421352 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.010936728492681016, 'subsample': 0.6888334044144468, 'colsample_bytree': 0.6515137082133596, 'gamma': 0.24359748804194817, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:32:24,192] Trial 239 finished with value: 0.5325968059505579 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.013028216796828621, 'subsample': 0.6462567406665977, 'colsample_bytree': 0.6312292086833196, 'gamma': 0.23713161990611137, 'min_child_weight': 5}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:33:04,656] Trial 240 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010024133356884292, 'subsample': 0.6236248774652333, 'colsample_bytree': 0.7049697661659396, 'gamma': 0.26020817182928707, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:33:41,778] Trial 241 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01236872535254995, 'subsample': 0.670450928861119, 'colsample_bytree': 0.6742383862185842, 'gamma': 0.04515066364357415, 'min_child_weight': 3}. Best is trial 192 with value: 0.6073069350251586.\n","[I 2025-03-13 08:34:18,658] Trial 242 finished with value: 0.6074163202800262 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011632930957000652, 'subsample': 0.6674735909843547, 'colsample_bytree': 0.7231291098304975, 'gamma': 0.24733869780560302, 'min_child_weight': 3}. Best is trial 242 with value: 0.6074163202800262.\n","[I 2025-03-13 08:34:50,557] Trial 243 finished with value: 0.5323780354408226 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011547774530250625, 'subsample': 0.6610627806143126, 'colsample_bytree': 0.6589628151301441, 'gamma': 0.22004925162809952, 'min_child_weight': 3}. Best is trial 242 with value: 0.6074163202800262.\n","[I 2025-03-13 08:35:27,293] Trial 244 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011887296073198312, 'subsample': 0.680434930386425, 'colsample_bytree': 0.7216475292991854, 'gamma': 0.2466524523279856, 'min_child_weight': 3}. Best is trial 242 with value: 0.6074163202800262.\n","[I 2025-03-13 08:36:04,066] Trial 245 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01363674234971785, 'subsample': 0.6676622925055522, 'colsample_bytree': 0.6903690501227202, 'gamma': 0.2415711848031415, 'min_child_weight': 3}. Best is trial 242 with value: 0.6074163202800262.\n","[I 2025-03-13 08:36:41,145] Trial 246 finished with value: 0.6043535331437322 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.010908375602342132, 'subsample': 0.656521320269517, 'colsample_bytree': 0.8338100388741736, 'gamma': 0.2569665112475805, 'min_child_weight': 3}. Best is trial 242 with value: 0.6074163202800262.\n","[I 2025-03-13 08:37:17,638] Trial 247 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012691539403020173, 'subsample': 0.6782195192954386, 'colsample_bytree': 0.8107422569797985, 'gamma': 0.2673411647085339, 'min_child_weight': 3}. Best is trial 242 with value: 0.6074163202800262.\n"]}],"source":["# Install required libraries (Uncomment if not installed)\n","# !pip install optuna lightgbm catboost xgboost imbalanced-learn joblib pandas numpy scikit-learn\n","\n","import pandas as pd\n","import numpy as np\n","import optuna\n","import joblib\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler, LabelEncoder\n","from sklearn.impute import KNNImputer\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from imblearn.over_sampling import BorderlineSMOTE\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from catboost import CatBoostClassifier\n","from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","# ✅ 1️⃣ LOAD DATA (Replace with actual dataset)\n","data = pd.read_csv('balanced_fall_detection_dataset.csv')  # Assuming this is your dataset file\n","# Assuming 'Label' is your target column\n","X = data.drop('Label', axis=1)\n","y = data['Label']  # Define y here\n","\n","# ✅ Encode labels\n","le = LabelEncoder()\n","y_encoded = le.fit_transform(y)\n","\n","# ✅ Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y_encoded, test_size=0.15, random_state=42\n",")\n","\n","# ✅ Feature Selection using XGBoost\n","xgb_temp = XGBClassifier(n_estimators=500, random_state=42)\n","xgb_temp.fit(X_train, y_train)\n","feature_importance = xgb_temp.feature_importances_\n","\n","# Keep only the top 20 most important features\n","important_features = X.columns[np.argsort(-feature_importance)[:20]]\n","X_train = X_train[important_features]\n","X_test = X_test[important_features]\n","\n","# ✅ Missing Value Handling using KNN Imputer\n","imputer = KNNImputer(n_neighbors=5)\n","X_train_imputed = imputer.fit_transform(X_train)\n","X_test_imputed = imputer.transform(X_test)\n","\n","# ✅ Robust Scaling (Better than StandardScaler)\n","scaler = RobustScaler()\n","X_train_scaled = scaler.fit_transform(X_train_imputed)\n","X_test_scaled = scaler.transform(X_test_imputed)\n","\n","# ✅ Apply PCA for Noise Reduction\n","max_pca_components = min(15, X_train_scaled.shape[1])  # Use at most 15 or the number of available features\n","pca = PCA(n_components=max_pca_components)\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","# ✅ Use Borderline-SMOTE for better data balancing\n","smote = BorderlineSMOTE(random_state=42, kind='borderline-1')\n","X_resampled, y_resampled = smote.fit_resample(X_train_pca, y_train)\n","\n","# ✅ Hyperparameter Tuning with Optuna (LGBM)\n","def lgbm_objective(trial):\n","    params = {\n","        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n","        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50)\n","    }\n","    model = LGBMClassifier(**params, random_state=42)\n","    model.fit(X_resampled, y_resampled)\n","    y_pred = model.predict(X_test_pca)\n","    return accuracy_score(y_test, y_pred)\n","\n","lgbm_study = optuna.create_study(direction='maximize')\n","lgbm_study.optimize(lgbm_objective, n_trials=300)  # Increased trials for better tuning\n","best_lgbm_params = lgbm_study.best_params\n","\n","# ✅ Hyperparameter Tuning with Optuna (XGBoost)\n","def xgb_objective(trial):\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n","        'max_depth': trial.suggest_int('max_depth', 3, 12),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'gamma': trial.suggest_float('gamma', 0, 0.3),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 1, 6)\n","    }\n","    model = XGBClassifier(**params, random_state=42, eval_metric='mlogloss')\n","    model.fit(X_resampled, y_resampled)\n","    y_pred = model.predict(X_test_pca)\n","    return accuracy_score(y_test, y_pred)\n","\n","xgb_study = optuna.create_study(direction='maximize')\n","xgb_study.optimize(xgb_objective, n_trials=300)\n","best_xgb_params = xgb_study.best_params\n","\n","# ✅ Train Final Models with Optimized Parameters\n","rf_model = RandomForestClassifier(n_estimators=500, random_state=42)\n","xgb_model = XGBClassifier(**best_xgb_params, random_state=42, eval_metric='mlogloss')\n","lgbm_model = LGBMClassifier(**best_lgbm_params, random_state=42)\n","cat_model = CatBoostClassifier(n_estimators=500, depth=8, learning_rate=0.03, verbose=0)\n","\n","# ✅ Stacking Classifier (Better than Voting Classifier)\n","meta_learner = LogisticRegression()\n","\n","stacking_clf = StackingClassifier(\n","    estimators=[\n","        ('rf', rf_model),\n","        ('xgb', xgb_model),\n","        ('lgbm', lgbm_model),\n","        ('cat', cat_model)\n","    ],\n","    final_estimator=meta_learner\n",")\n","\n","# ✅ Train Stacking Classifier\n","stacking_clf.fit(X_resampled, y_resampled)\n","\n","# ✅ Evaluate Model\n","y_pred = stacking_clf.predict(X_test_pca)\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","\n","# ✅ Save Model and Preprocessing Objects\n","joblib.dump(stacking_clf, 'final_stacking_model.pkl')\n","joblib.dump(scaler, 'scaler.pkl')\n","joblib.dump(imputer, 'imputer.pkl')\n","joblib.dump(pca, 'pca.pkl')\n","joblib.dump(le, 'label_encoder.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136993,"status":"ok","timestamp":1741885510793,"user":{"displayName":"abilesha","userId":"15267765829136202170"},"user_tz":-330},"id":"qAq2X85vdWFt","outputId":"fef04b9d-894b-43b3-a38f-554152669533"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.7349 - loss: 0.5266 - val_accuracy: 0.7604 - val_loss: 0.4905\n","Epoch 2/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7939 - loss: 0.4341 - val_accuracy: 0.8026 - val_loss: 0.4133\n","Epoch 3/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8115 - loss: 0.4036 - val_accuracy: 0.8180 - val_loss: 0.3893\n","Epoch 4/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8240 - loss: 0.3821 - val_accuracy: 0.8365 - val_loss: 0.3716\n","Epoch 5/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8336 - loss: 0.3673 - val_accuracy: 0.8468 - val_loss: 0.3619\n","Epoch 6/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8413 - loss: 0.3653 - val_accuracy: 0.8540 - val_loss: 0.3389\n","Epoch 7/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8595 - loss: 0.3310 - val_accuracy: 0.8581 - val_loss: 0.3273\n","Epoch 8/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8715 - loss: 0.3036 - val_accuracy: 0.8658 - val_loss: 0.3194\n","Epoch 9/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.8820 - loss: 0.2884 - val_accuracy: 0.8684 - val_loss: 0.3117\n","Epoch 10/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8538 - loss: 0.3222 - val_accuracy: 0.8715 - val_loss: 0.2974\n","Epoch 11/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8839 - loss: 0.2792 - val_accuracy: 0.8761 - val_loss: 0.2941\n","Epoch 12/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8872 - loss: 0.2653 - val_accuracy: 0.8838 - val_loss: 0.2960\n","Epoch 13/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8927 - loss: 0.2528 - val_accuracy: 0.8802 - val_loss: 0.2810\n","Epoch 14/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8896 - loss: 0.2638 - val_accuracy: 0.8869 - val_loss: 0.2794\n","Epoch 15/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9053 - loss: 0.2290 - val_accuracy: 0.8895 - val_loss: 0.2665\n","Epoch 16/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9160 - loss: 0.2164 - val_accuracy: 0.8915 - val_loss: 0.2613\n","Epoch 17/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9042 - loss: 0.2305 - val_accuracy: 0.9080 - val_loss: 0.2501\n","Epoch 18/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9261 - loss: 0.1958 - val_accuracy: 0.8972 - val_loss: 0.2612\n","Epoch 19/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9045 - loss: 0.2349 - val_accuracy: 0.8853 - val_loss: 0.2754\n","Epoch 20/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9112 - loss: 0.2217 - val_accuracy: 0.8972 - val_loss: 0.2580\n","Epoch 21/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9218 - loss: 0.1933 - val_accuracy: 0.9028 - val_loss: 0.2516\n","Epoch 22/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9272 - loss: 0.1914 - val_accuracy: 0.9075 - val_loss: 0.2505\n","Epoch 23/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9154 - loss: 0.2170 - val_accuracy: 0.9059 - val_loss: 0.2470\n","Epoch 24/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9240 - loss: 0.1873 - val_accuracy: 0.9172 - val_loss: 0.2354\n","Epoch 25/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9291 - loss: 0.1868 - val_accuracy: 0.9121 - val_loss: 0.2417\n","Epoch 26/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9408 - loss: 0.1604 - val_accuracy: 0.9075 - val_loss: 0.2484\n","Epoch 27/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9423 - loss: 0.1555 - val_accuracy: 0.9172 - val_loss: 0.2434\n","Epoch 28/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9323 - loss: 0.1742 - val_accuracy: 0.9116 - val_loss: 0.2354\n","Epoch 29/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9430 - loss: 0.1550 - val_accuracy: 0.9111 - val_loss: 0.2407\n","Epoch 30/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9432 - loss: 0.1506 - val_accuracy: 0.9131 - val_loss: 0.2490\n","Epoch 31/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9449 - loss: 0.1430 - val_accuracy: 0.9131 - val_loss: 0.2529\n","Epoch 32/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9406 - loss: 0.1530 - val_accuracy: 0.9080 - val_loss: 0.2512\n","Epoch 33/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9388 - loss: 0.1519 - val_accuracy: 0.9131 - val_loss: 0.2516\n","Epoch 34/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9479 - loss: 0.1449 - val_accuracy: 0.9054 - val_loss: 0.2545\n","Epoch 35/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9426 - loss: 0.1482 - val_accuracy: 0.9147 - val_loss: 0.2467\n","Epoch 36/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9353 - loss: 0.1743 - val_accuracy: 0.9188 - val_loss: 0.2311\n","Epoch 37/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9510 - loss: 0.1333 - val_accuracy: 0.9188 - val_loss: 0.2477\n","Epoch 38/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9439 - loss: 0.1410 - val_accuracy: 0.9162 - val_loss: 0.2504\n","Epoch 39/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9507 - loss: 0.1322 - val_accuracy: 0.9136 - val_loss: 0.2495\n","Epoch 40/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9385 - loss: 0.1616 - val_accuracy: 0.9152 - val_loss: 0.2340\n","Epoch 41/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9491 - loss: 0.1365 - val_accuracy: 0.9244 - val_loss: 0.2288\n","Epoch 42/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.9446 - loss: 0.1436 - val_accuracy: 0.9157 - val_loss: 0.2567\n","Epoch 43/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9482 - loss: 0.1313 - val_accuracy: 0.9188 - val_loss: 0.2541\n","Epoch 44/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9499 - loss: 0.1379 - val_accuracy: 0.9213 - val_loss: 0.2359\n","Epoch 45/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9512 - loss: 0.1325 - val_accuracy: 0.9265 - val_loss: 0.2373\n","Epoch 46/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9486 - loss: 0.1302 - val_accuracy: 0.9136 - val_loss: 0.2544\n","Epoch 47/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9373 - loss: 0.1629 - val_accuracy: 0.9219 - val_loss: 0.2370\n","Epoch 48/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9531 - loss: 0.1277 - val_accuracy: 0.9244 - val_loss: 0.2517\n","Epoch 49/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9582 - loss: 0.1155 - val_accuracy: 0.9224 - val_loss: 0.2444\n","Epoch 50/50\n","\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9592 - loss: 0.1088 - val_accuracy: 0.9275 - val_loss: 0.2329\n","\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9292 - loss: 0.2149\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["Test Accuracy: 92.75%\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from imblearn.over_sampling import SMOTE\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/cleaned_healthcare_dataset.csv\")\n","df.drop(columns=['id'], inplace=True)  # Remove ID column\n","\n","# Encoding categorical variables\n","encoder = LabelEncoder()\n","categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n","for col in categorical_cols:\n","    df[col] = encoder.fit_transform(df[col])\n","\n","# Normalize numerical features\n","scaler = StandardScaler()\n","numerical_cols = ['age', 'avg_glucose_level', 'bmi']\n","df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n","\n","# Feature selection for sensor data (Assuming sensor columns exist)\n","sensor_cols = ['ECG', 'PPG', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyro_X', 'Gyro_Y', 'Gyro_Z', 'EMG']\n","X = df[sensor_cols] if all(col in df.columns for col in sensor_cols) else df.drop(columns=['stroke'])\n","y = df['stroke']\n","\n","# Handle class imbalance using SMOTE\n","smote = SMOTE(random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X, y)\n","\n","# Reshape for LSTM if time-series data is available\n","X_resampled = np.array(X_resampled).reshape((X_resampled.shape[0], X_resampled.shape[1], 1))\n","\n","# Splitting into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# Build CNN-LSTM Model\n","model = Sequential([\n","    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n","    MaxPooling1D(pool_size=2),\n","    BatchNormalization(),\n","    LSTM(64, return_sequences=True),\n","    LSTM(32),\n","    Dropout(0.3),\n","    Dense(32, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train Model\n","model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n","\n","# Evaluate Model\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Save model for later use\n","model.save(\"stroke_prediction_model.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252763,"status":"ok","timestamp":1741903451400,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"},"user_tz":-330},"id":"SO-YwjsNjFF-","outputId":"1d2f7774-9a1e-4ccb-a7cd-85b315244e6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.7)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.14.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-13 17:32:43,542] A new study created in memory with name: no-name-ff096e77-4955-42d1-a346-babe9b509a5e\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002055 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:33:11,488] Trial 0 finished with value: 0.5899146795012032 and parameters: {'num_leaves': 94, 'learning_rate': 0.013059793370222367, 'n_estimators': 600, 'subsample': 0.6989421290966229, 'colsample_bytree': 0.6534394407258914, 'min_child_samples': 50}. Best is trial 0 with value: 0.5899146795012032.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001709 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:33:40,749] Trial 1 finished with value: 0.6141982060818202 and parameters: {'num_leaves': 76, 'learning_rate': 0.010073678862368696, 'n_estimators': 700, 'subsample': 0.7070510361909101, 'colsample_bytree': 0.9923848637862408, 'min_child_samples': 13}. Best is trial 1 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001720 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:33:54,152] Trial 2 finished with value: 0.5851017282870269 and parameters: {'num_leaves': 20, 'learning_rate': 0.012701449242031686, 'n_estimators': 500, 'subsample': 0.796819620954538, 'colsample_bytree': 0.7778484560979226, 'min_child_samples': 40}. Best is trial 1 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001726 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:34:04,283] Trial 3 finished with value: 0.6100415663968497 and parameters: {'num_leaves': 36, 'learning_rate': 0.022688735865653113, 'n_estimators': 300, 'subsample': 0.6907336367531969, 'colsample_bytree': 0.9893517487972189, 'min_child_samples': 29}. Best is trial 1 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001727 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:34:29,466] Trial 4 finished with value: 0.6119011157295996 and parameters: {'num_leaves': 44, 'learning_rate': 0.027000430240945446, 'n_estimators': 900, 'subsample': 0.9908779536005321, 'colsample_bytree': 0.9477977611454194, 'min_child_samples': 7}. Best is trial 1 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001700 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:34:36,865] Trial 5 finished with value: 0.6065412382410851 and parameters: {'num_leaves': 33, 'learning_rate': 0.01955289646308947, 'n_estimators': 200, 'subsample': 0.60728048598664, 'colsample_bytree': 0.9084459385173255, 'min_child_samples': 48}. Best is trial 1 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001691 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:34:57,010] Trial 6 finished with value: 0.6051192299278058 and parameters: {'num_leaves': 52, 'learning_rate': 0.0620927616683617, 'n_estimators': 700, 'subsample': 0.970001044675763, 'colsample_bytree': 0.8717537320201132, 'min_child_samples': 26}. Best is trial 1 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001863 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:35:18,233] Trial 7 finished with value: 0.5854298840516299 and parameters: {'num_leaves': 50, 'learning_rate': 0.04177550267754427, 'n_estimators': 700, 'subsample': 0.9201809794038833, 'colsample_bytree': 0.6879929787189756, 'min_child_samples': 32}. Best is trial 1 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002615 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:35:31,723] Trial 8 finished with value: 0.5876175891489828 and parameters: {'num_leaves': 24, 'learning_rate': 0.018911849104570885, 'n_estimators': 500, 'subsample': 0.6366688887554671, 'colsample_bytree': 0.6613527498219883, 'min_child_samples': 47}. Best is trial 1 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001712 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:35:44,121] Trial 9 finished with value: 0.6137606650623496 and parameters: {'num_leaves': 81, 'learning_rate': 0.044957070114794453, 'n_estimators': 300, 'subsample': 0.6398687656471872, 'colsample_bytree': 0.8630593002974547, 'min_child_samples': 38}. Best is trial 1 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001669 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:36:41,627] Trial 10 finished with value: 0.5876175891489828 and parameters: {'num_leaves': 137, 'learning_rate': 0.0055568253106797975, 'n_estimators': 1000, 'subsample': 0.8058609329323493, 'colsample_bytree': 0.7824447494995551, 'min_child_samples': 11}. Best is trial 1 with value: 0.6141982060818202.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001687 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:36:45,476] Trial 11 finished with value: 0.6143075913366878 and parameters: {'num_leaves': 81, 'learning_rate': 0.09674566713275022, 'n_estimators': 100, 'subsample': 0.7365829769981201, 'colsample_bytree': 0.8480281964025539, 'min_child_samples': 17}. Best is trial 11 with value: 0.6143075913366878.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001619 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:36:49,570] Trial 12 finished with value: 0.5483482826514986 and parameters: {'num_leaves': 88, 'learning_rate': 0.005951427494853903, 'n_estimators': 100, 'subsample': 0.770829387208269, 'colsample_bytree': 0.8258994037406557, 'min_child_samples': 17}. Best is trial 11 with value: 0.6143075913366878.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001697 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:37:23,654] Trial 13 finished with value: 0.581382629621527 and parameters: {'num_leaves': 110, 'learning_rate': 0.08544500329597, 'n_estimators': 800, 'subsample': 0.7463036664396732, 'colsample_bytree': 0.9968181019477536, 'min_child_samples': 19}. Best is trial 11 with value: 0.6143075913366878.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001640 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:37:39,500] Trial 14 finished with value: 0.5866331218551739 and parameters: {'num_leaves': 68, 'learning_rate': 0.010217810080048666, 'n_estimators': 400, 'subsample': 0.8530782156110751, 'colsample_bytree': 0.7233136519387913, 'min_child_samples': 17}. Best is trial 11 with value: 0.6143075913366878.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001894 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:37:44,867] Trial 15 finished with value: 0.6155108291402319 and parameters: {'num_leaves': 114, 'learning_rate': 0.0960334383279172, 'n_estimators': 100, 'subsample': 0.7107962535546034, 'colsample_bytree': 0.9349431103889256, 'min_child_samples': 11}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001714 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:37:49,598] Trial 16 finished with value: 0.6127761977685408 and parameters: {'num_leaves': 124, 'learning_rate': 0.08950961190698307, 'n_estimators': 100, 'subsample': 0.8406639266417543, 'colsample_bytree': 0.9167154731265776, 'min_child_samples': 22}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:37:58,801] Trial 17 finished with value: 0.6147451323561584 and parameters: {'num_leaves': 103, 'learning_rate': 0.05834337317778951, 'n_estimators': 200, 'subsample': 0.7264741576692313, 'colsample_bytree': 0.8382021631357719, 'min_child_samples': 7}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001634 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:38:13,093] Trial 18 finished with value: 0.5751476700940713 and parameters: {'num_leaves': 149, 'learning_rate': 0.05523891316284767, 'n_estimators': 300, 'subsample': 0.6539372715220174, 'colsample_bytree': 0.7446125244374434, 'min_child_samples': 7}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001630 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:38:22,337] Trial 19 finished with value: 0.586304966090571 and parameters: {'num_leaves': 106, 'learning_rate': 0.03289295229576617, 'n_estimators': 200, 'subsample': 0.8685338248316327, 'colsample_bytree': 0.6072696225935196, 'min_child_samples': 5}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001751 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:38:30,926] Trial 20 finished with value: 0.6120105009844673 and parameters: {'num_leaves': 117, 'learning_rate': 0.06696177029146028, 'n_estimators': 200, 'subsample': 0.7425611198787195, 'colsample_bytree': 0.9148002905624777, 'min_child_samples': 12}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001699 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:38:35,920] Trial 21 finished with value: 0.6127761977685408 and parameters: {'num_leaves': 98, 'learning_rate': 0.09759450511625338, 'n_estimators': 100, 'subsample': 0.71870742539428, 'colsample_bytree': 0.8374360449781796, 'min_child_samples': 10}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001698 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:38:39,679] Trial 22 finished with value: 0.6124480420039379 and parameters: {'num_leaves': 68, 'learning_rate': 0.06897458478116193, 'n_estimators': 100, 'subsample': 0.6822934923088696, 'colsample_bytree': 0.8785941635189477, 'min_child_samples': 14}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001769 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:38:58,614] Trial 23 finished with value: 0.5784292277401006 and parameters: {'num_leaves': 128, 'learning_rate': 0.050376065853846026, 'n_estimators': 400, 'subsample': 0.7566609050936819, 'colsample_bytree': 0.8156642338917858, 'min_child_samples': 22}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003123 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:39:06,603] Trial 24 finished with value: 0.6115729599649967 and parameters: {'num_leaves': 101, 'learning_rate': 0.08099108489719926, 'n_estimators': 200, 'subsample': 0.7948961483967129, 'colsample_bytree': 0.9583085490751597, 'min_child_samples': 10}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001741 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:39:23,749] Trial 25 finished with value: 0.6120105009844673 and parameters: {'num_leaves': 116, 'learning_rate': 0.0380533824642248, 'n_estimators': 400, 'subsample': 0.6643545529259414, 'colsample_bytree': 0.8546328393531752, 'min_child_samples': 5}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003304 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:39:27,845] Trial 26 finished with value: 0.6140888208269525 and parameters: {'num_leaves': 91, 'learning_rate': 0.09998682338037074, 'n_estimators': 100, 'subsample': 0.7251334075682314, 'colsample_bytree': 0.8965668281335626, 'min_child_samples': 16}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001762 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:39:38,149] Trial 27 finished with value: 0.6098227958871144 and parameters: {'num_leaves': 65, 'learning_rate': 0.0739166454660416, 'n_estimators': 300, 'subsample': 0.822943473342234, 'colsample_bytree': 0.9465885297355828, 'min_child_samples': 21}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001743 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:39:47,850] Trial 28 finished with value: 0.5870706628746445 and parameters: {'num_leaves': 142, 'learning_rate': 0.05603608512478906, 'n_estimators': 200, 'subsample': 0.7840311469111396, 'colsample_bytree': 0.7897274108851542, 'min_child_samples': 26}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002754 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:39:52,654] Trial 29 finished with value: 0.589367753226865 and parameters: {'num_leaves': 125, 'learning_rate': 0.07503527275997578, 'n_estimators': 100, 'subsample': 0.6877253208063205, 'colsample_bytree': 0.7471122406142173, 'min_child_samples': 9}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001777 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:40:17,080] Trial 30 finished with value: 0.6120105009844673 and parameters: {'num_leaves': 95, 'learning_rate': 0.034627132551925816, 'n_estimators': 600, 'subsample': 0.721631832334812, 'colsample_bytree': 0.8432526245684582, 'min_child_samples': 16}. Best is trial 15 with value: 0.6155108291402319.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001709 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:40:47,566] Trial 31 finished with value: 0.6157295996499672 and parameters: {'num_leaves': 79, 'learning_rate': 0.007580476494808592, 'n_estimators': 700, 'subsample': 0.701676513295357, 'colsample_bytree': 0.9661544764047831, 'min_child_samples': 13}. Best is trial 31 with value: 0.6157295996499672.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001741 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:41:22,202] Trial 32 finished with value: 0.613323124042879 and parameters: {'num_leaves': 78, 'learning_rate': 0.007838126299485743, 'n_estimators': 800, 'subsample': 0.7078746636392181, 'colsample_bytree': 0.9605195845826181, 'min_child_samples': 14}. Best is trial 31 with value: 0.6157295996499672.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001786 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:41:47,323] Trial 33 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 86, 'learning_rate': 0.014784345296199682, 'n_estimators': 600, 'subsample': 0.7629064149836463, 'colsample_bytree': 0.9311447615059586, 'min_child_samples': 8}. Best is trial 33 with value: 0.6178079194924524.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001702 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:42:14,624] Trial 34 finished with value: 0.616714066943776 and parameters: {'num_leaves': 106, 'learning_rate': 0.014810124244117139, 'n_estimators': 600, 'subsample': 0.7688792305249081, 'colsample_bytree': 0.9336717414441851, 'min_child_samples': 7}. Best is trial 33 with value: 0.6178079194924524.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001287 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:42:43,735] Trial 35 finished with value: 0.6136512798074819 and parameters: {'num_leaves': 112, 'learning_rate': 0.01452845337130885, 'n_estimators': 600, 'subsample': 0.7733676093144602, 'colsample_bytree': 0.9724349715585148, 'min_child_samples': 13}. Best is trial 33 with value: 0.6178079194924524.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003490 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:43:02,621] Trial 36 finished with value: 0.6139794355720849 and parameters: {'num_leaves': 59, 'learning_rate': 0.014612762667672035, 'n_estimators': 500, 'subsample': 0.883927571828099, 'colsample_bytree': 0.9275243757130751, 'min_child_samples': 8}. Best is trial 33 with value: 0.6178079194924524.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001867 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:43:36,770] Trial 37 finished with value: 0.6140888208269525 and parameters: {'num_leaves': 88, 'learning_rate': 0.01146959810416741, 'n_estimators': 800, 'subsample': 0.8202785482508067, 'colsample_bytree': 0.9316292888103536, 'min_child_samples': 12}. Best is trial 33 with value: 0.6178079194924524.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001698 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:44:05,274] Trial 38 finished with value: 0.6140888208269525 and parameters: {'num_leaves': 73, 'learning_rate': 0.0077593550722844205, 'n_estimators': 700, 'subsample': 0.7589999774583992, 'colsample_bytree': 0.8892948811545677, 'min_child_samples': 5}. Best is trial 33 with value: 0.6178079194924524.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001772 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:44:37,230] Trial 39 finished with value: 0.6146357471012908 and parameters: {'num_leaves': 96, 'learning_rate': 0.018225169112619934, 'n_estimators': 700, 'subsample': 0.6789738733890993, 'colsample_bytree': 0.9748660582997517, 'min_child_samples': 32}. Best is trial 33 with value: 0.6178079194924524.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003070 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:45:01,021] Trial 40 finished with value: 0.6115729599649967 and parameters: {'num_leaves': 133, 'learning_rate': 0.025911837097663922, 'n_estimators': 500, 'subsample': 0.6072468744234536, 'colsample_bytree': 0.9370704673947708, 'min_child_samples': 8}. Best is trial 33 with value: 0.6178079194924524.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001701 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:45:30,986] Trial 41 finished with value: 0.618245460511923 and parameters: {'num_leaves': 108, 'learning_rate': 0.008137362242217255, 'n_estimators': 600, 'subsample': 0.6967628007585532, 'colsample_bytree': 0.9764870124551623, 'min_child_samples': 7}. Best is trial 41 with value: 0.618245460511923.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001708 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:46:02,309] Trial 42 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 121, 'learning_rate': 0.00791025040418004, 'n_estimators': 600, 'subsample': 0.7026767217659157, 'colsample_bytree': 0.9851643565082752, 'min_child_samples': 10}. Best is trial 42 with value: 0.6189017720411288.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001692 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:46:33,333] Trial 43 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 119, 'learning_rate': 0.007844601495297334, 'n_estimators': 600, 'subsample': 0.6274930327773026, 'colsample_bytree': 0.9913779558709228, 'min_child_samples': 5}. Best is trial 42 with value: 0.6189017720411288.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001778 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:47:03,731] Trial 44 finished with value: 0.6184642310216583 and parameters: {'num_leaves': 121, 'learning_rate': 0.008930153494450232, 'n_estimators': 600, 'subsample': 0.6282772879310613, 'colsample_bytree': 0.9947179786422855, 'min_child_samples': 6}. Best is trial 42 with value: 0.6189017720411288.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001704 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:47:35,379] Trial 45 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 120, 'learning_rate': 0.0065675457246027295, 'n_estimators': 600, 'subsample': 0.6294761056689602, 'colsample_bytree': 0.998348276185765, 'min_child_samples': 5}. Best is trial 42 with value: 0.6189017720411288.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001743 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:48:04,205] Trial 46 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 134, 'learning_rate': 0.006609302445553561, 'n_estimators': 500, 'subsample': 0.6606282579170563, 'colsample_bytree': 0.9834311255753297, 'min_child_samples': 41}. Best is trial 42 with value: 0.6189017720411288.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001744 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:48:33,148] Trial 47 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 136, 'learning_rate': 0.006456538107780771, 'n_estimators': 500, 'subsample': 0.6244081997208647, 'colsample_bytree': 0.999444903789299, 'min_child_samples': 44}. Best is trial 42 with value: 0.6189017720411288.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001683 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:49:04,022] Trial 48 finished with value: 0.6184642310216583 and parameters: {'num_leaves': 145, 'learning_rate': 0.005069771973338217, 'n_estimators': 500, 'subsample': 0.6215527973646728, 'colsample_bytree': 0.9942751363504445, 'min_child_samples': 44}. Best is trial 42 with value: 0.6189017720411288.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001680 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:49:28,261] Trial 49 finished with value: 0.6174797637278495 and parameters: {'num_leaves': 149, 'learning_rate': 0.006302919060257801, 'n_estimators': 400, 'subsample': 0.6212967427331928, 'colsample_bytree': 0.9969887232802028, 'min_child_samples': 45}. Best is trial 42 with value: 0.6189017720411288.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001838 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:49:57,538] Trial 50 finished with value: 0.6163859111791731 and parameters: {'num_leaves': 141, 'learning_rate': 0.00987254179432921, 'n_estimators': 500, 'subsample': 0.6022705135393184, 'colsample_bytree': 0.99903793381733, 'min_child_samples': 39}. Best is trial 42 with value: 0.6189017720411288.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001772 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:50:32,495] Trial 51 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 129, 'learning_rate': 0.005047283928301474, 'n_estimators': 600, 'subsample': 0.6431963525817431, 'colsample_bytree': 0.9800332622208934, 'min_child_samples': 35}. Best is trial 51 with value: 0.6190111572959965.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001724 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:51:02,398] Trial 52 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 142, 'learning_rate': 0.005401225086317659, 'n_estimators': 500, 'subsample': 0.645645577239887, 'colsample_bytree': 0.9614333556698398, 'min_child_samples': 50}. Best is trial 51 with value: 0.6190111572959965.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001712 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:51:41,974] Trial 53 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 122, 'learning_rate': 0.005166577194414189, 'n_estimators': 700, 'subsample': 0.6216955303407836, 'colsample_bytree': 0.9490752163695872, 'min_child_samples': 43}. Best is trial 51 with value: 0.6190111572959965.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001726 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:52:10,933] Trial 54 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 132, 'learning_rate': 0.006803477599312252, 'n_estimators': 500, 'subsample': 0.6353590899312932, 'colsample_bytree': 0.9843999256582444, 'min_child_samples': 36}. Best is trial 54 with value: 0.6195580835703347.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001773 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:52:33,075] Trial 55 finished with value: 0.618573616276526 and parameters: {'num_leaves': 128, 'learning_rate': 0.009381803905886383, 'n_estimators': 400, 'subsample': 0.6667760560696351, 'colsample_bytree': 0.9816460493612752, 'min_child_samples': 33}. Best is trial 54 with value: 0.6195580835703347.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001737 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:52:56,502] Trial 56 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 128, 'learning_rate': 0.006594464914801093, 'n_estimators': 400, 'subsample': 0.6702149750906172, 'colsample_bytree': 0.9782922496694928, 'min_child_samples': 36}. Best is trial 54 with value: 0.6195580835703347.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002170 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:53:19,341] Trial 57 finished with value: 0.6205425508641436 and parameters: {'num_leaves': 132, 'learning_rate': 0.006721902764168278, 'n_estimators': 400, 'subsample': 0.6436029822542269, 'colsample_bytree': 0.9503611332928005, 'min_child_samples': 35}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001782 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:53:42,690] Trial 58 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 135, 'learning_rate': 0.006893209713273016, 'n_estimators': 400, 'subsample': 0.645199318401945, 'colsample_bytree': 0.952007257903443, 'min_child_samples': 35}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001736 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:54:05,842] Trial 59 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 130, 'learning_rate': 0.005999735717575884, 'n_estimators': 400, 'subsample': 0.647538223449583, 'colsample_bytree': 0.9079265355559125, 'min_child_samples': 36}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001725 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:54:22,774] Trial 60 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 138, 'learning_rate': 0.007018431825548483, 'n_estimators': 300, 'subsample': 0.6544550932947574, 'colsample_bytree': 0.9477135962665573, 'min_child_samples': 36}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:54:45,796] Trial 61 finished with value: 0.6174797637278495 and parameters: {'num_leaves': 134, 'learning_rate': 0.005719040585857272, 'n_estimators': 400, 'subsample': 0.6726737511219983, 'colsample_bytree': 0.9570534160462413, 'min_child_samples': 30}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001722 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:55:08,377] Trial 62 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 137, 'learning_rate': 0.011054761770989352, 'n_estimators': 400, 'subsample': 0.6436106257189323, 'colsample_bytree': 0.9755033346284754, 'min_child_samples': 36}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001752 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:55:24,978] Trial 63 finished with value: 0.6158389849048348 and parameters: {'num_leaves': 127, 'learning_rate': 0.006894051191754767, 'n_estimators': 300, 'subsample': 0.6107563363899695, 'colsample_bytree': 0.981875026924431, 'min_child_samples': 33}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001716 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:55:52,709] Trial 64 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 132, 'learning_rate': 0.005776933939136936, 'n_estimators': 500, 'subsample': 0.6389711543257838, 'colsample_bytree': 0.9478530832436842, 'min_child_samples': 30}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002768 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:56:10,585] Trial 65 finished with value: 0.6205425508641436 and parameters: {'num_leaves': 146, 'learning_rate': 0.00885381617240665, 'n_estimators': 300, 'subsample': 0.6891056672134319, 'colsample_bytree': 0.9204744021594847, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002244 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:56:28,006] Trial 66 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 146, 'learning_rate': 0.008534502072190513, 'n_estimators': 300, 'subsample': 0.6864021259966367, 'colsample_bytree': 0.9027126601846963, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001713 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:56:45,319] Trial 67 finished with value: 0.620105009844673 and parameters: {'num_leaves': 146, 'learning_rate': 0.008467702289039484, 'n_estimators': 300, 'subsample': 0.6886857569701668, 'colsample_bytree': 0.9002624428875868, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002994 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:57:03,621] Trial 68 finished with value: 0.6172609932181142 and parameters: {'num_leaves': 146, 'learning_rate': 0.010881018442717195, 'n_estimators': 300, 'subsample': 0.6880470840452819, 'colsample_bytree': 0.8958414067972607, 'min_child_samples': 41}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001716 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:57:21,032] Trial 69 finished with value: 0.6204331656092759 and parameters: {'num_leaves': 148, 'learning_rate': 0.00915144455625421, 'n_estimators': 300, 'subsample': 0.9998895317136034, 'colsample_bytree': 0.8796633762904994, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001826 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:57:39,131] Trial 70 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 150, 'learning_rate': 0.012059319506749103, 'n_estimators': 300, 'subsample': 0.977041764811514, 'colsample_bytree': 0.8741191920874778, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001730 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:57:56,157] Trial 71 finished with value: 0.618245460511923 and parameters: {'num_leaves': 140, 'learning_rate': 0.012565141217303781, 'n_estimators': 300, 'subsample': 0.9913372956440312, 'colsample_bytree': 0.8632499853444849, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:58:08,012] Trial 72 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 150, 'learning_rate': 0.00708242787683105, 'n_estimators': 200, 'subsample': 0.986786924421114, 'colsample_bytree': 0.8802163900530372, 'min_child_samples': 40}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001738 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:58:31,833] Trial 73 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 143, 'learning_rate': 0.008989093382949188, 'n_estimators': 400, 'subsample': 0.9545818325585764, 'colsample_bytree': 0.9240853623684219, 'min_child_samples': 34}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001710 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:58:49,327] Trial 74 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 144, 'learning_rate': 0.009099902886544008, 'n_estimators': 300, 'subsample': 0.9494226521803287, 'colsample_bytree': 0.920996538245461, 'min_child_samples': 28}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001769 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:59:13,775] Trial 75 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 150, 'learning_rate': 0.010083656983794365, 'n_estimators': 400, 'subsample': 0.9554024271996163, 'colsample_bytree': 0.9154290172594302, 'min_child_samples': 34}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001650 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:59:24,628] Trial 76 finished with value: 0.5758039816232772 and parameters: {'num_leaves': 139, 'learning_rate': 0.012547449451454029, 'n_estimators': 200, 'subsample': 0.9767226540064854, 'colsample_bytree': 0.8132067171515377, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001710 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:59:42,168] Trial 77 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 146, 'learning_rate': 0.00856649204097811, 'n_estimators': 300, 'subsample': 0.9252274801168138, 'colsample_bytree': 0.8699950974546925, 'min_child_samples': 31}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002880 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 17:59:53,868] Trial 78 finished with value: 0.6184642310216583 and parameters: {'num_leaves': 146, 'learning_rate': 0.0169893288092642, 'n_estimators': 200, 'subsample': 0.9253360401210059, 'colsample_bytree': 0.8811570264817447, 'min_child_samples': 31}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001768 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:00:04,636] Trial 79 finished with value: 0.6010719754977029 and parameters: {'num_leaves': 34, 'learning_rate': 0.008392298718678227, 'n_estimators': 300, 'subsample': 0.9246184198949684, 'colsample_bytree': 0.8648422896014833, 'min_child_samples': 42}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001746 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:00:22,512] Trial 80 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 143, 'learning_rate': 0.01187066659267448, 'n_estimators': 300, 'subsample': 0.9994607725145792, 'colsample_bytree': 0.8535823146437355, 'min_child_samples': 37}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001780 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:00:46,279] Trial 81 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 147, 'learning_rate': 0.007373694002635674, 'n_estimators': 400, 'subsample': 0.9625539582095457, 'colsample_bytree': 0.8930042334646144, 'min_child_samples': 34}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002873 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:01:03,515] Trial 82 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 137, 'learning_rate': 0.009423546756141076, 'n_estimators': 300, 'subsample': 0.9406241489523263, 'colsample_bytree': 0.9049904350610511, 'min_child_samples': 40}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001720 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:01:20,537] Trial 83 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 139, 'learning_rate': 0.009668483468338021, 'n_estimators': 300, 'subsample': 0.9042341141010563, 'colsample_bytree': 0.8746091459038626, 'min_child_samples': 39}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001698 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:01:32,248] Trial 84 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 142, 'learning_rate': 0.008743050257345267, 'n_estimators': 200, 'subsample': 0.9413604584506708, 'colsample_bytree': 0.9029272073923988, 'min_child_samples': 40}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001758 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:01:50,191] Trial 85 finished with value: 0.618245460511923 and parameters: {'num_leaves': 148, 'learning_rate': 0.010888135294620863, 'n_estimators': 300, 'subsample': 0.9745209743360608, 'colsample_bytree': 0.9205976193716692, 'min_child_samples': 32}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001695 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:01:58,754] Trial 86 finished with value: 0.5966965653029972 and parameters: {'num_leaves': 23, 'learning_rate': 0.01043747114013367, 'n_estimators': 300, 'subsample': 0.9077051537006999, 'colsample_bytree': 0.9398866219644301, 'min_child_samples': 27}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001732 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:02:10,153] Trial 87 finished with value: 0.6169328374535112 and parameters: {'num_leaves': 132, 'learning_rate': 0.009263420350967184, 'n_estimators': 200, 'subsample': 0.9423839475907937, 'colsample_bytree': 0.8887610429442302, 'min_child_samples': 37}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001628 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:03:09,679] Trial 88 finished with value: 0.589367753226865 and parameters: {'num_leaves': 144, 'learning_rate': 0.007399453803968956, 'n_estimators': 1000, 'subsample': 0.9623643956317449, 'colsample_bytree': 0.8268369504923708, 'min_child_samples': 34}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001714 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:03:27,307] Trial 89 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 150, 'learning_rate': 0.013458872886264597, 'n_estimators': 300, 'subsample': 0.9351194868052035, 'colsample_bytree': 0.9098242357632298, 'min_child_samples': 46}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001687 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:03:41,625] Trial 90 finished with value: 0.6091664843579085 and parameters: {'num_leaves': 40, 'learning_rate': 0.011662865164845333, 'n_estimators': 400, 'subsample': 0.9866173832828447, 'colsample_bytree': 0.9256308186952302, 'min_child_samples': 40}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003063 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:04:04,294] Trial 91 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 134, 'learning_rate': 0.008214039195670224, 'n_estimators': 400, 'subsample': 0.9089718980617707, 'colsample_bytree': 0.8709071265853505, 'min_child_samples': 35}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001731 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:04:27,581] Trial 92 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 136, 'learning_rate': 0.008704709319682645, 'n_estimators': 400, 'subsample': 0.9675199805686568, 'colsample_bytree': 0.9391628930265935, 'min_child_samples': 37}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001796 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:04:44,864] Trial 93 finished with value: 0.618245460511923 and parameters: {'num_leaves': 138, 'learning_rate': 0.010082686796835234, 'n_estimators': 300, 'subsample': 0.9634764645383308, 'colsample_bytree': 0.8861935011675559, 'min_child_samples': 42}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002939 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:05:01,651] Trial 94 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 125, 'learning_rate': 0.009039799055275275, 'n_estimators': 300, 'subsample': 0.9777987982422317, 'colsample_bytree': 0.9402290076300712, 'min_child_samples': 37}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001640 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:05:23,338] Trial 95 finished with value: 0.5835703347188799 and parameters: {'num_leaves': 141, 'learning_rate': 0.008574573474845642, 'n_estimators': 400, 'subsample': 0.9523517292030156, 'colsample_bytree': 0.6015860395770183, 'min_child_samples': 39}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003072 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:05:34,461] Trial 96 finished with value: 0.6164952964340407 and parameters: {'num_leaves': 144, 'learning_rate': 0.020824771674324487, 'n_estimators': 200, 'subsample': 0.7346947600741817, 'colsample_bytree': 0.9022736775145812, 'min_child_samples': 31}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002819 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:05:55,751] Trial 97 finished with value: 0.5770072194268213 and parameters: {'num_leaves': 136, 'learning_rate': 0.00605860954272995, 'n_estimators': 400, 'subsample': 0.8897993132336661, 'colsample_bytree': 0.6851282250770937, 'min_child_samples': 33}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001727 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:06:13,060] Trial 98 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 130, 'learning_rate': 0.007530469058981418, 'n_estimators': 300, 'subsample': 0.853436641856016, 'colsample_bytree': 0.9640815831014584, 'min_child_samples': 41}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003024 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:06:35,916] Trial 99 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 140, 'learning_rate': 0.009602152569652887, 'n_estimators': 400, 'subsample': 0.9973684268525178, 'colsample_bytree': 0.8542056904051002, 'min_child_samples': 37}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001688 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:07:06,182] Trial 100 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 148, 'learning_rate': 0.008048217574530574, 'n_estimators': 500, 'subsample': 0.9723505668038175, 'colsample_bytree': 0.9124814120397858, 'min_child_samples': 35}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002467 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:07:29,181] Trial 101 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 137, 'learning_rate': 0.0069941259165670604, 'n_estimators': 400, 'subsample': 0.6587315271603279, 'colsample_bytree': 0.9530143419702014, 'min_child_samples': 35}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001709 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:07:46,026] Trial 102 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 135, 'learning_rate': 0.006143102512326289, 'n_estimators': 300, 'subsample': 0.6769780004712016, 'colsample_bytree': 0.9687996432939644, 'min_child_samples': 39}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001803 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:08:42,350] Trial 103 finished with value: 0.6146357471012908 and parameters: {'num_leaves': 143, 'learning_rate': 0.007264883828750638, 'n_estimators': 900, 'subsample': 0.9345681111397125, 'colsample_bytree': 0.9254620004704588, 'min_child_samples': 34}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002904 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:09:05,012] Trial 104 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 133, 'learning_rate': 0.007856595153103711, 'n_estimators': 400, 'subsample': 0.9834664640630223, 'colsample_bytree': 0.9431375269734438, 'min_child_samples': 24}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001702 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:09:21,751] Trial 105 finished with value: 0.6157295996499672 and parameters: {'num_leaves': 124, 'learning_rate': 0.006762921725180629, 'n_estimators': 300, 'subsample': 0.964552895256447, 'colsample_bytree': 0.9357435019853934, 'min_child_samples': 36}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002721 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:09:43,327] Trial 106 finished with value: 0.5837891052286152 and parameters: {'num_leaves': 147, 'learning_rate': 0.00864940984801697, 'n_estimators': 400, 'subsample': 0.6350095099827141, 'colsample_bytree': 0.7670088682783959, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001703 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:09:55,139] Trial 107 finished with value: 0.618573616276526 and parameters: {'num_leaves': 145, 'learning_rate': 0.010443559773812883, 'n_estimators': 200, 'subsample': 0.6128083984473841, 'colsample_bytree': 0.9553481742806329, 'min_child_samples': 31}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:10:23,612] Trial 108 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 131, 'learning_rate': 0.005499683160020773, 'n_estimators': 500, 'subsample': 0.9535418650672436, 'colsample_bytree': 0.8995202623396845, 'min_child_samples': 29}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002928 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:10:39,033] Trial 109 finished with value: 0.6144169765915555 and parameters: {'num_leaves': 117, 'learning_rate': 0.026002496656858116, 'n_estimators': 300, 'subsample': 0.7154163107576924, 'colsample_bytree': 0.8697570353042949, 'min_child_samples': 37}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001783 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:11:02,005] Trial 110 finished with value: 0.6172609932181142 and parameters: {'num_leaves': 140, 'learning_rate': 0.0094233230503948, 'n_estimators': 400, 'subsample': 0.6542381234503621, 'colsample_bytree': 0.9187300112350636, 'min_child_samples': 33}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002818 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:11:22,796] Trial 111 finished with value: 0.5787573835047035 and parameters: {'num_leaves': 127, 'learning_rate': 0.006441777241915555, 'n_estimators': 400, 'subsample': 0.6715624390716824, 'colsample_bytree': 0.6239558697472185, 'min_child_samples': 36}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:11:45,988] Trial 112 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 136, 'learning_rate': 0.0075935499923726986, 'n_estimators': 400, 'subsample': 0.6921163120400672, 'colsample_bytree': 0.9291105604613589, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001909 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:12:03,715] Trial 113 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 150, 'learning_rate': 0.006540126557001144, 'n_estimators': 300, 'subsample': 0.9940198791035132, 'colsample_bytree': 0.9715223274568636, 'min_child_samples': 35}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002783 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:12:34,149] Trial 114 finished with value: 0.618245460511923 and parameters: {'num_leaves': 142, 'learning_rate': 0.008320601953781868, 'n_estimators': 500, 'subsample': 0.9819404573281452, 'colsample_bytree': 0.9087006921996609, 'min_child_samples': 42}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001748 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:12:57,130] Trial 115 finished with value: 0.618245460511923 and parameters: {'num_leaves': 138, 'learning_rate': 0.007014319576252034, 'n_estimators': 400, 'subsample': 0.6330591032207842, 'colsample_bytree': 0.9879815107281005, 'min_child_samples': 32}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001707 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:13:08,655] Trial 116 finished with value: 0.6103697221614527 and parameters: {'num_leaves': 51, 'learning_rate': 0.013525271727905967, 'n_estimators': 300, 'subsample': 0.809656701495343, 'colsample_bytree': 0.8459568444692472, 'min_child_samples': 40}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001702 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:13:32,192] Trial 117 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 147, 'learning_rate': 0.009023933293724515, 'n_estimators': 400, 'subsample': 0.9679995271009844, 'colsample_bytree': 0.9629488293329874, 'min_child_samples': 39}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002833 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:13:49,454] Trial 118 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 129, 'learning_rate': 0.007960330700279461, 'n_estimators': 300, 'subsample': 0.6666586745880175, 'colsample_bytree': 0.8821378905478925, 'min_child_samples': 36}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001738 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:14:05,739] Trial 119 finished with value: 0.6162765259243054 and parameters: {'num_leaves': 126, 'learning_rate': 0.0059407572330874275, 'n_estimators': 300, 'subsample': 0.9432257839107089, 'colsample_bytree': 0.8940624908223329, 'min_child_samples': 37}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001942 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:14:17,481] Trial 120 finished with value: 0.6176985342375848 and parameters: {'num_leaves': 144, 'learning_rate': 0.010577379933807005, 'n_estimators': 200, 'subsample': 0.6497417044530623, 'colsample_bytree': 0.9529782775507548, 'min_child_samples': 34}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001690 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:14:35,088] Trial 121 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 146, 'learning_rate': 0.008642343363159962, 'n_estimators': 300, 'subsample': 0.6979306649956799, 'colsample_bytree': 0.9057041762232007, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002844 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:14:52,604] Trial 122 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 141, 'learning_rate': 0.00734881556974256, 'n_estimators': 300, 'subsample': 0.6999094538687288, 'colsample_bytree': 0.9324658910593833, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001709 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:15:09,897] Trial 123 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 146, 'learning_rate': 0.010006779361875755, 'n_estimators': 300, 'subsample': 0.676328830996151, 'colsample_bytree': 0.9166154489196333, 'min_child_samples': 40}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001728 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:15:34,836] Trial 124 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 135, 'learning_rate': 0.008639867900583888, 'n_estimators': 400, 'subsample': 0.6835977411252346, 'colsample_bytree': 0.9432982641566171, 'min_child_samples': 35}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001852 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:16:01,765] Trial 125 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 135, 'learning_rate': 0.00863009216157202, 'n_estimators': 400, 'subsample': 0.7346697817148659, 'colsample_bytree': 0.9248407357875015, 'min_child_samples': 35}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002835 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:16:23,049] Trial 126 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 150, 'learning_rate': 0.011392377304275463, 'n_estimators': 300, 'subsample': 0.6929212882369242, 'colsample_bytree': 0.9071291079984369, 'min_child_samples': 41}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001871 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:16:37,163] Trial 127 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 150, 'learning_rate': 0.012301647113908144, 'n_estimators': 200, 'subsample': 0.7055324977754738, 'colsample_bytree': 0.9063006462331065, 'min_child_samples': 41}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002894 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:16:57,321] Trial 128 finished with value: 0.6175891489827171 and parameters: {'num_leaves': 148, 'learning_rate': 0.011336413430988802, 'n_estimators': 300, 'subsample': 0.6842848099846065, 'colsample_bytree': 0.8978664706437401, 'min_child_samples': 43}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001747 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:17:17,584] Trial 129 finished with value: 0.611791730474732 and parameters: {'num_leaves': 144, 'learning_rate': 0.030377055496526816, 'n_estimators': 300, 'subsample': 0.7149890561972568, 'colsample_bytree': 0.8598060814135919, 'min_child_samples': 39}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002896 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:17:31,342] Trial 130 finished with value: 0.6157295996499672 and parameters: {'num_leaves': 139, 'learning_rate': 0.009715875409123045, 'n_estimators': 200, 'subsample': 0.6930248523343664, 'colsample_bytree': 0.887173501006202, 'min_child_samples': 43}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001718 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:17:51,399] Trial 131 finished with value: 0.6173703784729818 and parameters: {'num_leaves': 142, 'learning_rate': 0.01607376060137742, 'n_estimators': 300, 'subsample': 0.6628445519208066, 'colsample_bytree': 0.9133895901866897, 'min_child_samples': 37}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001761 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:18:19,286] Trial 132 finished with value: 0.6174797637278495 and parameters: {'num_leaves': 146, 'learning_rate': 0.009084978258343045, 'n_estimators': 400, 'subsample': 0.9173620814881995, 'colsample_bytree': 0.9496442720911833, 'min_child_samples': 33}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003611 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:18:39,825] Trial 133 finished with value: 0.6174797637278495 and parameters: {'num_leaves': 132, 'learning_rate': 0.00824100841428872, 'n_estimators': 300, 'subsample': 0.7257631995285676, 'colsample_bytree': 0.9406125410164633, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001682 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:18:57,276] Trial 134 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 137, 'learning_rate': 0.010833060864846346, 'n_estimators': 300, 'subsample': 0.7108443536467903, 'colsample_bytree': 0.8763677190575804, 'min_child_samples': 34}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001825 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:19:28,098] Trial 135 finished with value: 0.6154014438853642 and parameters: {'num_leaves': 150, 'learning_rate': 0.009382139439147534, 'n_estimators': 500, 'subsample': 0.6830418131804719, 'colsample_bytree': 0.9325656250536547, 'min_child_samples': 41}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001726 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:19:51,015] Trial 136 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 140, 'learning_rate': 0.007832761908630359, 'n_estimators': 400, 'subsample': 0.9326943275363266, 'colsample_bytree': 0.8343750879471635, 'min_child_samples': 36}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002717 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:20:07,334] Trial 137 finished with value: 0.576788448917086 and parameters: {'num_leaves': 140, 'learning_rate': 0.007812741549959219, 'n_estimators': 300, 'subsample': 0.9575349585623837, 'colsample_bytree': 0.7948062998925387, 'min_child_samples': 36}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001709 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:20:30,919] Trial 138 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 143, 'learning_rate': 0.010043633705662844, 'n_estimators': 400, 'subsample': 0.9261647821456651, 'colsample_bytree': 0.840678889810603, 'min_child_samples': 39}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001641 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:20:47,083] Trial 139 finished with value: 0.5768978341719536 and parameters: {'num_leaves': 147, 'learning_rate': 0.008753275597720659, 'n_estimators': 300, 'subsample': 0.9359368004730381, 'colsample_bytree': 0.8285461351121934, 'min_child_samples': 40}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001744 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:21:11,028] Trial 140 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 145, 'learning_rate': 0.008376443688691691, 'n_estimators': 400, 'subsample': 0.9471422363944456, 'colsample_bytree': 0.8919325506040761, 'min_child_samples': 37}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001648 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:21:32,235] Trial 141 finished with value: 0.5812732443666594 and parameters: {'num_leaves': 135, 'learning_rate': 0.007585725825610339, 'n_estimators': 400, 'subsample': 0.7503536448029998, 'colsample_bytree': 0.816889627690268, 'min_child_samples': 35}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001747 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:22:00,904] Trial 142 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 139, 'learning_rate': 0.006936704443575259, 'n_estimators': 500, 'subsample': 0.6155531545845958, 'colsample_bytree': 0.8690219130819691, 'min_child_samples': 38}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001717 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:22:23,748] Trial 143 finished with value: 0.6186830015313936 and parameters: {'num_leaves': 133, 'learning_rate': 0.009214955797907414, 'n_estimators': 400, 'subsample': 0.9729709717858835, 'colsample_bytree': 0.9207663399784726, 'min_child_samples': 36}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001793 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:22:36,364] Trial 144 finished with value: 0.6086195580835704 and parameters: {'num_leaves': 59, 'learning_rate': 0.008263538322289845, 'n_estimators': 300, 'subsample': 0.6973092418068683, 'colsample_bytree': 0.9088485350921235, 'min_child_samples': 35}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003131 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:22:59,879] Trial 145 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 142, 'learning_rate': 0.009688985745295987, 'n_estimators': 400, 'subsample': 0.7900754399739897, 'colsample_bytree': 0.9009663839458459, 'min_child_samples': 34}. Best is trial 57 with value: 0.6205425508641436.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001765 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:23:17,302] Trial 146 finished with value: 0.6208707066287464 and parameters: {'num_leaves': 147, 'learning_rate': 0.00730159094167797, 'n_estimators': 300, 'subsample': 0.638366309936121, 'colsample_bytree': 0.9442067991852162, 'min_child_samples': 37}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001764 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:23:34,925] Trial 147 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 148, 'learning_rate': 0.011896310694164379, 'n_estimators': 300, 'subsample': 0.9876033951502166, 'colsample_bytree': 0.9424656750224981, 'min_child_samples': 39}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001740 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:23:53,576] Trial 148 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 150, 'learning_rate': 0.007394312331242826, 'n_estimators': 300, 'subsample': 0.9142878457374752, 'colsample_bytree': 0.8807123825224003, 'min_child_samples': 37}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001725 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:24:10,793] Trial 149 finished with value: 0.620105009844673 and parameters: {'num_leaves': 145, 'learning_rate': 0.008811935224305612, 'n_estimators': 300, 'subsample': 0.9363934965617743, 'colsample_bytree': 0.9288032231986361, 'min_child_samples': 32}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002040 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:24:22,456] Trial 150 finished with value: 0.6168234521986437 and parameters: {'num_leaves': 145, 'learning_rate': 0.007826721908023486, 'n_estimators': 200, 'subsample': 0.9316456636286442, 'colsample_bytree': 0.8608738593032025, 'min_child_samples': 32}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001688 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:24:40,034] Trial 151 finished with value: 0.618245460511923 and parameters: {'num_leaves': 142, 'learning_rate': 0.008856188789021542, 'n_estimators': 300, 'subsample': 0.9540434626344461, 'colsample_bytree': 0.9267592995993013, 'min_child_samples': 30}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002997 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:24:57,593] Trial 152 finished with value: 0.6184642310216583 and parameters: {'num_leaves': 147, 'learning_rate': 0.0086310060581577, 'n_estimators': 300, 'subsample': 0.9422506146968276, 'colsample_bytree': 0.9161591650551905, 'min_child_samples': 38}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001721 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:25:14,372] Trial 153 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 139, 'learning_rate': 0.010318933819164586, 'n_estimators': 300, 'subsample': 0.6403568308499465, 'colsample_bytree': 0.9370491634914837, 'min_child_samples': 33}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:25:32,078] Trial 154 finished with value: 0.61977685408007 and parameters: {'num_leaves': 144, 'learning_rate': 0.008076203783988363, 'n_estimators': 300, 'subsample': 0.8918839221981294, 'colsample_bytree': 0.9058059537960788, 'min_child_samples': 32}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001730 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:25:48,690] Trial 155 finished with value: 0.618245460511923 and parameters: {'num_leaves': 137, 'learning_rate': 0.007241649076232972, 'n_estimators': 300, 'subsample': 0.8998627404366465, 'colsample_bytree': 0.8898708190725687, 'min_child_samples': 31}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001728 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:26:05,825] Trial 156 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 144, 'learning_rate': 0.008082698120963043, 'n_estimators': 300, 'subsample': 0.881659353418721, 'colsample_bytree': 0.9225976624817704, 'min_child_samples': 29}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:26:23,783] Trial 157 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 141, 'learning_rate': 0.006257824820417297, 'n_estimators': 300, 'subsample': 0.8388217753751562, 'colsample_bytree': 0.9584403105872766, 'min_child_samples': 32}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001710 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:26:29,938] Trial 158 finished with value: 0.5937431634215707 and parameters: {'num_leaves': 29, 'learning_rate': 0.009367157148259099, 'n_estimators': 200, 'subsample': 0.9284494855404115, 'colsample_bytree': 0.9345123906116171, 'min_child_samples': 36}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001749 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:26:53,554] Trial 159 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 145, 'learning_rate': 0.007666779735196812, 'n_estimators': 400, 'subsample': 0.9626420591198497, 'colsample_bytree': 0.8979049304444787, 'min_child_samples': 33}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001746 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:27:11,308] Trial 160 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 147, 'learning_rate': 0.006847979815756008, 'n_estimators': 300, 'subsample': 0.9181469677378757, 'colsample_bytree': 0.9137873961214489, 'min_child_samples': 36}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002972 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:27:29,241] Trial 161 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 150, 'learning_rate': 0.008885476001632834, 'n_estimators': 300, 'subsample': 0.9466681681030074, 'colsample_bytree': 0.9013645790294953, 'min_child_samples': 39}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001701 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:27:46,720] Trial 162 finished with value: 0.6207613213738788 and parameters: {'num_leaves': 144, 'learning_rate': 0.008226760989353215, 'n_estimators': 300, 'subsample': 0.6011829772285626, 'colsample_bytree': 0.9277508468900976, 'min_child_samples': 37}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:28:04,974] Trial 163 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 143, 'learning_rate': 0.007941348786859058, 'n_estimators': 300, 'subsample': 0.600459049898862, 'colsample_bytree': 0.9264680338123255, 'min_child_samples': 38}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002986 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:28:26,741] Trial 164 finished with value: 0.6147451323561584 and parameters: {'num_leaves': 138, 'learning_rate': 0.023247075176089052, 'n_estimators': 400, 'subsample': 0.627455832134644, 'colsample_bytree': 0.9493242863141674, 'min_child_samples': 37}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001755 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:28:44,418] Trial 165 finished with value: 0.620105009844673 and parameters: {'num_leaves': 141, 'learning_rate': 0.008397370279493277, 'n_estimators': 300, 'subsample': 0.9386976500154609, 'colsample_bytree': 0.9437730723444493, 'min_child_samples': 34}. Best is trial 146 with value: 0.6208707066287464.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001787 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:29:02,049] Trial 166 finished with value: 0.6210894771384817 and parameters: {'num_leaves': 141, 'learning_rate': 0.008388588473348963, 'n_estimators': 300, 'subsample': 0.6076039578690107, 'colsample_bytree': 0.9441693374656261, 'min_child_samples': 34}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001725 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:29:14,040] Trial 167 finished with value: 0.6168234521986437 and parameters: {'num_leaves': 141, 'learning_rate': 0.008241748034692317, 'n_estimators': 200, 'subsample': 0.6215911777482784, 'colsample_bytree': 0.9449471848293465, 'min_child_samples': 34}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001773 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:29:31,554] Trial 168 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 131, 'learning_rate': 0.007322400488473447, 'n_estimators': 300, 'subsample': 0.6079393685858833, 'colsample_bytree': 0.9655179208031986, 'min_child_samples': 35}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001724 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:29:54,878] Trial 169 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 140, 'learning_rate': 0.006579972771687449, 'n_estimators': 400, 'subsample': 0.6031842470385077, 'colsample_bytree': 0.9416624743856423, 'min_child_samples': 35}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002848 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:30:24,441] Trial 170 finished with value: 0.6176985342375848 and parameters: {'num_leaves': 143, 'learning_rate': 0.007741460577584101, 'n_estimators': 500, 'subsample': 0.6351514713773715, 'colsample_bytree': 0.9292347457864819, 'min_child_samples': 34}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001740 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:30:41,702] Trial 171 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 147, 'learning_rate': 0.008528060208259995, 'n_estimators': 300, 'subsample': 0.6083960076600629, 'colsample_bytree': 0.9358613226396739, 'min_child_samples': 32}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001739 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:30:59,363] Trial 172 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 145, 'learning_rate': 0.008863635152200355, 'n_estimators': 300, 'subsample': 0.6177500211349789, 'colsample_bytree': 0.9579120223559225, 'min_child_samples': 33}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001796 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:31:16,539] Trial 173 finished with value: 0.6202143950995406 and parameters: {'num_leaves': 145, 'learning_rate': 0.009087993991888014, 'n_estimators': 300, 'subsample': 0.6169127607776052, 'colsample_bytree': 0.9469204582047105, 'min_child_samples': 36}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001766 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:31:33,680] Trial 174 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 145, 'learning_rate': 0.009696087246079123, 'n_estimators': 300, 'subsample': 0.6169360959288838, 'colsample_bytree': 0.9605688812736061, 'min_child_samples': 36}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:31:51,994] Trial 175 finished with value: 0.6202143950995406 and parameters: {'num_leaves': 148, 'learning_rate': 0.008071443561674753, 'n_estimators': 300, 'subsample': 0.6156705014908191, 'colsample_bytree': 0.9545335848812034, 'min_child_samples': 37}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001758 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:32:09,389] Trial 176 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 148, 'learning_rate': 0.008139175015063534, 'n_estimators': 300, 'subsample': 0.6130487726417582, 'colsample_bytree': 0.9509526665292621, 'min_child_samples': 37}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001700 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:32:27,446] Trial 177 finished with value: 0.6210894771384817 and parameters: {'num_leaves': 148, 'learning_rate': 0.008351757864359861, 'n_estimators': 300, 'subsample': 0.60939887597029, 'colsample_bytree': 0.9478342750056965, 'min_child_samples': 37}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003011 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:32:44,463] Trial 178 finished with value: 0.6073069350251586 and parameters: {'num_leaves': 149, 'learning_rate': 0.04270654623961566, 'n_estimators': 300, 'subsample': 0.626207445494984, 'colsample_bytree': 0.9522287497809336, 'min_child_samples': 37}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001851 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:33:01,985] Trial 179 finished with value: 0.6202143950995406 and parameters: {'num_leaves': 148, 'learning_rate': 0.00825966761710682, 'n_estimators': 300, 'subsample': 0.6008261949762754, 'colsample_bytree': 0.9457112352882706, 'min_child_samples': 37}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001736 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:33:20,356] Trial 180 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 148, 'learning_rate': 0.008460331838046876, 'n_estimators': 300, 'subsample': 0.6015384907149464, 'colsample_bytree': 0.9688714974507144, 'min_child_samples': 38}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:33:37,729] Trial 181 finished with value: 0.6210894771384817 and parameters: {'num_leaves': 148, 'learning_rate': 0.008261140412241425, 'n_estimators': 300, 'subsample': 0.611800624783945, 'colsample_bytree': 0.9433678918848479, 'min_child_samples': 37}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001919 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:33:55,743] Trial 182 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 148, 'learning_rate': 0.009239406348321717, 'n_estimators': 300, 'subsample': 0.6100272351009103, 'colsample_bytree': 0.9453519420233163, 'min_child_samples': 37}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001697 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:34:13,398] Trial 183 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 150, 'learning_rate': 0.009073364264029429, 'n_estimators': 300, 'subsample': 0.6094402991045931, 'colsample_bytree': 0.9433797491940772, 'min_child_samples': 38}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001778 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:34:30,734] Trial 184 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 146, 'learning_rate': 0.01017544088391892, 'n_estimators': 300, 'subsample': 0.6199105593746626, 'colsample_bytree': 0.9449059299296234, 'min_child_samples': 37}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001779 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:34:49,378] Trial 185 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 148, 'learning_rate': 0.009691368798414349, 'n_estimators': 300, 'subsample': 0.6077074323005879, 'colsample_bytree': 0.9339722112340395, 'min_child_samples': 39}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001723 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:35:01,255] Trial 186 finished with value: 0.6176985342375848 and parameters: {'num_leaves': 146, 'learning_rate': 0.007254804506796396, 'n_estimators': 200, 'subsample': 0.6006619368559026, 'colsample_bytree': 0.9568738219761238, 'min_child_samples': 37}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001997 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:35:18,817] Trial 187 finished with value: 0.620105009844673 and parameters: {'num_leaves': 150, 'learning_rate': 0.009003783180108303, 'n_estimators': 300, 'subsample': 0.6223425849842786, 'colsample_bytree': 0.9391476691033843, 'min_child_samples': 35}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001781 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:35:37,048] Trial 188 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 150, 'learning_rate': 0.009199700571295513, 'n_estimators': 300, 'subsample': 0.6307648569465294, 'colsample_bytree': 0.9383467575563212, 'min_child_samples': 38}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001775 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:35:48,273] Trial 189 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 147, 'learning_rate': 0.008269602289935587, 'n_estimators': 200, 'subsample': 0.6183643602692966, 'colsample_bytree': 0.9654912069152035, 'min_child_samples': 36}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003008 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:36:05,800] Trial 190 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 143, 'learning_rate': 0.007555513646886901, 'n_estimators': 300, 'subsample': 0.6236013108277728, 'colsample_bytree': 0.9498929096867988, 'min_child_samples': 39}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001691 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:36:23,299] Trial 191 finished with value: 0.620105009844673 and parameters: {'num_leaves': 150, 'learning_rate': 0.008737297927477308, 'n_estimators': 300, 'subsample': 0.6117757045087733, 'colsample_bytree': 0.9426414361489268, 'min_child_samples': 35}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001740 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:36:41,462] Trial 192 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 150, 'learning_rate': 0.00911173096978464, 'n_estimators': 300, 'subsample': 0.6120835338599374, 'colsample_bytree': 0.9304917144148195, 'min_child_samples': 36}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001785 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:36:58,742] Trial 193 finished with value: 0.6202143950995406 and parameters: {'num_leaves': 147, 'learning_rate': 0.008609968018742906, 'n_estimators': 300, 'subsample': 0.6158298279369997, 'colsample_bytree': 0.9443994100938912, 'min_child_samples': 35}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001752 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:37:16,179] Trial 194 finished with value: 0.6210894771384817 and parameters: {'num_leaves': 145, 'learning_rate': 0.008501951005057889, 'n_estimators': 300, 'subsample': 0.6302851905250827, 'colsample_bytree': 0.9365817660103956, 'min_child_samples': 35}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002833 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:37:33,829] Trial 195 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 143, 'learning_rate': 0.00814481319182743, 'n_estimators': 300, 'subsample': 0.6370487886466646, 'colsample_bytree': 0.9565822363631439, 'min_child_samples': 35}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001694 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:37:51,066] Trial 196 finished with value: 0.6209800918836141 and parameters: {'num_leaves': 145, 'learning_rate': 0.007592668140927179, 'n_estimators': 300, 'subsample': 0.6250089290178655, 'colsample_bytree': 0.934980668554944, 'min_child_samples': 35}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001843 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:38:08,890] Trial 197 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 145, 'learning_rate': 0.007162573642197847, 'n_estimators': 300, 'subsample': 0.6280977588670386, 'colsample_bytree': 0.9353249604115679, 'min_child_samples': 34}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001818 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:38:26,333] Trial 198 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 150, 'learning_rate': 0.00763213258480801, 'n_estimators': 300, 'subsample': 0.6171685934870823, 'colsample_bytree': 0.9503007475495443, 'min_child_samples': 35}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001776 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:38:43,768] Trial 199 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 147, 'learning_rate': 0.00979037072165037, 'n_estimators': 300, 'subsample': 0.646138583216547, 'colsample_bytree': 0.9240605701350051, 'min_child_samples': 35}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002943 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:39:01,527] Trial 200 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 142, 'learning_rate': 0.00784464538485854, 'n_estimators': 300, 'subsample': 0.6290585244395238, 'colsample_bytree': 0.9719851127064897, 'min_child_samples': 36}. Best is trial 166 with value: 0.6210894771384817.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001707 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:39:19,054] Trial 201 finished with value: 0.6222927149420258 and parameters: {'num_leaves': 145, 'learning_rate': 0.008465894175557338, 'n_estimators': 300, 'subsample': 0.601132663364135, 'colsample_bytree': 0.9402009865460146, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001683 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:39:37,093] Trial 202 finished with value: 0.6206519361190111 and parameters: {'num_leaves': 145, 'learning_rate': 0.008585663586167615, 'n_estimators': 300, 'subsample': 0.6059718970581358, 'colsample_bytree': 0.9363363762138763, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001684 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:39:54,503] Trial 203 finished with value: 0.6206519361190111 and parameters: {'num_leaves': 145, 'learning_rate': 0.008488440735315047, 'n_estimators': 300, 'subsample': 0.6011102212872198, 'colsample_bytree': 0.9368429616835309, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003022 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:40:12,064] Trial 204 finished with value: 0.61977685408007 and parameters: {'num_leaves': 144, 'learning_rate': 0.008363237038106318, 'n_estimators': 300, 'subsample': 0.6001177941962674, 'colsample_bytree': 0.9310500082093106, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002902 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:40:29,635] Trial 205 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 141, 'learning_rate': 0.007570775859060956, 'n_estimators': 300, 'subsample': 0.607201709066055, 'colsample_bytree': 0.9397760133935359, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001775 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:40:46,847] Trial 206 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 145, 'learning_rate': 0.008101996915399513, 'n_estimators': 300, 'subsample': 0.6226068460964482, 'colsample_bytree': 0.9513431502250425, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001806 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:41:04,705] Trial 207 finished with value: 0.618573616276526 and parameters: {'num_leaves': 146, 'learning_rate': 0.006927144866842448, 'n_estimators': 300, 'subsample': 0.6168151966313437, 'colsample_bytree': 0.9275327348029329, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001822 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:41:21,909] Trial 208 finished with value: 0.618573616276526 and parameters: {'num_leaves': 142, 'learning_rate': 0.008562188765801384, 'n_estimators': 300, 'subsample': 0.6064829066699536, 'colsample_bytree': 0.9194025172695549, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001717 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:41:39,396] Trial 209 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 147, 'learning_rate': 0.00937882837663538, 'n_estimators': 300, 'subsample': 0.6391478209437561, 'colsample_bytree': 0.9356425685018979, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002871 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:41:57,360] Trial 210 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 144, 'learning_rate': 0.007913170294432961, 'n_estimators': 300, 'subsample': 0.6241291523509734, 'colsample_bytree': 0.9602314561231291, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001786 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:42:14,694] Trial 211 finished with value: 0.62163640341282 and parameters: {'num_leaves': 148, 'learning_rate': 0.008730160506178995, 'n_estimators': 300, 'subsample': 0.6117870412023986, 'colsample_bytree': 0.9444148765345456, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001700 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:42:32,603] Trial 212 finished with value: 0.6204331656092759 and parameters: {'num_leaves': 147, 'learning_rate': 0.008877095289727546, 'n_estimators': 300, 'subsample': 0.601652327667926, 'colsample_bytree': 0.9460705130132744, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001802 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:42:49,875] Trial 213 finished with value: 0.6205425508641436 and parameters: {'num_leaves': 147, 'learning_rate': 0.008465412621866869, 'n_estimators': 300, 'subsample': 0.6031490053895331, 'colsample_bytree': 0.9456581459456758, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001685 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:43:07,326] Trial 214 finished with value: 0.618245460511923 and parameters: {'num_leaves': 147, 'learning_rate': 0.007423141194868551, 'n_estimators': 300, 'subsample': 0.600198446598702, 'colsample_bytree': 0.9541110649238217, 'min_child_samples': 37}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002990 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:43:25,334] Trial 215 finished with value: 0.6206519361190111 and parameters: {'num_leaves': 145, 'learning_rate': 0.008885510994876935, 'n_estimators': 300, 'subsample': 0.6000189261698838, 'colsample_bytree': 0.9455981861986156, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001684 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:43:42,770] Trial 216 finished with value: 0.61977685408007 and parameters: {'num_leaves': 147, 'learning_rate': 0.008141041132482133, 'n_estimators': 300, 'subsample': 0.6121040005514853, 'colsample_bytree': 0.9477736937006442, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001736 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:43:57,927] Trial 217 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 100, 'learning_rate': 0.00960835701207623, 'n_estimators': 300, 'subsample': 0.600886164587676, 'colsample_bytree': 0.9618061225054331, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002803 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:44:14,648] Trial 218 finished with value: 0.5778823014657624 and parameters: {'num_leaves': 144, 'learning_rate': 0.008753182152474495, 'n_estimators': 300, 'subsample': 0.6076965178766712, 'colsample_bytree': 0.7227249372685205, 'min_child_samples': 37}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001738 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:44:26,570] Trial 219 finished with value: 0.6176985342375848 and parameters: {'num_leaves': 148, 'learning_rate': 0.007651296000897233, 'n_estimators': 200, 'subsample': 0.6151176992217794, 'colsample_bytree': 0.9486558465730965, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001750 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:44:43,754] Trial 220 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 143, 'learning_rate': 0.010389853568227242, 'n_estimators': 300, 'subsample': 0.6001066331085723, 'colsample_bytree': 0.9554049840943291, 'min_child_samples': 38}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001686 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:45:01,521] Trial 221 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 145, 'learning_rate': 0.008845289487027203, 'n_estimators': 300, 'subsample': 0.6125830377588807, 'colsample_bytree': 0.9336542183846707, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002968 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:45:19,374] Trial 222 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 147, 'learning_rate': 0.008446435374945545, 'n_estimators': 300, 'subsample': 0.6084640152371915, 'colsample_bytree': 0.9401840425994882, 'min_child_samples': 37}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:45:36,670] Trial 223 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 145, 'learning_rate': 0.009268857575406143, 'n_estimators': 300, 'subsample': 0.6171276143246146, 'colsample_bytree': 0.9269723948510692, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001737 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:45:50,486] Trial 224 finished with value: 0.6109166484357909 and parameters: {'num_leaves': 82, 'learning_rate': 0.008070645324300515, 'n_estimators': 300, 'subsample': 0.6071715617992931, 'colsample_bytree': 0.9324546305184537, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003094 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:46:08,528] Trial 225 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 140, 'learning_rate': 0.008915475559595392, 'n_estimators': 300, 'subsample': 0.6288309188188776, 'colsample_bytree': 0.9455975873573099, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001699 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:46:25,960] Trial 226 finished with value: 0.6207613213738788 and parameters: {'num_leaves': 148, 'learning_rate': 0.008450572854596248, 'n_estimators': 300, 'subsample': 0.6001271358902793, 'colsample_bytree': 0.9457136996645522, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001720 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:46:43,674] Trial 227 finished with value: 0.6172609932181142 and parameters: {'num_leaves': 147, 'learning_rate': 0.007255572866002939, 'n_estimators': 300, 'subsample': 0.6001753114886613, 'colsample_bytree': 0.958949999452052, 'min_child_samples': 19}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001704 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:47:00,943] Trial 228 finished with value: 0.620105009844673 and parameters: {'num_leaves': 148, 'learning_rate': 0.007842319013610217, 'n_estimators': 300, 'subsample': 0.6146301631561693, 'colsample_bytree': 0.9492359985388662, 'min_child_samples': 37}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001765 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:47:17,993] Trial 229 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 142, 'learning_rate': 0.008243979079673943, 'n_estimators': 300, 'subsample': 0.6076315753663473, 'colsample_bytree': 0.9389113559854909, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001724 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:47:36,619] Trial 230 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 150, 'learning_rate': 0.009746863548606703, 'n_estimators': 300, 'subsample': 0.6229994722481144, 'colsample_bytree': 0.9696955788628717, 'min_child_samples': 38}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001694 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:47:53,767] Trial 231 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 145, 'learning_rate': 0.008621837718429035, 'n_estimators': 300, 'subsample': 0.6066875792415239, 'colsample_bytree': 0.9453972152096597, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001808 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:48:11,952] Trial 232 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 144, 'learning_rate': 0.008480036687182736, 'n_estimators': 300, 'subsample': 0.61511672021502, 'colsample_bytree': 0.9456126657009749, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002849 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:48:29,400] Trial 233 finished with value: 0.6208707066287464 and parameters: {'num_leaves': 148, 'learning_rate': 0.009175868676481173, 'n_estimators': 300, 'subsample': 0.6065564880306887, 'colsample_bytree': 0.9520778776843433, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001708 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:48:46,727] Trial 234 finished with value: 0.6204331656092759 and parameters: {'num_leaves': 148, 'learning_rate': 0.009363336918932624, 'n_estimators': 300, 'subsample': 0.6004490319891252, 'colsample_bytree': 0.9555305783026766, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003016 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:49:05,174] Trial 235 finished with value: 0.620105009844673 and parameters: {'num_leaves': 148, 'learning_rate': 0.00942558456938415, 'n_estimators': 300, 'subsample': 0.600322064612896, 'colsample_bytree': 0.9547781202163881, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001862 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:49:22,787] Trial 236 finished with value: 0.6207613213738788 and parameters: {'num_leaves': 150, 'learning_rate': 0.009173828529956109, 'n_estimators': 300, 'subsample': 0.6069450036568087, 'colsample_bytree': 0.9610957599008758, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001679 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:49:41,075] Trial 237 finished with value: 0.620105009844673 and parameters: {'num_leaves': 150, 'learning_rate': 0.010205503951402002, 'n_estimators': 300, 'subsample': 0.608933698911173, 'colsample_bytree': 0.9651492276541532, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001754 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:49:52,087] Trial 238 finished with value: 0.6055567709472763 and parameters: {'num_leaves': 45, 'learning_rate': 0.009301530528931004, 'n_estimators': 300, 'subsample': 0.6091653209892205, 'colsample_bytree': 0.979235392504884, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:50:09,440] Trial 239 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 144, 'learning_rate': 0.009151206494716312, 'n_estimators': 300, 'subsample': 0.6327194841316318, 'colsample_bytree': 0.9590299677030729, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001766 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:50:21,281] Trial 240 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 145, 'learning_rate': 0.009985277238208837, 'n_estimators': 200, 'subsample': 0.6198253275948802, 'colsample_bytree': 0.9535014407096815, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001686 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:50:38,649] Trial 241 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 147, 'learning_rate': 0.008817371045734101, 'n_estimators': 300, 'subsample': 0.6003697453376158, 'colsample_bytree': 0.9395985331132642, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001694 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:50:55,653] Trial 242 finished with value: 0.607197549770291 and parameters: {'num_leaves': 150, 'learning_rate': 0.048085624202832834, 'n_estimators': 300, 'subsample': 0.6070938090220072, 'colsample_bytree': 0.9486928190109966, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:51:13,264] Trial 243 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 148, 'learning_rate': 0.008054758191615944, 'n_estimators': 300, 'subsample': 0.6070815940211785, 'colsample_bytree': 0.9634197232442624, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001690 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:51:27,903] Trial 244 finished with value: 0.6141982060818202 and parameters: {'num_leaves': 90, 'learning_rate': 0.00841686575090546, 'n_estimators': 300, 'subsample': 0.616186579500281, 'colsample_bytree': 0.9372748208903304, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002904 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:51:45,850] Trial 245 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 142, 'learning_rate': 0.007725661988830237, 'n_estimators': 300, 'subsample': 0.6055778807538789, 'colsample_bytree': 0.9536976172280425, 'min_child_samples': 37}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001908 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:52:03,170] Trial 246 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 141, 'learning_rate': 0.007545719188058976, 'n_estimators': 300, 'subsample': 0.623535487206576, 'colsample_bytree': 0.9574315144723282, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001693 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:52:20,451] Trial 247 finished with value: 0.6207613213738788 and parameters: {'num_leaves': 143, 'learning_rate': 0.009107515604644612, 'n_estimators': 300, 'subsample': 0.6078817176924782, 'colsample_bytree': 0.9705237919845856, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001710 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:52:38,923] Trial 248 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 142, 'learning_rate': 0.009612469928495045, 'n_estimators': 300, 'subsample': 0.6072966305293802, 'colsample_bytree': 0.9755431276980773, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001798 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:52:55,963] Trial 249 finished with value: 0.6173703784729818 and parameters: {'num_leaves': 140, 'learning_rate': 0.010764028469618162, 'n_estimators': 300, 'subsample': 0.6001992157662958, 'colsample_bytree': 0.9488991492982062, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001765 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:53:13,896] Trial 250 finished with value: 0.6204331656092759 and parameters: {'num_leaves': 144, 'learning_rate': 0.009012105976639402, 'n_estimators': 300, 'subsample': 0.6108809177686492, 'colsample_bytree': 0.9665139659819815, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001710 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:53:31,078] Trial 251 finished with value: 0.6215270181579523 and parameters: {'num_leaves': 143, 'learning_rate': 0.008993230282363606, 'n_estimators': 300, 'subsample': 0.6087357849829793, 'colsample_bytree': 0.9699345105448806, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001723 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:54:27,246] Trial 252 finished with value: 0.6113541894552614 and parameters: {'num_leaves': 144, 'learning_rate': 0.008984451136751857, 'n_estimators': 900, 'subsample': 0.6100589189736706, 'colsample_bytree': 0.9697425575266141, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001855 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:54:45,149] Trial 253 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 144, 'learning_rate': 0.009610763333522253, 'n_estimators': 300, 'subsample': 0.626277199136523, 'colsample_bytree': 0.9731199363077845, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001741 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:55:02,137] Trial 254 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 139, 'learning_rate': 0.008769917134267927, 'n_estimators': 300, 'subsample': 0.6135696947825532, 'colsample_bytree': 0.9822140248564398, 'min_child_samples': 32}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001692 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:55:19,931] Trial 255 finished with value: 0.6190111572959965 and parameters: {'num_leaves': 146, 'learning_rate': 0.010077850878142646, 'n_estimators': 300, 'subsample': 0.6064715082227403, 'colsample_bytree': 0.9672835278637903, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001779 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:55:31,319] Trial 256 finished with value: 0.6168234521986437 and parameters: {'num_leaves': 142, 'learning_rate': 0.009406686516524989, 'n_estimators': 200, 'subsample': 0.6199933570010652, 'colsample_bytree': 0.9351090135131694, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002867 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:55:44,307] Trial 257 finished with value: 0.6108072631809233 and parameters: {'num_leaves': 75, 'learning_rate': 0.008698598958142591, 'n_estimators': 300, 'subsample': 0.6002264591354196, 'colsample_bytree': 0.9639115892812175, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001688 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:56:01,540] Trial 258 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 146, 'learning_rate': 0.008448603043221436, 'n_estimators': 300, 'subsample': 0.6323306818648796, 'colsample_bytree': 0.9764373000341452, 'min_child_samples': 32}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001730 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:56:18,947] Trial 259 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 150, 'learning_rate': 0.00910049109576218, 'n_estimators': 300, 'subsample': 0.6129078896200935, 'colsample_bytree': 0.9627992997882276, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001681 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:56:34,881] Trial 260 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 111, 'learning_rate': 0.009916348715677828, 'n_estimators': 300, 'subsample': 0.6227617820102683, 'colsample_bytree': 0.9241381692810305, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001696 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:56:51,747] Trial 261 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 139, 'learning_rate': 0.006730818578338437, 'n_estimators': 300, 'subsample': 0.608733962860975, 'colsample_bytree': 0.9409396111026298, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001745 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:57:09,109] Trial 262 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 145, 'learning_rate': 0.007945516136547297, 'n_estimators': 300, 'subsample': 0.600190520227327, 'colsample_bytree': 0.9306253193223478, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001729 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:57:27,283] Trial 263 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 143, 'learning_rate': 0.008611639430245314, 'n_estimators': 300, 'subsample': 0.6136280644647115, 'colsample_bytree': 0.9567800304127702, 'min_child_samples': 24}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001732 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:57:44,897] Trial 264 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 148, 'learning_rate': 0.009266530868917303, 'n_estimators': 300, 'subsample': 0.6206576783919532, 'colsample_bytree': 0.9390789115451492, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001792 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:58:02,693] Trial 265 finished with value: 0.6191205425508641 and parameters: {'num_leaves': 146, 'learning_rate': 0.008183067586109942, 'n_estimators': 300, 'subsample': 0.6441060338390148, 'colsample_bytree': 0.946609835724913, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002664 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:58:13,781] Trial 266 finished with value: 0.6158389849048348 and parameters: {'num_leaves': 143, 'learning_rate': 0.007116031655492876, 'n_estimators': 200, 'subsample': 0.6081287845371832, 'colsample_bytree': 0.9706399487735523, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002980 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:58:31,875] Trial 267 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 150, 'learning_rate': 0.01068489131665375, 'n_estimators': 300, 'subsample': 0.6287486553422462, 'colsample_bytree': 0.9327746619216994, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002141 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:58:48,892] Trial 268 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 138, 'learning_rate': 0.008916241024618653, 'n_estimators': 300, 'subsample': 0.600241826575575, 'colsample_bytree': 0.9619051979748552, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001723 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:59:07,114] Trial 269 finished with value: 0.6184642310216583 and parameters: {'num_leaves': 147, 'learning_rate': 0.007620359467477964, 'n_estimators': 300, 'subsample': 0.6134974193505077, 'colsample_bytree': 0.9890110116708237, 'min_child_samples': 31}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001742 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:59:24,323] Trial 270 finished with value: 0.6187923867862613 and parameters: {'num_leaves': 142, 'learning_rate': 0.009599377458574457, 'n_estimators': 300, 'subsample': 0.6073463689427594, 'colsample_bytree': 0.9220260305778345, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001747 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 18:59:40,794] Trial 271 finished with value: 0.611791730474732 and parameters: {'num_leaves': 145, 'learning_rate': 0.0357530164641686, 'n_estimators': 300, 'subsample': 0.6215829470727271, 'colsample_bytree': 0.9495787599657957, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003136 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:00:30,534] Trial 272 finished with value: 0.6158389849048348 and parameters: {'num_leaves': 148, 'learning_rate': 0.0083881514134853, 'n_estimators': 800, 'subsample': 0.6380023533660646, 'colsample_bytree': 0.9419166246487296, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001727 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:00:48,645] Trial 273 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 140, 'learning_rate': 0.009005944475550181, 'n_estimators': 300, 'subsample': 0.6148137728341603, 'colsample_bytree': 0.9551876004972074, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001732 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:01:06,028] Trial 274 finished with value: 0.621308247648217 and parameters: {'num_leaves': 145, 'learning_rate': 0.007895440208958507, 'n_estimators': 300, 'subsample': 0.6517744850247522, 'colsample_bytree': 0.9302294036987285, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001631 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:01:17,143] Trial 275 finished with value: 0.5680376285276745 and parameters: {'num_leaves': 150, 'learning_rate': 0.0074295892499840685, 'n_estimators': 200, 'subsample': 0.6518832458745394, 'colsample_bytree': 0.7721543036506527, 'min_child_samples': 38}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001693 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:01:34,286] Trial 276 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 143, 'learning_rate': 0.0064459350681334284, 'n_estimators': 300, 'subsample': 0.637965010707432, 'colsample_bytree': 0.9298480386083954, 'min_child_samples': 37}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003024 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:01:52,720] Trial 277 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 147, 'learning_rate': 0.008061999681505403, 'n_estimators': 300, 'subsample': 0.6526935001518283, 'colsample_bytree': 0.9168197899904311, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001777 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:02:09,993] Trial 278 finished with value: 0.6180266900021877 and parameters: {'num_leaves': 145, 'learning_rate': 0.007080627619193248, 'n_estimators': 300, 'subsample': 0.7731070958128189, 'colsample_bytree': 0.9348928119019441, 'min_child_samples': 32}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001722 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:02:28,310] Trial 279 finished with value: 0.6206519361190111 and parameters: {'num_leaves': 148, 'learning_rate': 0.007732672277775027, 'n_estimators': 300, 'subsample': 0.6297003168296137, 'colsample_bytree': 0.9238255863120978, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001725 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:02:46,034] Trial 280 finished with value: 0.6195580835703347 and parameters: {'num_leaves': 150, 'learning_rate': 0.007719677903499846, 'n_estimators': 300, 'subsample': 0.6305691268394346, 'colsample_bytree': 0.9220010125739526, 'min_child_samples': 37}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001735 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:03:04,178] Trial 281 finished with value: 0.6192299278057318 and parameters: {'num_leaves': 148, 'learning_rate': 0.006983033057413817, 'n_estimators': 300, 'subsample': 0.639466364507453, 'colsample_bytree': 0.9259572435692079, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002987 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:03:21,994] Trial 282 finished with value: 0.6196674688252024 and parameters: {'num_leaves': 147, 'learning_rate': 0.007842625291749205, 'n_estimators': 300, 'subsample': 0.8651966892492584, 'colsample_bytree': 0.9174558890483636, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001766 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:03:27,403] Trial 283 finished with value: 0.612994968278276 and parameters: {'num_leaves': 141, 'learning_rate': 0.06389934084114078, 'n_estimators': 100, 'subsample': 0.6275301602719731, 'colsample_bytree': 0.9354607943914766, 'min_child_samples': 38}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:03:40,472] Trial 284 finished with value: 0.6106978779260556 and parameters: {'num_leaves': 67, 'learning_rate': 0.007396185421207303, 'n_estimators': 300, 'subsample': 0.6487382940598145, 'colsample_bytree': 0.9404911158884199, 'min_child_samples': 39}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002886 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:03:59,006] Trial 285 finished with value: 0.6203237803544083 and parameters: {'num_leaves': 150, 'learning_rate': 0.008302745179376807, 'n_estimators': 300, 'subsample': 0.6001169429136649, 'colsample_bytree': 0.9299101401429111, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:04:16,642] Trial 286 finished with value: 0.6206519361190111 and parameters: {'num_leaves': 146, 'learning_rate': 0.008173665501616604, 'n_estimators': 300, 'subsample': 0.6200804763607866, 'colsample_bytree': 0.9519504034927615, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001745 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:04:28,396] Trial 287 finished with value: 0.6166046816889084 and parameters: {'num_leaves': 138, 'learning_rate': 0.007892476744818583, 'n_estimators': 200, 'subsample': 0.6321106168187607, 'colsample_bytree': 0.9431215327933984, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001998 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:04:46,393] Trial 288 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 145, 'learning_rate': 0.007566516547551842, 'n_estimators': 300, 'subsample': 0.6259930612898716, 'colsample_bytree': 0.9298286287732975, 'min_child_samples': 37}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002457 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:05:03,663] Trial 289 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 142, 'learning_rate': 0.006697629664557117, 'n_estimators': 300, 'subsample': 0.6569810184873031, 'colsample_bytree': 0.9494966719100093, 'min_child_samples': 38}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001688 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:05:20,882] Trial 290 finished with value: 0.6198862393349377 and parameters: {'num_leaves': 146, 'learning_rate': 0.008345647249416607, 'n_estimators': 300, 'subsample': 0.6200416695906574, 'colsample_bytree': 0.937673919286454, 'min_child_samples': 37}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001783 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:05:39,068] Trial 291 finished with value: 0.618245460511923 and parameters: {'num_leaves': 143, 'learning_rate': 0.006188915499289655, 'n_estimators': 300, 'subsample': 0.8260965470237124, 'colsample_bytree': 0.9206425610893123, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:05:54,149] Trial 292 finished with value: 0.6162765259243054 and parameters: {'num_leaves': 106, 'learning_rate': 0.0073254823250787655, 'n_estimators': 300, 'subsample': 0.6187868414784631, 'colsample_bytree': 0.950724693380591, 'min_child_samples': 35}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001807 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:06:11,526] Trial 293 finished with value: 0.6171516079632465 and parameters: {'num_leaves': 148, 'learning_rate': 0.018575358611553523, 'n_estimators': 300, 'subsample': 0.6408664561846904, 'colsample_bytree': 0.942543031256689, 'min_child_samples': 39}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001722 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:06:29,494] Trial 294 finished with value: 0.6202143950995406 and parameters: {'num_leaves': 141, 'learning_rate': 0.008203742887377909, 'n_estimators': 300, 'subsample': 0.6140169790946791, 'colsample_bytree': 0.9335645922699894, 'min_child_samples': 37}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:06:41,456] Trial 295 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 146, 'learning_rate': 0.008566950378075732, 'n_estimators': 200, 'subsample': 0.6313497094815784, 'colsample_bytree': 0.914380718112231, 'min_child_samples': 49}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001912 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:06:58,845] Trial 296 finished with value: 0.6210894771384817 and parameters: {'num_leaves': 150, 'learning_rate': 0.007830791784360094, 'n_estimators': 300, 'subsample': 0.6210758558649069, 'colsample_bytree': 0.9595970659795937, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001755 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:07:16,658] Trial 297 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 150, 'learning_rate': 0.007856699307429304, 'n_estimators': 300, 'subsample': 0.6238793503282543, 'colsample_bytree': 0.9597475103598426, 'min_child_samples': 33}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003025 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:07:34,297] Trial 298 finished with value: 0.6189017720411288 and parameters: {'num_leaves': 144, 'learning_rate': 0.007092419211974109, 'n_estimators': 300, 'subsample': 0.6462892770215294, 'colsample_bytree': 0.9757902590328354, 'min_child_samples': 36}. Best is trial 201 with value: 0.6222927149420258.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001698 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-13 19:07:51,693] Trial 299 finished with value: 0.6194486983154671 and parameters: {'num_leaves': 148, 'learning_rate': 0.007585295740555217, 'n_estimators': 300, 'subsample': 0.6338223963963796, 'colsample_bytree': 0.9640628619761106, 'min_child_samples': 34}. Best is trial 201 with value: 0.6222927149420258.\n","[I 2025-03-13 19:07:51,694] A new study created in memory with name: no-name-045ed30b-3d0f-4426-92ed-830aa05dabcc\n","[I 2025-03-13 19:08:15,331] Trial 0 finished with value: 0.6001968934587617 and parameters: {'n_estimators': 900, 'max_depth': 4, 'learning_rate': 0.0428108281769119, 'subsample': 0.725092511008717, 'colsample_bytree': 0.803349907508063, 'gamma': 0.07571173974232502, 'min_child_weight': 5}. Best is trial 0 with value: 0.6001968934587617.\n","[I 2025-03-13 19:08:40,801] Trial 1 finished with value: 0.6016189017720411 and parameters: {'n_estimators': 1000, 'max_depth': 4, 'learning_rate': 0.07419738084428942, 'subsample': 0.7679272121528915, 'colsample_bytree': 0.9805406148625537, 'gamma': 0.13554546357238967, 'min_child_weight': 3}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:08:54,735] Trial 2 finished with value: 0.5595055786479982 and parameters: {'n_estimators': 600, 'max_depth': 3, 'learning_rate': 0.013304727316805025, 'subsample': 0.7700242593777988, 'colsample_bytree': 0.8908639207240856, 'gamma': 0.10028447156224513, 'min_child_weight': 1}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:09:17,219] Trial 3 finished with value: 0.5985561146357471 and parameters: {'n_estimators': 1000, 'max_depth': 3, 'learning_rate': 0.06794538056451031, 'subsample': 0.9597387604580446, 'colsample_bytree': 0.9892837775733258, 'gamma': 0.15609089275888077, 'min_child_weight': 2}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:09:53,273] Trial 4 finished with value: 0.533800043754102 and parameters: {'n_estimators': 1000, 'max_depth': 10, 'learning_rate': 0.022784483605945965, 'subsample': 0.6770624244650687, 'colsample_bytree': 0.6572581189938339, 'gamma': 0.28517692719124466, 'min_child_weight': 6}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:10:02,501] Trial 5 finished with value: 0.5905709910304091 and parameters: {'n_estimators': 100, 'max_depth': 12, 'learning_rate': 0.010169544557358403, 'subsample': 0.6829827164348252, 'colsample_bytree': 0.7689014108594113, 'gamma': 0.21378161310647262, 'min_child_weight': 1}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:10:35,402] Trial 6 finished with value: 0.5898052942463355 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.006138871459062049, 'subsample': 0.7591680267326414, 'colsample_bytree': 0.9177360378047795, 'gamma': 0.1708787943338493, 'min_child_weight': 6}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:10:40,254] Trial 7 finished with value: 0.5705534893896302 and parameters: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.005923286561174158, 'subsample': 0.734439501125579, 'colsample_bytree': 0.9670535593212597, 'gamma': 0.039089850012999305, 'min_child_weight': 6}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:10:48,806] Trial 8 finished with value: 0.5527236928462044 and parameters: {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.013717577989663145, 'subsample': 0.6048837692169002, 'colsample_bytree': 0.9596140480482167, 'gamma': 0.12422573192559128, 'min_child_weight': 6}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:11:16,381] Trial 9 finished with value: 0.5818201706409976 and parameters: {'n_estimators': 500, 'max_depth': 11, 'learning_rate': 0.0763691129501314, 'subsample': 0.8752969155177428, 'colsample_bytree': 0.7031674119140487, 'gamma': 0.04538979354049857, 'min_child_weight': 5}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:11:43,469] Trial 10 finished with value: 0.6005250492233647 and parameters: {'n_estimators': 700, 'max_depth': 8, 'learning_rate': 0.03597105541909009, 'subsample': 0.856064697389035, 'colsample_bytree': 0.8518376910399097, 'gamma': 0.22949631339950116, 'min_child_weight': 3}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:12:10,708] Trial 11 finished with value: 0.6012907460074381 and parameters: {'n_estimators': 700, 'max_depth': 8, 'learning_rate': 0.03634794827970237, 'subsample': 0.8453890028651403, 'colsample_bytree': 0.85009213258436, 'gamma': 0.23378045888181787, 'min_child_weight': 3}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:12:39,260] Trial 12 finished with value: 0.5977904178516736 and parameters: {'n_estimators': 800, 'max_depth': 8, 'learning_rate': 0.04809141484464668, 'subsample': 0.8443383273789745, 'colsample_bytree': 0.8329614344644716, 'gamma': 0.2745499195038918, 'min_child_weight': 3}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:13:01,245] Trial 13 finished with value: 0.6006344344782323 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.08863921202756499, 'subsample': 0.9334628373937727, 'colsample_bytree': 0.7640715741912684, 'gamma': 0.1961568872188334, 'min_child_weight': 4}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:13:15,680] Trial 14 finished with value: 0.5316123386567491 and parameters: {'n_estimators': 400, 'max_depth': 9, 'learning_rate': 0.029987266873714398, 'subsample': 0.8176004661436491, 'colsample_bytree': 0.6049205225820272, 'gamma': 0.23354390396448255, 'min_child_weight': 2}. Best is trial 1 with value: 0.6016189017720411.\n","[I 2025-03-13 19:13:36,133] Trial 15 finished with value: 0.6044629183985999 and parameters: {'n_estimators': 700, 'max_depth': 5, 'learning_rate': 0.056624586904769805, 'subsample': 0.9100827748786056, 'colsample_bytree': 0.915185875919527, 'gamma': 0.00574266649765881, 'min_child_weight': 4}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:13:43,550] Trial 16 finished with value: 0.5974622620870707 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.05790783045675436, 'subsample': 0.9959778460597558, 'colsample_bytree': 0.9161979081170403, 'gamma': 0.006719571913911815, 'min_child_weight': 4}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:14:06,178] Trial 17 finished with value: 0.6007438197331 and parameters: {'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.09782681467665694, 'subsample': 0.9022173539274981, 'colsample_bytree': 0.9981396291237574, 'gamma': 0.0071115066411667205, 'min_child_weight': 4}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:14:24,101] Trial 18 finished with value: 0.5991030409100854 and parameters: {'n_estimators': 600, 'max_depth': 5, 'learning_rate': 0.02510243251947011, 'subsample': 0.7974859068271338, 'colsample_bytree': 0.9322002570523468, 'gamma': 0.11994737356032992, 'min_child_weight': 2}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:14:45,720] Trial 19 finished with value: 0.6015095165171734 and parameters: {'n_estimators': 900, 'max_depth': 4, 'learning_rate': 0.05189761145063273, 'subsample': 0.9196630034784518, 'colsample_bytree': 0.8820740635249191, 'gamma': 0.06302343122412872, 'min_child_weight': 5}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:15:11,557] Trial 20 finished with value: 0.6000875082038941 and parameters: {'n_estimators': 700, 'max_depth': 7, 'learning_rate': 0.06426254945735323, 'subsample': 0.6106021910665849, 'colsample_bytree': 0.9414289200504904, 'gamma': 0.11033644693207313, 'min_child_weight': 3}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:15:34,614] Trial 21 finished with value: 0.6012907460074381 and parameters: {'n_estimators': 900, 'max_depth': 4, 'learning_rate': 0.048575130054233896, 'subsample': 0.9077391091329082, 'colsample_bytree': 0.8875201987771795, 'gamma': 0.05978767434679463, 'min_child_weight': 5}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:15:57,215] Trial 22 finished with value: 0.6014001312623058 and parameters: {'n_estimators': 900, 'max_depth': 4, 'learning_rate': 0.05642441822008898, 'subsample': 0.9748530897409606, 'colsample_bytree': 0.8852643885867376, 'gamma': 0.024682165476627917, 'min_child_weight': 5}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:16:19,891] Trial 23 finished with value: 0.6003062787136294 and parameters: {'n_estimators': 900, 'max_depth': 4, 'learning_rate': 0.08227437607408898, 'subsample': 0.9353756225733902, 'colsample_bytree': 0.9619015797021229, 'gamma': 0.08153898929182883, 'min_child_weight': 4}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:16:43,554] Trial 24 finished with value: 0.5976810325968059 and parameters: {'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.01793157937238774, 'subsample': 0.8904714987108262, 'colsample_bytree': 0.8787027572063058, 'gamma': 0.06843007346704766, 'min_child_weight': 4}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:17:17,667] Trial 25 finished with value: 0.6000875082038941 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.037947555468822405, 'subsample': 0.7940633667823589, 'colsample_bytree': 0.8158647447851347, 'gamma': 0.0280298441145515, 'min_child_weight': 5}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:17:34,903] Trial 26 finished with value: 0.5833515642091446 and parameters: {'n_estimators': 700, 'max_depth': 3, 'learning_rate': 0.03108662026678139, 'subsample': 0.9321689926568892, 'colsample_bytree': 0.908173443679203, 'gamma': 0.13481974129169824, 'min_child_weight': 3}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:18:02,866] Trial 27 finished with value: 0.6000875082038941 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.07180897314381113, 'subsample': 0.820401415265186, 'colsample_bytree': 0.858351526850673, 'gamma': 0.09435260297084921, 'min_child_weight': 4}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:18:18,249] Trial 28 finished with value: 0.6014001312623058 and parameters: {'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.05103652989438376, 'subsample': 0.7058584401005505, 'colsample_bytree': 0.7741330278966242, 'gamma': 0.002934732700233589, 'min_child_weight': 2}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:18:42,725] Trial 29 finished with value: 0.6030409100853205 and parameters: {'n_estimators': 900, 'max_depth': 5, 'learning_rate': 0.04421711518721929, 'subsample': 0.9987670684690229, 'colsample_bytree': 0.9511121501496023, 'gamma': 0.05663075076005599, 'min_child_weight': 5}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:19:05,046] Trial 30 finished with value: 0.6023845985561146 and parameters: {'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.0421178020121333, 'subsample': 0.963649261610511, 'colsample_bytree': 0.9796306968193172, 'gamma': 0.17133691663957357, 'min_child_weight': 4}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:19:24,920] Trial 31 finished with value: 0.6010719754977029 and parameters: {'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.044149457325095086, 'subsample': 0.9966512617746451, 'colsample_bytree': 0.9802965141938205, 'gamma': 0.1604786814794591, 'min_child_weight': 4}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:19:52,196] Trial 32 finished with value: 0.6042441478888646 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.041128906148980404, 'subsample': 0.9620907340279191, 'colsample_bytree': 0.9520162949278874, 'gamma': 0.19357834397419188, 'min_child_weight': 3}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:20:16,222] Trial 33 finished with value: 0.6036972216145263 and parameters: {'n_estimators': 700, 'max_depth': 7, 'learning_rate': 0.0281381881842843, 'subsample': 0.9560593414055825, 'colsample_bytree': 0.9456480669792113, 'gamma': 0.1864025817528355, 'min_child_weight': 5}. Best is trial 15 with value: 0.6044629183985999.\n","[I 2025-03-13 19:20:33,884] Trial 34 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.026689969571631323, 'subsample': 0.9613277788545755, 'colsample_bytree': 0.9436237662706118, 'gamma': 0.18613524460564185, 'min_child_weight': 5}. Best is trial 34 with value: 0.6045723036534675.\n","[I 2025-03-13 19:20:57,712] Trial 35 finished with value: 0.6021658280463793 and parameters: {'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.020181555770231025, 'subsample': 0.951026954426804, 'colsample_bytree': 0.92420414598926, 'gamma': 0.19539095758299369, 'min_child_weight': 5}. Best is trial 34 with value: 0.6045723036534675.\n","[I 2025-03-13 19:21:17,907] Trial 36 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 600, 'max_depth': 7, 'learning_rate': 0.027661199606396084, 'subsample': 0.9605512241187454, 'colsample_bytree': 0.9995764208887494, 'gamma': 0.2614019090206135, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:21:41,131] Trial 37 finished with value: 0.6033690658499234 and parameters: {'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.015795816601457683, 'subsample': 0.9774060965275987, 'colsample_bytree': 0.9951797584797654, 'gamma': 0.2665565890119473, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:21:52,746] Trial 38 finished with value: 0.5895865237366003 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.010101726936085532, 'subsample': 0.878899889934039, 'colsample_bytree': 0.9712245285323949, 'gamma': 0.26000832117066386, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:22:05,854] Trial 39 finished with value: 0.5940713191861737 and parameters: {'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.024120559960243135, 'subsample': 0.9437368028872983, 'colsample_bytree': 0.9110412554265067, 'gamma': 0.296186650320611, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:22:38,478] Trial 40 finished with value: 0.5975716473419384 and parameters: {'n_estimators': 600, 'max_depth': 10, 'learning_rate': 0.021438905114613535, 'subsample': 0.9756283597106991, 'colsample_bytree': 0.9340869578064509, 'gamma': 0.2476027386419014, 'min_child_weight': 1}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:22:59,399] Trial 41 finished with value: 0.6041347626339969 and parameters: {'n_estimators': 600, 'max_depth': 7, 'learning_rate': 0.029835141295011643, 'subsample': 0.9596257170303778, 'colsample_bytree': 0.9461266188189852, 'gamma': 0.19810204058910308, 'min_child_weight': 5}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:23:18,787] Trial 42 finished with value: 0.6039159921242616 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.03261178013153973, 'subsample': 0.914758621841243, 'colsample_bytree': 0.9701230487488792, 'gamma': 0.2122479913192206, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:23:41,040] Trial 43 finished with value: 0.6044629183985999 and parameters: {'n_estimators': 600, 'max_depth': 7, 'learning_rate': 0.028261085869983216, 'subsample': 0.9715805972721792, 'colsample_bytree': 0.8999334119334406, 'gamma': 0.1459500224088268, 'min_child_weight': 5}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:23:59,469] Trial 44 finished with value: 0.601947057536644 and parameters: {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.02663298872150534, 'subsample': 0.9797735460201938, 'colsample_bytree': 0.9042061718519461, 'gamma': 0.14297606945311617, 'min_child_weight': 3}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:24:21,164] Trial 45 finished with value: 0.6042441478888646 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.019080622644026633, 'subsample': 0.9234329994331479, 'colsample_bytree': 0.9991765335305473, 'gamma': 0.2163659841058162, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:24:34,376] Trial 46 finished with value: 0.6014001312623058 and parameters: {'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.03566304504868448, 'subsample': 0.8789031613478399, 'colsample_bytree': 0.8654524709014307, 'gamma': 0.17115773309171234, 'min_child_weight': 5}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:24:46,127] Trial 47 finished with value: 0.5918836140888208 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.015542196817514457, 'subsample': 0.9395704247218303, 'colsample_bytree': 0.7130462193426319, 'gamma': 0.2523442324818312, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:25:13,394] Trial 48 finished with value: 0.5993218114198207 and parameters: {'n_estimators': 700, 'max_depth': 8, 'learning_rate': 0.04097389028764664, 'subsample': 0.8931085310814942, 'colsample_bytree': 0.9006491110653575, 'gamma': 0.15632757607957287, 'min_child_weight': 4}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:25:36,749] Trial 49 finished with value: 0.6018376722817764 and parameters: {'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.02342426002437032, 'subsample': 0.969756009685874, 'colsample_bytree': 0.9255388953263163, 'gamma': 0.28062968273949546, 'min_child_weight': 3}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:26:18,772] Trial 50 finished with value: 0.5725224239772478 and parameters: {'n_estimators': 1000, 'max_depth': 12, 'learning_rate': 0.06177414915092743, 'subsample': 0.8625515848668973, 'colsample_bytree': 0.8324346185083608, 'gamma': 0.18189433034207678, 'min_child_weight': 2}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:26:40,018] Trial 51 finished with value: 0.6040253773791293 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.01917378216188373, 'subsample': 0.927829084150779, 'colsample_bytree': 0.9947024290719066, 'gamma': 0.23220204537149713, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:27:05,564] Trial 52 finished with value: 0.6026033690658499 and parameters: {'n_estimators': 600, 'max_depth': 8, 'learning_rate': 0.01207417738278706, 'subsample': 0.9194211483982017, 'colsample_bytree': 0.9613341740897159, 'gamma': 0.21326890316124283, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:27:20,260] Trial 53 finished with value: 0.5973528768322031 and parameters: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.017815304002612747, 'subsample': 0.9855880024695091, 'colsample_bytree': 0.9779988211150433, 'gamma': 0.22036005635680908, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:27:40,858] Trial 54 finished with value: 0.602275213301247 and parameters: {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.026887281583583608, 'subsample': 0.9503733846104534, 'colsample_bytree': 0.9997680813284485, 'gamma': 0.2041301536426983, 'min_child_weight': 5}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:28:13,271] Trial 55 finished with value: 0.5931962371472326 and parameters: {'n_estimators': 700, 'max_depth': 10, 'learning_rate': 0.03219867180321026, 'subsample': 0.9067372490706923, 'colsample_bytree': 0.9603981019963159, 'gamma': 0.1847249436759352, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:28:33,604] Trial 56 finished with value: 0.6020564427915117 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.021245742231759873, 'subsample': 0.9615852880755384, 'colsample_bytree': 0.9367284992411411, 'gamma': 0.24772727005807688, 'min_child_weight': 5}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:28:42,232] Trial 57 finished with value: 0.5975716473419384 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.03686205148621862, 'subsample': 0.7513397029251135, 'colsample_bytree': 0.9844106148066986, 'gamma': 0.22566025637879164, 'min_child_weight': 3}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:28:59,950] Trial 58 finished with value: 0.6008532049879676 and parameters: {'n_estimators': 400, 'max_depth': 9, 'learning_rate': 0.024850728774936794, 'subsample': 0.9850866389317989, 'colsample_bytree': 0.9534251404560801, 'gamma': 0.292266034221666, 'min_child_weight': 6}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:29:48,728] Trial 59 finished with value: 0.5987748851454824 and parameters: {'n_estimators': 700, 'max_depth': 11, 'learning_rate': 0.006893625367362767, 'subsample': 0.9287503727980893, 'colsample_bytree': 0.8940879304221323, 'gamma': 0.13124685856087515, 'min_child_weight': 4}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:30:06,376] Trial 60 finished with value: 0.6039159921242616 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.055596054388589805, 'subsample': 0.9469382980769324, 'colsample_bytree': 0.9280271703282427, 'gamma': 0.14677642788111533, 'min_child_weight': 5}. Best is trial 36 with value: 0.6058849267118792.\n","[I 2025-03-13 19:30:26,825] Trial 61 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 600, 'max_depth': 7, 'learning_rate': 0.030324251679905892, 'subsample': 0.9622337815303449, 'colsample_bytree': 0.9495235238079315, 'gamma': 0.20261800903169638, 'min_child_weight': 5}. Best is trial 61 with value: 0.6059943119667469.\n","[I 2025-03-13 19:30:47,563] Trial 62 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.03446726669178378, 'subsample': 0.6403098258023674, 'colsample_bytree': 0.9692572748675531, 'gamma': 0.23853641006288895, 'min_child_weight': 5}. Best is trial 61 with value: 0.6059943119667469.\n","[I 2025-03-13 19:31:06,597] Trial 63 finished with value: 0.6032596805950557 and parameters: {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.033037810432145605, 'subsample': 0.6318491377863106, 'colsample_bytree': 0.8722432876820417, 'gamma': 0.2425486960395788, 'min_child_weight': 5}. Best is trial 61 with value: 0.6059943119667469.\n","[I 2025-03-13 19:31:30,070] Trial 64 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.028661147497991098, 'subsample': 0.7340837535528082, 'colsample_bytree': 0.9208914224102731, 'gamma': 0.2723075437790682, 'min_child_weight': 5}. Best is trial 61 with value: 0.6059943119667469.\n","[I 2025-03-13 19:31:53,595] Trial 65 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.028517949676913693, 'subsample': 0.6569409433832527, 'colsample_bytree': 0.916363371437744, 'gamma': 0.2675939323140989, 'min_child_weight': 5}. Best is trial 61 with value: 0.6059943119667469.\n","[I 2025-03-13 19:32:17,213] Trial 66 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.03461069888336922, 'subsample': 0.6612098221098578, 'colsample_bytree': 0.916674372214809, 'gamma': 0.2698302198456488, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:32:40,845] Trial 67 finished with value: 0.6044629183985999 and parameters: {'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.034662076239168275, 'subsample': 0.6672276283737137, 'colsample_bytree': 0.9192374186180017, 'gamma': 0.2731433813122001, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:33:07,113] Trial 68 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.03026645482389181, 'subsample': 0.6584897513550195, 'colsample_bytree': 0.9687701887576105, 'gamma': 0.2837471880886229, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:33:33,569] Trial 69 finished with value: 0.6055567709472763 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.025937079687001093, 'subsample': 0.7143574170179698, 'colsample_bytree': 0.839682857970636, 'gamma': 0.284546887860933, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:33:57,792] Trial 70 finished with value: 0.6016189017720411 and parameters: {'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.022600525161865042, 'subsample': 0.7053802707853006, 'colsample_bytree': 0.839400219725718, 'gamma': 0.2843156358961933, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:34:24,202] Trial 71 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.029243738223162986, 'subsample': 0.6674399585288422, 'colsample_bytree': 0.7985883844846957, 'gamma': 0.2600686793934331, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:34:50,495] Trial 72 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.030270242768415, 'subsample': 0.6671413286106656, 'colsample_bytree': 0.8039601016337222, 'gamma': 0.2604940570668254, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:35:13,955] Trial 73 finished with value: 0.6031502953401882 and parameters: {'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.03779547136898339, 'subsample': 0.6916585586061977, 'colsample_bytree': 0.7879235793131498, 'gamma': 0.27493872304374584, 'min_child_weight': 4}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:35:38,030] Trial 74 finished with value: 0.6018376722817764 and parameters: {'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.024633595257703457, 'subsample': 0.6490536569283658, 'colsample_bytree': 0.7674809030414681, 'gamma': 0.2999073638880596, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:36:03,912] Trial 75 finished with value: 0.6036972216145263 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.02982920720823658, 'subsample': 0.7262845925660877, 'colsample_bytree': 0.7552778420490138, 'gamma': 0.26217340322553, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:36:25,432] Trial 76 finished with value: 0.603806606869394 and parameters: {'n_estimators': 700, 'max_depth': 5, 'learning_rate': 0.02649818635118018, 'subsample': 0.6146487128547908, 'colsample_bytree': 0.7419836664837278, 'gamma': 0.2902134500945792, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:36:51,420] Trial 77 finished with value: 0.6040253773791293 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.03061532321208699, 'subsample': 0.6863116302155371, 'colsample_bytree': 0.8146984383211978, 'gamma': 0.2698966179563879, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:37:10,704] Trial 78 finished with value: 0.6046816889083352 and parameters: {'n_estimators': 700, 'max_depth': 5, 'learning_rate': 0.039912190499200195, 'subsample': 0.7437235718385812, 'colsample_bytree': 0.8239497539432228, 'gamma': 0.28069834286769785, 'min_child_weight': 4}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:37:33,330] Trial 79 finished with value: 0.535331437322249 and parameters: {'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.021764259133030404, 'subsample': 0.6583651938635517, 'colsample_bytree': 0.6539140480225546, 'gamma': 0.2565170668675091, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:38:01,922] Trial 80 finished with value: 0.602275213301247 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.032889327394212534, 'subsample': 0.7055271742320978, 'colsample_bytree': 0.7968331023955639, 'gamma': 0.28672448690159413, 'min_child_weight': 4}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:38:28,225] Trial 81 finished with value: 0.6044629183985999 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.028808076595091582, 'subsample': 0.7734453922976224, 'colsample_bytree': 0.8049845850328586, 'gamma': 0.26029264019433174, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:38:58,767] Trial 82 finished with value: 0.605338000437541 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.0257523610446955, 'subsample': 0.6642971914970939, 'colsample_bytree': 0.8462491686594936, 'gamma': 0.26593927693225844, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:39:27,668] Trial 83 finished with value: 0.6027127543207176 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.04403200933117147, 'subsample': 0.6257994135675681, 'colsample_bytree': 0.8612907749667011, 'gamma': 0.2668978808839741, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:39:54,114] Trial 84 finished with value: 0.603478451104791 and parameters: {'n_estimators': 900, 'max_depth': 5, 'learning_rate': 0.026653907331935282, 'subsample': 0.6792737818923145, 'colsample_bytree': 0.8483243052507295, 'gamma': 0.27668725889553836, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:40:27,493] Trial 85 finished with value: 0.6003062787136294 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.04726256222931959, 'subsample': 0.6490238062719068, 'colsample_bytree': 0.9145750228195173, 'gamma': 0.2513451883529067, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:40:53,815] Trial 86 finished with value: 0.6049004594180705 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.025322166993230293, 'subsample': 0.6974350710014989, 'colsample_bytree': 0.9886884795045987, 'gamma': 0.2677181883607874, 'min_child_weight': 5}. Best is trial 66 with value: 0.6062130824764822.\n","[I 2025-03-13 19:41:22,542] Trial 87 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.024292492134522285, 'subsample': 0.6947791258031076, 'colsample_bytree': 0.9871998374830244, 'gamma': 0.24092036662558722, 'min_child_weight': 6}. Best is trial 87 with value: 0.6063224677313498.\n","[I 2025-03-13 19:41:53,290] Trial 88 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.0229863575224573, 'subsample': 0.69295113074739, 'colsample_bytree': 0.9859922904777261, 'gamma': 0.2406249644135597, 'min_child_weight': 6}. Best is trial 87 with value: 0.6063224677313498.\n","[I 2025-03-13 19:42:29,893] Trial 89 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.02306191277688958, 'subsample': 0.6695906186674925, 'colsample_bytree': 0.9795238286819717, 'gamma': 0.2400234638690303, 'min_child_weight': 6}. Best is trial 87 with value: 0.6063224677313498.\n","[I 2025-03-13 19:43:06,513] Trial 90 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.016619455635913005, 'subsample': 0.7205824356914835, 'colsample_bytree': 0.9750830617256577, 'gamma': 0.23873247445127735, 'min_child_weight': 6}. Best is trial 90 with value: 0.6064318529862175.\n","[I 2025-03-13 19:43:42,340] Trial 91 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.015906284995498194, 'subsample': 0.7153359270392353, 'colsample_bytree': 0.9754918949198716, 'gamma': 0.24311542918134357, 'min_child_weight': 6}. Best is trial 90 with value: 0.6064318529862175.\n","[I 2025-03-13 19:44:19,340] Trial 92 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014295639222427658, 'subsample': 0.7209791127643901, 'colsample_bytree': 0.974848917435184, 'gamma': 0.24442076244321692, 'min_child_weight': 6}. Best is trial 92 with value: 0.6065412382410851.\n","[I 2025-03-13 19:44:56,654] Trial 93 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013734719018626586, 'subsample': 0.722177151978538, 'colsample_bytree': 0.9755222130314909, 'gamma': 0.24284375889083432, 'min_child_weight': 6}. Best is trial 92 with value: 0.6065412382410851.\n","[I 2025-03-13 19:45:39,657] Trial 94 finished with value: 0.6055567709472763 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.012253536417374152, 'subsample': 0.7237475327197186, 'colsample_bytree': 0.9732788963924836, 'gamma': 0.22455964579825324, 'min_child_weight': 6}. Best is trial 92 with value: 0.6065412382410851.\n","[I 2025-03-13 19:46:17,017] Trial 95 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014568522302083019, 'subsample': 0.7165214878009004, 'colsample_bytree': 0.9884165345283801, 'gamma': 0.23109274066572438, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:46:54,060] Trial 96 finished with value: 0.605338000437541 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.015340003886411132, 'subsample': 0.7662333070819322, 'colsample_bytree': 0.9870971746642174, 'gamma': 0.23382912710638953, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:47:30,657] Trial 97 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014402417403123619, 'subsample': 0.7156821575383903, 'colsample_bytree': 0.9548463360278239, 'gamma': 0.2445642962135054, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:48:11,846] Trial 98 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.014164577138396363, 'subsample': 0.7210135541700933, 'colsample_bytree': 0.9541055706160112, 'gamma': 0.24764721450295082, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:48:49,839] Trial 99 finished with value: 0.6032596805950557 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.009299786764560397, 'subsample': 0.7848188097399607, 'colsample_bytree': 0.9618405187669208, 'gamma': 0.2048004480859995, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:49:27,466] Trial 100 finished with value: 0.6055567709472763 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012795355478194362, 'subsample': 0.7426233669428267, 'colsample_bytree': 0.9393236627634551, 'gamma': 0.2318400247857682, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:50:04,590] Trial 101 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.017213805506290827, 'subsample': 0.7148394014847763, 'colsample_bytree': 0.9916830712526931, 'gamma': 0.24231947487711528, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:50:41,524] Trial 102 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01700658064159406, 'subsample': 0.7170135783808314, 'colsample_bytree': 0.9920417209913357, 'gamma': 0.22438728954586534, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:51:18,633] Trial 103 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01700416380378043, 'subsample': 0.7136128333304121, 'colsample_bytree': 0.9763487585300764, 'gamma': 0.22086930243216965, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:51:59,621] Trial 104 finished with value: 0.605338000437541 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.017139107837737595, 'subsample': 0.7136285526182492, 'colsample_bytree': 0.9920123919479388, 'gamma': 0.2226809019646114, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:52:37,088] Trial 105 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014410920400809749, 'subsample': 0.7313788999623686, 'colsample_bytree': 0.9755906323078416, 'gamma': 0.2185357780274512, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:53:17,786] Trial 106 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01449054763503156, 'subsample': 0.7316112797366527, 'colsample_bytree': 0.9811684502867372, 'gamma': 0.21856700302966156, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:53:55,224] Trial 107 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011238268947524223, 'subsample': 0.7445782420556778, 'colsample_bytree': 0.9664933047599016, 'gamma': 0.21144634883098098, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:54:32,656] Trial 108 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01108655582830583, 'subsample': 0.7553678021131408, 'colsample_bytree': 0.963219980502058, 'gamma': 0.20952699554819568, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:55:07,169] Trial 109 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011185127266331866, 'subsample': 0.6987909297302339, 'colsample_bytree': 0.9557735381716873, 'gamma': 0.23604967231903384, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:55:47,899] Trial 110 finished with value: 0.6049004594180705 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.016259225035414385, 'subsample': 0.7421763508255852, 'colsample_bytree': 0.9701936958083732, 'gamma': 0.252418651289652, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:56:25,705] Trial 111 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013370100320815871, 'subsample': 0.7095225029779858, 'colsample_bytree': 0.9747130023526399, 'gamma': 0.22935571298266677, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:57:03,365] Trial 112 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014974590656497469, 'subsample': 0.731962862157424, 'colsample_bytree': 0.9758945468164196, 'gamma': 0.2416403964644237, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:57:40,605] Trial 113 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01932902408908595, 'subsample': 0.6848060386435088, 'colsample_bytree': 0.943792567746859, 'gamma': 0.24432237948439312, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:58:14,615] Trial 114 finished with value: 0.6052286151826733 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01890257336884056, 'subsample': 0.6839899926299735, 'colsample_bytree': 0.9349041213185325, 'gamma': 0.2545613802019803, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:58:54,863] Trial 115 finished with value: 0.6027127543207176 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01973675602693772, 'subsample': 0.7003805511239442, 'colsample_bytree': 0.9421362128459615, 'gamma': 0.2282097950195106, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 19:59:28,912] Trial 116 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.016463643517616876, 'subsample': 0.6884529871197207, 'colsample_bytree': 0.9634948265353888, 'gamma': 0.20932433674703438, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:00:05,821] Trial 117 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.0181065239204812, 'subsample': 0.7490483018816585, 'colsample_bytree': 0.947679431485476, 'gamma': 0.2480036789710061, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:00:41,761] Trial 118 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.02093433888966489, 'subsample': 0.7507532210596733, 'colsample_bytree': 0.9499012433162233, 'gamma': 0.21690223883157236, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:01:17,104] Trial 119 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.018584187190306353, 'subsample': 0.6767900907969596, 'colsample_bytree': 0.9869425513753897, 'gamma': 0.24690109539033328, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:01:53,904] Trial 120 finished with value: 0.6055567709472763 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01786223200952147, 'subsample': 0.7360337224533255, 'colsample_bytree': 0.9304931760099677, 'gamma': 0.23525406481312078, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:02:30,841] Trial 121 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014568875196579768, 'subsample': 0.817588163667379, 'colsample_bytree': 0.9557172158361308, 'gamma': 0.2441177641354138, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:03:07,949] Trial 122 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.015622591947906835, 'subsample': 0.7085101174662697, 'colsample_bytree': 0.9995476308769095, 'gamma': 0.2499042484405657, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:03:49,449] Trial 123 finished with value: 0.6052286151826733 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.012859563103209246, 'subsample': 0.7628548530158796, 'colsample_bytree': 0.9673032841794005, 'gamma': 0.22867896925435152, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:04:26,080] Trial 124 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011765479005519342, 'subsample': 0.7288395337583123, 'colsample_bytree': 0.980250102262207, 'gamma': 0.23511011210160776, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:05:00,790] Trial 125 finished with value: 0.6024939838109823 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.009851654310162907, 'subsample': 0.7748354230438389, 'colsample_bytree': 0.9437107791009954, 'gamma': 0.21980041427465533, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:05:38,909] Trial 126 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.00865976662565254, 'subsample': 0.7476029130272297, 'colsample_bytree': 0.981493429021941, 'gamma': 0.2370906598630745, 'min_child_weight': 6}. Best is trial 95 with value: 0.6067600087508204.\n","[I 2025-03-13 20:06:16,731] Trial 127 finished with value: 0.6074163202800262 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011471217300783845, 'subsample': 0.728497054616732, 'colsample_bytree': 0.9615805491287053, 'gamma': 0.2548728430418906, 'min_child_weight': 6}. Best is trial 127 with value: 0.6074163202800262.\n","[I 2025-03-13 20:06:54,393] Trial 128 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012176675066762128, 'subsample': 0.7298994356872777, 'colsample_bytree': 0.992133658091713, 'gamma': 0.25582160894851025, 'min_child_weight': 6}. Best is trial 127 with value: 0.6074163202800262.\n","[I 2025-03-13 20:07:36,622] Trial 129 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.011204608031593934, 'subsample': 0.7280348935574751, 'colsample_bytree': 0.9893134526765633, 'gamma': 0.25623377376803186, 'min_child_weight': 6}. Best is trial 127 with value: 0.6074163202800262.\n","[I 2025-03-13 20:08:09,396] Trial 130 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011609243931735787, 'subsample': 0.7363046623066015, 'colsample_bytree': 0.9936192771432779, 'gamma': 0.2338218402310829, 'min_child_weight': 6}. Best is trial 127 with value: 0.6074163202800262.\n","[I 2025-03-13 20:08:47,074] Trial 131 finished with value: 0.6077444760446292 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01169174797001815, 'subsample': 0.7413184653901768, 'colsample_bytree': 0.9933065013724891, 'gamma': 0.23254824512601943, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:09:21,423] Trial 132 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01202625260477797, 'subsample': 0.7329818120769288, 'colsample_bytree': 0.9924954055750091, 'gamma': 0.2338222972245613, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:09:55,188] Trial 133 finished with value: 0.605338000437541 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.010878033147816527, 'subsample': 0.7596543003006091, 'colsample_bytree': 0.9988366899815219, 'gamma': 0.25508988698350543, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:10:32,196] Trial 134 finished with value: 0.6073069350251586 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011746656388773554, 'subsample': 0.7401662399357496, 'colsample_bytree': 0.9643999285162383, 'gamma': 0.2480652602275319, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:11:09,869] Trial 135 finished with value: 0.6032596805950557 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.008671479369790342, 'subsample': 0.7390764464840117, 'colsample_bytree': 0.9661380282312421, 'gamma': 0.24963877602661602, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:11:47,257] Trial 136 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011820759435462066, 'subsample': 0.7244487266804024, 'colsample_bytree': 0.9597118956651008, 'gamma': 0.22872171112943956, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:12:24,275] Trial 137 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01336475404867352, 'subsample': 0.7488024166104104, 'colsample_bytree': 0.9691357127604043, 'gamma': 0.21216837117098078, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:13:07,618] Trial 138 finished with value: 0.605338000437541 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.010542550676412266, 'subsample': 0.7828171941737152, 'colsample_bytree': 0.982721116820236, 'gamma': 0.1960879498030509, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:13:45,474] Trial 139 finished with value: 0.6043535331437322 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.009698190292384083, 'subsample': 0.7380488668119063, 'colsample_bytree': 0.9679258755714238, 'gamma': 0.2133144990080149, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:14:22,979] Trial 140 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012842274879757181, 'subsample': 0.7589211285150255, 'colsample_bytree': 0.9821147373167722, 'gamma': 0.20450088202078087, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:15:00,297] Trial 141 finished with value: 0.607197549770291 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013773351183291333, 'subsample': 0.7484321813724081, 'colsample_bytree': 0.9928836326277092, 'gamma': 0.23740932646256477, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:15:37,821] Trial 142 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013866456354025789, 'subsample': 0.721619781599816, 'colsample_bytree': 0.9938829244373113, 'gamma': 0.23736123836614392, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:16:15,568] Trial 143 finished with value: 0.6076350907897615 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013639181426143591, 'subsample': 0.717355542997539, 'colsample_bytree': 0.9929922713029123, 'gamma': 0.2366522699462307, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:16:53,111] Trial 144 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013554407754857997, 'subsample': 0.7544573305970894, 'colsample_bytree': 0.9930259100602498, 'gamma': 0.23341415213746305, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:17:31,091] Trial 145 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.011757377549713748, 'subsample': 0.7432894403422938, 'colsample_bytree': 0.998918866536656, 'gamma': 0.22712156239978454, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:18:08,879] Trial 146 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010489702377129602, 'subsample': 0.7061675987875183, 'colsample_bytree': 0.9892174496318363, 'gamma': 0.26241523280903006, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:18:41,749] Trial 147 finished with value: 0.5342375847735725 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012582016599388336, 'subsample': 0.7246517532645865, 'colsample_bytree': 0.6061758579597363, 'gamma': 0.25669528796904073, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:19:18,831] Trial 148 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013733453155963667, 'subsample': 0.7192869406366954, 'colsample_bytree': 0.984508507691631, 'gamma': 0.17822717910559494, 'min_child_weight': 6}. Best is trial 131 with value: 0.6077444760446292.\n","[I 2025-03-13 20:19:56,701] Trial 149 finished with value: 0.6078538612994968 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011462516173927332, 'subsample': 0.7377052888791713, 'colsample_bytree': 0.9854987232057216, 'gamma': 0.16214078754177846, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:20:35,179] Trial 150 finished with value: 0.6031502953401882 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.009148371513226915, 'subsample': 0.7694418567422212, 'colsample_bytree': 0.9818276192249851, 'gamma': 0.17936788990947022, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:21:12,771] Trial 151 finished with value: 0.607197549770291 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011921478329687738, 'subsample': 0.7386958925505389, 'colsample_bytree': 0.9687700333832397, 'gamma': 0.16882742027723843, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:21:49,382] Trial 152 finished with value: 0.6074163202800262 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013352765889595325, 'subsample': 0.7466539215510116, 'colsample_bytree': 0.9608802219141879, 'gamma': 0.14231589027288877, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:22:26,242] Trial 153 finished with value: 0.6073069350251586 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013322328009702258, 'subsample': 0.7493957875197137, 'colsample_bytree': 0.9677385011903796, 'gamma': 0.16475215589163694, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:23:03,036] Trial 154 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012735700353827907, 'subsample': 0.7466259155553405, 'colsample_bytree': 0.9675539317159219, 'gamma': 0.16307385674129313, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:23:39,616] Trial 155 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013091299017814511, 'subsample': 0.750091462794913, 'colsample_bytree': 0.9626188431804132, 'gamma': 0.1625517853964847, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:24:13,482] Trial 156 finished with value: 0.6017282870269087 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.013302333452315797, 'subsample': 0.745757054389395, 'colsample_bytree': 0.9673497794443181, 'gamma': 0.1537088256708218, 'min_child_weight': 1}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:24:18,626] Trial 157 finished with value: 0.5856486545613652 and parameters: {'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.010355385499705756, 'subsample': 0.7653740041683086, 'colsample_bytree': 0.958853464278112, 'gamma': 0.16677617359652092, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:24:54,484] Trial 158 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01122289573527485, 'subsample': 0.7777310787175836, 'colsample_bytree': 0.9816950681133545, 'gamma': 0.14016940952090914, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:25:30,836] Trial 159 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012248181530085663, 'subsample': 0.7575333158416654, 'colsample_bytree': 0.9677760144439256, 'gamma': 0.1531666837168237, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:26:07,775] Trial 160 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013562361296409897, 'subsample': 0.741895015426899, 'colsample_bytree': 0.9845308678377098, 'gamma': 0.17715257390967465, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:26:44,351] Trial 161 finished with value: 0.6068693940056881 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01241718010199097, 'subsample': 0.7296104026508299, 'colsample_bytree': 0.9712473011289661, 'gamma': 0.17300410758061757, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:27:21,032] Trial 162 finished with value: 0.607197549770291 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012643712934755079, 'subsample': 0.7384921772076722, 'colsample_bytree': 0.9717447450010624, 'gamma': 0.17302581549292548, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:27:57,878] Trial 163 finished with value: 0.6073069350251586 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012603031971890855, 'subsample': 0.7365392802845622, 'colsample_bytree': 0.9733443384828808, 'gamma': 0.17203129470700002, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:28:34,283] Trial 164 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.015045006823674267, 'subsample': 0.7296735975954877, 'colsample_bytree': 0.9751315355086431, 'gamma': 0.17548441996861203, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:29:11,129] Trial 165 finished with value: 0.6078538612994968 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011860732997146522, 'subsample': 0.737333014960888, 'colsample_bytree': 0.9584660252116824, 'gamma': 0.17310530314489347, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:29:48,075] Trial 166 finished with value: 0.6069787792605557 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01163271341946103, 'subsample': 0.7350619714714902, 'colsample_bytree': 0.9531999022089765, 'gamma': 0.1908680455186451, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:29:57,345] Trial 167 finished with value: 0.5913366878144826 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.01047224013031191, 'subsample': 0.7387314723756634, 'colsample_bytree': 0.9520001893899002, 'gamma': 0.16956945468012394, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:30:34,425] Trial 168 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01191199868693517, 'subsample': 0.7332149961068052, 'colsample_bytree': 0.9573881733917289, 'gamma': 0.18565322184932573, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:31:04,413] Trial 169 finished with value: 0.5964777947932619 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.009927472099882671, 'subsample': 0.7098309667040713, 'colsample_bytree': 0.9480792579433214, 'gamma': 0.12909308253230642, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:31:26,959] Trial 170 finished with value: 0.5706628746444979 and parameters: {'n_estimators': 1000, 'max_depth': 3, 'learning_rate': 0.011404190472087593, 'subsample': 0.7279563982984101, 'colsample_bytree': 0.9759190598383013, 'gamma': 0.18776855950960483, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:32:03,556] Trial 171 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012637111232149535, 'subsample': 0.7530283332336859, 'colsample_bytree': 0.9722642355504088, 'gamma': 0.17009048082777734, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:32:40,460] Trial 172 finished with value: 0.6078538612994968 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01242757121335477, 'subsample': 0.7375780386374942, 'colsample_bytree': 0.9587053166060727, 'gamma': 0.15894223867806134, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:33:17,268] Trial 173 finished with value: 0.6068693940056881 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01222962478403883, 'subsample': 0.7374840920938213, 'colsample_bytree': 0.9595238176993796, 'gamma': 0.14865200942159804, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:33:54,732] Trial 174 finished with value: 0.6073069350251586 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010830735440791215, 'subsample': 0.7399677508624095, 'colsample_bytree': 0.9382540377710654, 'gamma': 0.15818443378332672, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:34:32,443] Trial 175 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010850229972416392, 'subsample': 0.738486544068943, 'colsample_bytree': 0.9554469891294232, 'gamma': 0.15923203497716917, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:35:09,555] Trial 176 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01237512815767097, 'subsample': 0.7604628907432797, 'colsample_bytree': 0.9384236470009022, 'gamma': 0.14902817188125647, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:35:43,255] Trial 177 finished with value: 0.6046816889083352 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01084458951906889, 'subsample': 0.7369406322514589, 'colsample_bytree': 0.9504572615482291, 'gamma': 0.13832453515475185, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:36:20,790] Trial 178 finished with value: 0.603478451104791 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.009232681456565122, 'subsample': 0.7544091183161146, 'colsample_bytree': 0.9587931922576762, 'gamma': 0.15779316989422737, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:36:54,661] Trial 179 finished with value: 0.5921023845985561 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.00755135429955676, 'subsample': 0.7652152028067865, 'colsample_bytree': 0.9613911767863631, 'gamma': 0.14447800373712377, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:37:31,754] Trial 180 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011661944679933213, 'subsample': 0.742415781556172, 'colsample_bytree': 0.942734715116702, 'gamma': 0.16756129400987452, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:38:08,767] Trial 181 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012660896705963846, 'subsample': 0.7180122188862526, 'colsample_bytree': 0.9635945543544181, 'gamma': 0.15188707970773907, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:38:45,524] Trial 182 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.015067838367950635, 'subsample': 0.7260808277612414, 'colsample_bytree': 0.932424235132832, 'gamma': 0.17355499862128432, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:39:22,183] Trial 183 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014024439324198276, 'subsample': 0.7350764329256415, 'colsample_bytree': 0.9868545073316918, 'gamma': 0.1623392909276774, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:39:59,094] Trial 184 finished with value: 0.6055567709472763 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012173065941583515, 'subsample': 0.7137433016510961, 'colsample_bytree': 0.9998052439079729, 'gamma': 0.16642616875238792, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:40:36,065] Trial 185 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010105310562342294, 'subsample': 0.7498600723093262, 'colsample_bytree': 0.9752121223289537, 'gamma': 0.15599729556543815, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:41:16,383] Trial 186 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.011418456910015588, 'subsample': 0.7258904690631212, 'colsample_bytree': 0.970898744701358, 'gamma': 0.18975731253742104, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:41:53,196] Trial 187 finished with value: 0.607197549770291 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01309143820589956, 'subsample': 0.6996432194160744, 'colsample_bytree': 0.9512415734864094, 'gamma': 0.14441897332692477, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:42:30,309] Trial 188 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013125596783888164, 'subsample': 0.7388144061701781, 'colsample_bytree': 0.9520825537379227, 'gamma': 0.11994352606917803, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:43:05,996] Trial 189 finished with value: 0.5935243929118355 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.005060891026827545, 'subsample': 0.70379219005788, 'colsample_bytree': 0.9469607735562486, 'gamma': 0.13294968995429124, 'min_child_weight': 2}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:43:39,557] Trial 190 finished with value: 0.5997593524392912 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.010911374952455253, 'subsample': 0.8043218330102447, 'colsample_bytree': 0.9574487146902255, 'gamma': 0.14707058864818903, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:44:16,176] Trial 191 finished with value: 0.6069787792605557 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014149185910670888, 'subsample': 0.7321593186028372, 'colsample_bytree': 0.9794091974504964, 'gamma': 0.16341474353557042, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:44:52,875] Trial 192 finished with value: 0.6070881645154234 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014120916212875098, 'subsample': 0.7311713908704367, 'colsample_bytree': 0.963316990795957, 'gamma': 0.16130569574953138, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:45:29,543] Trial 193 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012510451366760005, 'subsample': 0.7432535596548143, 'colsample_bytree': 0.9618124877521077, 'gamma': 0.15948389416109346, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:46:06,151] Trial 194 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013834419898295392, 'subsample': 0.7323496411822431, 'colsample_bytree': 0.9388487455964574, 'gamma': 0.17404268269347176, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:46:42,841] Trial 195 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011588807465768148, 'subsample': 0.75176983592257, 'colsample_bytree': 0.9513627609129927, 'gamma': 0.18043613419962468, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:47:19,474] Trial 196 finished with value: 0.607197549770291 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013266824960249066, 'subsample': 0.730157759745732, 'colsample_bytree': 0.9695160931567367, 'gamma': 0.14932077558463802, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:47:56,539] Trial 197 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013192510292638321, 'subsample': 0.727781733643651, 'colsample_bytree': 0.9710627282894064, 'gamma': 0.16419022023927995, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:48:33,147] Trial 198 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01443437152557991, 'subsample': 0.7217321862866904, 'colsample_bytree': 0.9797796947899475, 'gamma': 0.14111118749725615, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:49:09,900] Trial 199 finished with value: 0.6076350907897615 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013229175971305863, 'subsample': 0.745085494828961, 'colsample_bytree': 0.9693317536582905, 'gamma': 0.15549583969837913, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:49:46,274] Trial 200 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01550095123799038, 'subsample': 0.7596396798295845, 'colsample_bytree': 0.9257138662000255, 'gamma': 0.15142002999732568, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:50:23,072] Trial 201 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013124532028008042, 'subsample': 0.7484905430521933, 'colsample_bytree': 0.9648489450130966, 'gamma': 0.15589157343643767, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:50:59,910] Trial 202 finished with value: 0.6068693940056881 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012090737112665926, 'subsample': 0.7439897813082853, 'colsample_bytree': 0.9727793706618625, 'gamma': 0.17042473269281638, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:51:36,584] Trial 203 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013730556304516838, 'subsample': 0.7347434579328757, 'colsample_bytree': 0.980146557527378, 'gamma': 0.1597957090458911, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:52:32,871] Trial 204 finished with value: 0.5960402537737913 and parameters: {'n_estimators': 1000, 'max_depth': 11, 'learning_rate': 0.012841099907452948, 'subsample': 0.7270612132926461, 'colsample_bytree': 0.9668064928796967, 'gamma': 0.1644608482840052, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:53:10,959] Trial 205 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011200813502856339, 'subsample': 0.743183945385961, 'colsample_bytree': 0.9563743901455928, 'gamma': 0.14468701712793067, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:53:47,718] Trial 206 finished with value: 0.6069787792605557 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010521154773392377, 'subsample': 0.7568055396288318, 'colsample_bytree': 0.984102418705354, 'gamma': 0.173621985652452, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:54:25,240] Trial 207 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010216100808023848, 'subsample': 0.7552287684439885, 'colsample_bytree': 0.985921521721168, 'gamma': 0.15533723993623008, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:55:02,131] Trial 208 finished with value: 0.6052286151826733 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010677085281188207, 'subsample': 0.7675774372710947, 'colsample_bytree': 0.9785857808884404, 'gamma': 0.1819912812765531, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:55:35,917] Trial 209 finished with value: 0.6031502953401882 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.00968617585087997, 'subsample': 0.7480375540091915, 'colsample_bytree': 0.7264716757404245, 'gamma': 0.16199133872559562, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:56:16,901] Trial 210 finished with value: 0.6041347626339969 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01462336309802246, 'subsample': 0.7411293848560255, 'colsample_bytree': 0.9873012231770405, 'gamma': 0.1260159453854442, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:56:53,203] Trial 211 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011797947658355956, 'subsample': 0.7318844614379857, 'colsample_bytree': 0.9710722666435879, 'gamma': 0.17362784979410184, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:57:30,147] Trial 212 finished with value: 0.6075257055348939 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012565179844528223, 'subsample': 0.7338667564479007, 'colsample_bytree': 0.9639978835364688, 'gamma': 0.16975765866669623, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:58:06,519] Trial 213 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013619710211070295, 'subsample': 0.7375532762812774, 'colsample_bytree': 0.9473251072785855, 'gamma': 0.1656757254150043, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:58:43,380] Trial 214 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011604741859110443, 'subsample': 0.7501298243364535, 'colsample_bytree': 0.9624220620478359, 'gamma': 0.1508480328342397, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:59:20,102] Trial 215 finished with value: 0.6045723036534675 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01260901224863267, 'subsample': 0.7578712956434, 'colsample_bytree': 0.9553951690177742, 'gamma': 0.13760089951039844, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 20:59:57,524] Trial 216 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011094513003619605, 'subsample': 0.7217786806923588, 'colsample_bytree': 0.6820925568215584, 'gamma': 0.1777427006186973, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 21:00:34,296] Trial 217 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013265621887573456, 'subsample': 0.7360094670777011, 'colsample_bytree': 0.9796774082499505, 'gamma': 0.16733135061371032, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 21:01:12,010] Trial 218 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012119838032679207, 'subsample': 0.7429450622510911, 'colsample_bytree': 0.966010331613039, 'gamma': 0.15750899225301618, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 21:01:48,933] Trial 219 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014036186209247037, 'subsample': 0.7193452953885626, 'colsample_bytree': 0.9763642898500134, 'gamma': 0.1703697704522497, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 21:02:27,031] Trial 220 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010410584066042316, 'subsample': 0.7106772823456718, 'colsample_bytree': 0.9917783568717775, 'gamma': 0.14999074601246426, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 21:03:03,306] Trial 221 finished with value: 0.607197549770291 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012575959746574394, 'subsample': 0.7295248130138987, 'colsample_bytree': 0.9700941906037639, 'gamma': 0.1745352266844724, 'min_child_weight': 6}. Best is trial 149 with value: 0.6078538612994968.\n","[I 2025-03-13 21:03:39,894] Trial 222 finished with value: 0.6082914023189674 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01302268190206325, 'subsample': 0.7318871384848771, 'colsample_bytree': 0.9620495160104293, 'gamma': 0.16110828513652234, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:04:16,629] Trial 223 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012973985174672758, 'subsample': 0.7307432141459879, 'colsample_bytree': 0.9593984253261213, 'gamma': 0.16069565485346723, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:04:53,458] Trial 224 finished with value: 0.6055567709472763 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.015404940757591074, 'subsample': 0.7240067095645275, 'colsample_bytree': 0.9437273714111802, 'gamma': 0.19091431106212872, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:05:30,502] Trial 225 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013949915303169637, 'subsample': 0.7339474480827748, 'colsample_bytree': 0.95209068260415, 'gamma': 0.16686791878257015, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:06:08,176] Trial 226 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011916138698658691, 'subsample': 0.7421767095996212, 'colsample_bytree': 0.9644685897644494, 'gamma': 0.1839167474484704, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:06:45,335] Trial 227 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012904576029524766, 'subsample': 0.7262699577172278, 'colsample_bytree': 0.9706989077925791, 'gamma': 0.1430776848271773, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:07:49,046] Trial 228 finished with value: 0.5898052942463355 and parameters: {'n_estimators': 1000, 'max_depth': 12, 'learning_rate': 0.01449305030746576, 'subsample': 0.7358267574303502, 'colsample_bytree': 0.9535321370685923, 'gamma': 0.1595316625350195, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:08:25,844] Trial 229 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01235204845078113, 'subsample': 0.7161949010353572, 'colsample_bytree': 0.9614438847175656, 'gamma': 0.1550427082239219, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:09:02,531] Trial 230 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011338600375373218, 'subsample': 0.7458713338896666, 'colsample_bytree': 0.9754303863577573, 'gamma': 0.1782272453026154, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:09:38,383] Trial 231 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013381403568296311, 'subsample': 0.7517967789767084, 'colsample_bytree': 0.9841583164658343, 'gamma': 0.17252777095176572, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:10:15,841] Trial 232 finished with value: 0.6075257055348939 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011720823000434866, 'subsample': 0.7369732312407757, 'colsample_bytree': 0.967459588211143, 'gamma': 0.16544193011980893, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:10:52,944] Trial 233 finished with value: 0.607197549770291 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01177544231108329, 'subsample': 0.7342308531790632, 'colsample_bytree': 0.9664432340493981, 'gamma': 0.1646838679736792, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:11:30,103] Trial 234 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011556323333569415, 'subsample': 0.7385078535389259, 'colsample_bytree': 0.9649570564140503, 'gamma': 0.16725919984478668, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:12:06,515] Trial 235 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012318609942202229, 'subsample': 0.7265824911977868, 'colsample_bytree': 0.9508571067421181, 'gamma': 0.1533213821129736, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:12:43,807] Trial 236 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01175340076153468, 'subsample': 0.7432191178256671, 'colsample_bytree': 0.9692252084034175, 'gamma': 0.1602607742281286, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:13:20,622] Trial 237 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012648260479687238, 'subsample': 0.7332893994566747, 'colsample_bytree': 0.9611089955341036, 'gamma': 0.0845499980808571, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:13:57,619] Trial 238 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011410342527680004, 'subsample': 0.7202259640720005, 'colsample_bytree': 0.9389372047370544, 'gamma': 0.14912195130324207, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:14:34,935] Trial 239 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010905640882811471, 'subsample': 0.7005128393491167, 'colsample_bytree': 0.9569836899382198, 'gamma': 0.16988893307869457, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:15:11,496] Trial 240 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013006803433341693, 'subsample': 0.7483125878358284, 'colsample_bytree': 0.9693310202554155, 'gamma': 0.14495281298512364, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:15:48,351] Trial 241 finished with value: 0.6063224677313498 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013576251197056254, 'subsample': 0.7307852040841394, 'colsample_bytree': 0.9758570688975163, 'gamma': 0.16477568472520532, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:16:25,918] Trial 242 finished with value: 0.6070881645154234 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012259554700165166, 'subsample': 0.739122978051399, 'colsample_bytree': 0.9675755270102673, 'gamma': 0.16078747425091652, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:17:02,928] Trial 243 finished with value: 0.607197549770291 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012080396620095506, 'subsample': 0.7385059489811252, 'colsample_bytree': 0.9598319023204662, 'gamma': 0.158292760188696, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:17:40,098] Trial 244 finished with value: 0.6069787792605557 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012196167183750453, 'subsample': 0.7397649953614666, 'colsample_bytree': 0.9643032827096566, 'gamma': 0.15693110017041126, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:18:16,468] Trial 245 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01260354246270607, 'subsample': 0.8454131881547706, 'colsample_bytree': 0.9689324613852143, 'gamma': 0.1528669452123451, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:18:53,206] Trial 246 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013183156026400978, 'subsample': 0.7475213297025586, 'colsample_bytree': 0.9578844332052698, 'gamma': 0.16201270475398258, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:19:30,421] Trial 247 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012038501478184464, 'subsample': 0.7409289490256188, 'colsample_bytree': 0.945821663645819, 'gamma': 0.13593460595885062, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:20:08,250] Trial 248 finished with value: 0.6052286151826733 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010913412149137679, 'subsample': 0.7241573555695016, 'colsample_bytree': 0.9733432577972692, 'gamma': 0.16938432417066906, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:20:45,050] Trial 249 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012238138563333861, 'subsample': 0.7525085527179826, 'colsample_bytree': 0.9626265886843063, 'gamma': 0.15648640135546882, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:21:21,724] Trial 250 finished with value: 0.6073069350251586 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01307910373941439, 'subsample': 0.7304757422060919, 'colsample_bytree': 0.9714807213058495, 'gamma': 0.14685545683862322, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:21:55,074] Trial 251 finished with value: 0.6023845985561146 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.013384685602655488, 'subsample': 0.7129649023079652, 'colsample_bytree': 0.9792944181348607, 'gamma': 0.14603074632097285, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:22:31,840] Trial 252 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014889164990655769, 'subsample': 0.7275807368454265, 'colsample_bytree': 0.9923202282003876, 'gamma': 0.140464537604367, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:23:05,669] Trial 253 finished with value: 0.6078538612994968 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.012997810883568283, 'subsample': 0.7321827031576458, 'colsample_bytree': 0.9571760677785079, 'gamma': 0.12029186777953327, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:23:38,550] Trial 254 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011399764930505903, 'subsample': 0.7213723284751844, 'colsample_bytree': 0.9476735256172126, 'gamma': 0.17791959505969315, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:24:15,206] Trial 255 finished with value: 0.6050098446729381 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.012891493189508284, 'subsample': 0.7459215702438037, 'colsample_bytree': 0.9997739853332005, 'gamma': 0.10627309702112867, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:24:51,917] Trial 256 finished with value: 0.6070881645154234 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011725617429481087, 'subsample': 0.7356285190247119, 'colsample_bytree': 0.9566508154691912, 'gamma': 0.11068951191866364, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:25:27,939] Trial 257 finished with value: 0.6064318529862175 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01291251057607002, 'subsample': 0.760061522681859, 'colsample_bytree': 0.9738323373551896, 'gamma': 0.15032850485222066, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:26:04,739] Trial 258 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010718275386643805, 'subsample': 0.729275043978627, 'colsample_bytree': 0.9819967721260551, 'gamma': 0.13327178044873528, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:26:41,062] Trial 259 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.0134013581224858, 'subsample': 0.7080423544483737, 'colsample_bytree': 0.9876064340605144, 'gamma': 0.12245823576896205, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:27:14,714] Trial 260 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011136123832445197, 'subsample': 0.7403362055978624, 'colsample_bytree': 0.9524157170389523, 'gamma': 0.09822014019127134, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:27:47,598] Trial 261 finished with value: 0.6030409100853205 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.013954650335353466, 'subsample': 0.718027452512839, 'colsample_bytree': 0.9710464310195834, 'gamma': 0.11445757247810237, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:28:24,279] Trial 262 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012384016301362176, 'subsample': 0.7525312359701113, 'colsample_bytree': 0.9592946860904552, 'gamma': 0.015127961341325019, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:29:01,106] Trial 263 finished with value: 0.6049004594180705 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.009913795699659072, 'subsample': 0.7340481671848176, 'colsample_bytree': 0.9765132738633002, 'gamma': 0.17559965080286896, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:29:37,591] Trial 264 finished with value: 0.6055567709472763 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011875709185707274, 'subsample': 0.7256326837936297, 'colsample_bytree': 0.9650216569762828, 'gamma': 0.16863748730663025, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:30:18,417] Trial 265 finished with value: 0.6047910741632028 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.012755440379460714, 'subsample': 0.7458325405511326, 'colsample_bytree': 0.9440452191508575, 'gamma': 0.048738641913616404, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:30:55,035] Trial 266 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01386891234369453, 'subsample': 0.7389751250491094, 'colsample_bytree': 0.9859567132252425, 'gamma': 0.15546066940793665, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:31:32,359] Trial 267 finished with value: 0.6051192299278058 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011263687061153969, 'subsample': 0.7309572442800206, 'colsample_bytree': 0.9546548858284798, 'gamma': 0.1644235849031235, 'min_child_weight': 3}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:32:08,751] Trial 268 finished with value: 0.6067600087508204 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01489704067495332, 'subsample': 0.7561473416184258, 'colsample_bytree': 0.9724343778613896, 'gamma': 0.1461983036206505, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:32:45,578] Trial 269 finished with value: 0.6068693940056881 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011892646850279208, 'subsample': 0.7456087370220971, 'colsample_bytree': 0.9365537400553487, 'gamma': 0.17221526707484663, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:33:19,128] Trial 270 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.01297869425122993, 'subsample': 0.7189115549323596, 'colsample_bytree': 0.9613063209852657, 'gamma': 0.18376104308222335, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:33:56,123] Trial 271 finished with value: 0.6074163202800262 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01255662864925829, 'subsample': 0.7367739744152719, 'colsample_bytree': 0.9921514543502011, 'gamma': 0.15460462213420717, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:34:32,935] Trial 272 finished with value: 0.6054473856924086 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01360674492234806, 'subsample': 0.7245465191414351, 'colsample_bytree': 0.9906819076680764, 'gamma': 0.13774713935083466, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:35:05,816] Trial 273 finished with value: 0.5984467293808795 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.010205366686733245, 'subsample': 0.7101993431340348, 'colsample_bytree': 0.9988586377457447, 'gamma': 0.12904190127038812, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:35:38,035] Trial 274 finished with value: 0.602275213301247 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.012652372047526571, 'subsample': 0.7333283475416947, 'colsample_bytree': 0.9828721734048724, 'gamma': 0.15155655473417068, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:36:14,846] Trial 275 finished with value: 0.6075257055348939 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011400576494992265, 'subsample': 0.6963194135627377, 'colsample_bytree': 0.9906467501694999, 'gamma': 0.14224059444246148, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:36:51,044] Trial 276 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01592092790261187, 'subsample': 0.6991009597304861, 'colsample_bytree': 0.9938667050582488, 'gamma': 0.1428933105719051, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:37:27,652] Trial 277 finished with value: 0.6061036972216145 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010693407252612388, 'subsample': 0.6913858979701817, 'colsample_bytree': 0.9888356021464044, 'gamma': 0.1352072549371907, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:38:04,187] Trial 278 finished with value: 0.6066506234959528 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013865048986761903, 'subsample': 0.677249981349161, 'colsample_bytree': 0.9818630365902735, 'gamma': 0.14884599058352266, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:38:40,693] Trial 279 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.014796029407516273, 'subsample': 0.7040069559874432, 'colsample_bytree': 0.9945091715722467, 'gamma': 0.1408889514079272, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:39:18,149] Trial 280 finished with value: 0.6059943119667469 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011304262899993894, 'subsample': 0.7513098340568237, 'colsample_bytree': 0.977919466270673, 'gamma': 0.15144389197330468, 'min_child_weight': 1}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:39:52,497] Trial 281 finished with value: 0.5888208269525268 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.08859160552835048, 'subsample': 0.7619054741361662, 'colsample_bytree': 0.9905221132751151, 'gamma': 0.18183886164206578, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:40:28,995] Trial 282 finished with value: 0.6058849267118792 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012904168512980404, 'subsample': 0.7146121831825901, 'colsample_bytree': 0.9827503247443428, 'gamma': 0.08887224646615519, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:41:05,406] Trial 283 finished with value: 0.6068693940056881 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012373026620827024, 'subsample': 0.7443125404615154, 'colsample_bytree': 0.9988971379368317, 'gamma': 0.15660973276162937, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:41:12,058] Trial 284 finished with value: 0.5866331218551739 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.01080053184240863, 'subsample': 0.7710982200628674, 'colsample_bytree': 0.9736864684287556, 'gamma': 0.11945153666488856, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:41:48,162] Trial 285 finished with value: 0.6065412382410851 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013617651163180118, 'subsample': 0.7211463165202489, 'colsample_bytree': 0.9869574120980233, 'gamma': 0.1465123456049493, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:42:24,593] Trial 286 finished with value: 0.6068693940056881 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011398050044384702, 'subsample': 0.6960086894934543, 'colsample_bytree': 0.9709243741134009, 'gamma': 0.17417374270863833, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:43:01,161] Trial 287 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013142591427993457, 'subsample': 0.7267977053394958, 'colsample_bytree': 0.977155346745234, 'gamma': 0.06663538127543368, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:43:39,316] Trial 288 finished with value: 0.6046816889083352 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.012462635544994846, 'subsample': 0.7421788622813383, 'colsample_bytree': 0.9493499579418987, 'gamma': 0.1660558451352297, 'min_child_weight': 2}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:44:15,981] Trial 289 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.014519277445215523, 'subsample': 0.7541499136964267, 'colsample_bytree': 0.9677178282353333, 'gamma': 0.12938916025195601, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:44:49,508] Trial 290 finished with value: 0.6021658280463793 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.011870527240934958, 'subsample': 0.7359232511246135, 'colsample_bytree': 0.9808885107269987, 'gamma': 0.1543115998535424, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:45:26,523] Trial 291 finished with value: 0.6040253773791293 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.00964310058271617, 'subsample': 0.7290808262376248, 'colsample_bytree': 0.9997401707966097, 'gamma': 0.16013539698678692, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:46:03,500] Trial 292 finished with value: 0.6056661562021439 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.010234075840355272, 'subsample': 0.7477031608373955, 'colsample_bytree': 0.7787603192932848, 'gamma': 0.1720798621000951, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:46:39,962] Trial 293 finished with value: 0.6062130824764822 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.013318047813207917, 'subsample': 0.6874964565707911, 'colsample_bytree': 0.958593516938081, 'gamma': 0.2504492452946466, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:47:16,874] Trial 294 finished with value: 0.6057755414570116 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.011133193679245838, 'subsample': 0.7114259813714735, 'colsample_bytree': 0.9895256941865845, 'gamma': 0.1415901292305206, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:47:53,565] Trial 295 finished with value: 0.607197549770291 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012408400074028695, 'subsample': 0.737342034263178, 'colsample_bytree': 0.9689822594371605, 'gamma': 0.16334547150444897, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:48:44,147] Trial 296 finished with value: 0.5997593524392912 and parameters: {'n_estimators': 1000, 'max_depth': 10, 'learning_rate': 0.014198635632792658, 'subsample': 0.7196229936233793, 'colsample_bytree': 0.950144020998774, 'gamma': 0.17781862131929485, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:49:17,957] Trial 297 finished with value: 0.6046816889083352 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.011685872524511772, 'subsample': 0.7650536527387782, 'colsample_bytree': 0.9782312520481058, 'gamma': 0.1688581311941043, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:49:54,503] Trial 298 finished with value: 0.6039159921242616 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01563497073992259, 'subsample': 0.7295736700067248, 'colsample_bytree': 0.965453055927154, 'gamma': 0.15001034505625105, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","[I 2025-03-13 21:50:31,463] Trial 299 finished with value: 0.6068693940056881 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.012856009779747287, 'subsample': 0.7456514884773623, 'colsample_bytree': 0.9564763960184981, 'gamma': 0.07736276641893934, 'min_child_weight': 6}. Best is trial 222 with value: 0.6082914023189674.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001769 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002402 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 41572, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791711\n","[LightGBM] [Info] Start training from score -1.791711\n","[LightGBM] [Info] Start training from score -1.791711\n","[LightGBM] [Info] Start training from score -1.791856\n","[LightGBM] [Info] Start training from score -1.791856\n","[LightGBM] [Info] Start training from score -1.791711\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001397 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 41573, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791880\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001393 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 41573, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791880\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001431 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 41573, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791880\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001406 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 41573, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791880\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n","[LightGBM] [Info] Start training from score -1.791735\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.60      0.74      0.66      1518\n","           1       0.64      0.75      0.69      1506\n","           2       0.69      0.67      0.68      1530\n","           3       0.72      0.63      0.67      1496\n","           4       0.52      0.40      0.45      1521\n","           5       0.52      0.52      0.52      1571\n","\n","    accuracy                           0.62      9142\n","   macro avg       0.62      0.62      0.61      9142\n","weighted avg       0.61      0.62      0.61      9142\n","\n","Confusion Matrix:\n"," [[1124   56   92   12   69  165]\n"," [  13 1122   77  100  116   78]\n"," [ 111  119 1025  168   31   76]\n"," [  34  181  212  937   80   52]\n"," [ 295  175   28   44  605  374]\n"," [ 303  111   54   32  254  817]]\n","Accuracy Score: 0.6158389849048348\n"]},{"data":{"text/plain":["['label_encoder.pkl']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Install required libraries (Uncomment if not installed)\n","# !pip install optuna lightgbm catboost xgboost imbalanced-learn joblib pandas numpy scikit-learn\n","!pip install optuna\n","!pip install catboost\n","import pandas as pd\n","import numpy as np\n","import optuna\n","import joblib\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler, LabelEncoder\n","from sklearn.impute import KNNImputer\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from imblearn.over_sampling import BorderlineSMOTE\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from catboost import CatBoostClassifier\n","from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","# ✅ 1️⃣ LOAD DATA (Replace with actual dataset)\n","data = pd.read_csv('/balanced_fall_detection_dataset.csv')  # Assuming this is your dataset file\n","\n","# Assuming 'Label' is your target column\n","X = data.drop('Label', axis=1)\n","y = data['Label']  # Define y here\n","\n","# ✅ Encode labels\n","le = LabelEncoder()\n","y_encoded = le.fit_transform(y)\n","\n","# ✅ Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y_encoded, test_size=0.15, random_state=42\n",")\n","\n","# ✅ Feature Selection using XGBoost\n","xgb_temp = XGBClassifier(n_estimators=500, random_state=42)\n","xgb_temp.fit(X_train, y_train)\n","feature_importance = xgb_temp.feature_importances_\n","\n","# Keep only the top 20 most important features\n","important_features = X.columns[np.argsort(-feature_importance)[:20]]\n","X_train = X_train[important_features]\n","X_test = X_test[important_features]\n","\n","# ✅ Missing Value Handling using KNN Imputer\n","imputer = KNNImputer(n_neighbors=5)\n","X_train_imputed = imputer.fit_transform(X_train)\n","X_test_imputed = imputer.transform(X_test)\n","\n","# ✅ Robust Scaling (Better than StandardScaler)\n","scaler = RobustScaler()\n","X_train_scaled = scaler.fit_transform(X_train_imputed)\n","X_test_scaled = scaler.transform(X_test_imputed)\n","\n","# ✅ Apply PCA for Noise Reduction\n","max_pca_components = min(15, X_train_scaled.shape[1])  # Ensure PCA does not exceed feature count\n","pca = PCA(n_components=max_pca_components)\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","# ✅ Use Borderline-SMOTE for better data balancing\n","smote = BorderlineSMOTE(random_state=42, kind='borderline-1')\n","X_resampled, y_resampled = smote.fit_resample(X_train_pca, y_train)\n","\n","# ✅ Hyperparameter Tuning with Optuna (LGBM)\n","def lgbm_objective(trial):\n","    params = {\n","        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n","        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50)\n","    }\n","    model = LGBMClassifier(**params, random_state=42)\n","    model.fit(X_resampled, y_resampled)\n","    y_pred = model.predict(X_test_pca)\n","    return accuracy_score(y_test, y_pred)\n","\n","lgbm_study = optuna.create_study(direction='maximize')\n","lgbm_study.optimize(lgbm_objective, n_trials=300)  # Increased trials for better tuning\n","best_lgbm_params = lgbm_study.best_params\n","\n","# ✅ Hyperparameter Tuning with Optuna (XGBoost)\n","def xgb_objective(trial):\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n","        'max_depth': trial.suggest_int('max_depth', 3, 12),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'gamma': trial.suggest_float('gamma', 0, 0.3),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 1, 6)\n","    }\n","    model = XGBClassifier(**params, random_state=42, eval_metric='mlogloss')\n","    model.fit(X_resampled, y_resampled)\n","    y_pred = model.predict(X_test_pca)\n","    return accuracy_score(y_test, y_pred)\n","\n","xgb_study = optuna.create_study(direction='maximize')\n","xgb_study.optimize(xgb_objective, n_trials=300)\n","best_xgb_params = xgb_study.best_params\n","\n","# ✅ Train Final Models with Optimized Parameters\n","rf_model = RandomForestClassifier(n_estimators=500, random_state=42)\n","xgb_model = XGBClassifier(**best_xgb_params, random_state=42, eval_metric='mlogloss')\n","lgbm_model = LGBMClassifier(**best_lgbm_params, random_state=42)\n","cat_model = CatBoostClassifier(n_estimators=500, depth=8, learning_rate=0.03, verbose=0)\n","\n","# ✅ Stacking Classifier (Better than Voting Classifier)\n","meta_learner = LogisticRegression()\n","\n","stacking_clf = StackingClassifier(\n","    estimators=[\n","        ('rf', rf_model),\n","        ('xgb', xgb_model),\n","        ('lgbm', lgbm_model),\n","        ('cat', cat_model)\n","    ],\n","    final_estimator=meta_learner\n",")\n","\n","# ✅ Train Stacking Classifier\n","stacking_clf.fit(X_resampled, y_resampled)\n","\n","# ✅ Evaluate Model\n","y_pred = stacking_clf.predict(X_test_pca)\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n","\n","# ✅ Save Model and Preprocessing Objects\n","joblib.dump(stacking_clf, 'final_stacking_model.pkl')\n","joblib.dump(scaler, 'scaler.pkl')\n","joblib.dump(imputer, 'imputer.pkl')\n","joblib.dump(pca, 'pca.pkl')\n","joblib.dump(le, 'label_encoder.pkl')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"1I7noyzEHDj9","outputId":"2075458a-e97c-4b84-a22d-fd7faee2f32f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.7)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.14.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-14 05:27:22,752] A new study created in memory with name: no-name-0b6ed93a-a054-4ec1-a13a-6b54fe557349\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001914 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:28:09,208] Trial 0 finished with value: 0.5544738569240867 and parameters: {'num_leaves': 85, 'learning_rate': 0.05053973983080206, 'n_estimators': 1500, 'subsample': 0.6104769660045817, 'colsample_bytree': 0.7235065927015019, 'min_child_samples': 8}. Best is trial 0 with value: 0.5544738569240867.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002992 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:28:18,977] Trial 1 finished with value: 0.5973528768322031 and parameters: {'num_leaves': 31, 'learning_rate': 0.006594068297712214, 'n_estimators': 300, 'subsample': 0.8298349347409999, 'colsample_bytree': 0.8822782356621104, 'min_child_samples': 68}. Best is trial 1 with value: 0.5973528768322031.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001666 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:29:06,640] Trial 2 finished with value: 0.5956027127543208 and parameters: {'num_leaves': 34, 'learning_rate': 0.06324565399270965, 'n_estimators': 1900, 'subsample': 0.917313161861009, 'colsample_bytree': 0.899218646433078, 'min_child_samples': 53}. Best is trial 1 with value: 0.5973528768322031.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001460 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:30:06,620] Trial 3 finished with value: 0.5887114416976592 and parameters: {'num_leaves': 52, 'learning_rate': 0.0035582024017227266, 'n_estimators': 1700, 'subsample': 0.7033483368681347, 'colsample_bytree': 0.7331885255551611, 'min_child_samples': 21}. Best is trial 1 with value: 0.5973528768322031.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002645 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:32:08,447] Trial 4 finished with value: 0.5588492671187923 and parameters: {'num_leaves': 196, 'learning_rate': 0.023998588773961438, 'n_estimators': 1500, 'subsample': 0.8363513284239394, 'colsample_bytree': 0.8079247345678107, 'min_child_samples': 100}. Best is trial 1 with value: 0.5973528768322031.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002924 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:32:38,601] Trial 5 finished with value: 0.5613651279807482 and parameters: {'num_leaves': 88, 'learning_rate': 0.08115398708883592, 'n_estimators': 800, 'subsample': 0.7998549697224818, 'colsample_bytree': 0.7279745673422742, 'min_child_samples': 61}. Best is trial 1 with value: 0.5973528768322031.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001818 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:34:23,474] Trial 6 finished with value: 0.6158389849048348 and parameters: {'num_leaves': 95, 'learning_rate': 0.0018791392561323496, 'n_estimators': 2000, 'subsample': 0.9696769015908059, 'colsample_bytree': 0.9585426461152223, 'min_child_samples': 25}. Best is trial 6 with value: 0.6158389849048348.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001484 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:34:45,561] Trial 7 finished with value: 0.5731787355064537 and parameters: {'num_leaves': 177, 'learning_rate': 0.004778424296889038, 'n_estimators': 400, 'subsample': 0.880685467495665, 'colsample_bytree': 0.6731301843921522, 'min_child_samples': 45}. Best is trial 6 with value: 0.6158389849048348.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001990 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:36:01,355] Trial 8 finished with value: 0.5790855392693065 and parameters: {'num_leaves': 72, 'learning_rate': 0.0013537365809889257, 'n_estimators': 1800, 'subsample': 0.9829669724351504, 'colsample_bytree': 0.7898352511644189, 'min_child_samples': 70}. Best is trial 6 with value: 0.6158389849048348.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001704 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:36:41,368] Trial 9 finished with value: 0.611463574710129 and parameters: {'num_leaves': 63, 'learning_rate': 0.004824121816562245, 'n_estimators': 1000, 'subsample': 0.6149717198719968, 'colsample_bytree': 0.862588722981654, 'min_child_samples': 92}. Best is trial 6 with value: 0.6158389849048348.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001653 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:37:50,805] Trial 10 finished with value: 0.6146357471012908 and parameters: {'num_leaves': 137, 'learning_rate': 0.0011410981116342052, 'n_estimators': 1200, 'subsample': 0.981989174401882, 'colsample_bytree': 0.9854563239633398, 'min_child_samples': 33}. Best is trial 6 with value: 0.6158389849048348.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001691 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:39:07,315] Trial 11 finished with value: 0.6138700503172172 and parameters: {'num_leaves': 136, 'learning_rate': 0.0010180943794167363, 'n_estimators': 1300, 'subsample': 0.99213882903436, 'colsample_bytree': 0.9999939447349349, 'min_child_samples': 32}. Best is trial 6 with value: 0.6158389849048348.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001610 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:39:50,614] Trial 12 finished with value: 0.6154014438853642 and parameters: {'num_leaves': 134, 'learning_rate': 0.0019009138324551011, 'n_estimators': 800, 'subsample': 0.9140338115640657, 'colsample_bytree': 0.9970007842653218, 'min_child_samples': 32}. Best is trial 6 with value: 0.6158389849048348.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001793 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:40:20,907] Trial 13 finished with value: 0.6131043535331437 and parameters: {'num_leaves': 122, 'learning_rate': 0.0023488911749048155, 'n_estimators': 600, 'subsample': 0.902103527080376, 'colsample_bytree': 0.9422555183519985, 'min_child_samples': 13}. Best is trial 6 with value: 0.6158389849048348.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001689 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:41:16,624] Trial 14 finished with value: 0.6051192299278058 and parameters: {'num_leaves': 159, 'learning_rate': 0.014379528503219037, 'n_estimators': 900, 'subsample': 0.7626405288444812, 'colsample_bytree': 0.9385506550509323, 'min_child_samples': 30}. Best is trial 6 with value: 0.6158389849048348.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001691 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:41:45,191] Trial 15 finished with value: 0.5666156202143952 and parameters: {'num_leaves': 100, 'learning_rate': 0.0021866343418338826, 'n_estimators': 600, 'subsample': 0.9323989742186762, 'colsample_bytree': 0.6053479909600968, 'min_child_samples': 39}. Best is trial 6 with value: 0.6158389849048348.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001716 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:42:19,692] Trial 16 finished with value: 0.6166046816889084 and parameters: {'num_leaves': 108, 'learning_rate': 0.010458996590318556, 'n_estimators': 700, 'subsample': 0.9439991806633156, 'colsample_bytree': 0.9457041104896856, 'min_child_samples': 17}. Best is trial 16 with value: 0.6166046816889084.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:43:50,685] Trial 17 finished with value: 0.6096040253773791 and parameters: {'num_leaves': 110, 'learning_rate': 0.01136184561030962, 'n_estimators': 2000, 'subsample': 0.9594559787430353, 'colsample_bytree': 0.8402260144630611, 'min_child_samples': 5}. Best is trial 16 with value: 0.6166046816889084.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001747 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:45:13,355] Trial 18 finished with value: 0.595821483264056 and parameters: {'num_leaves': 155, 'learning_rate': 0.021143515186831732, 'n_estimators': 1400, 'subsample': 0.865692201268294, 'colsample_bytree': 0.9340405752342622, 'min_child_samples': 19}. Best is trial 16 with value: 0.6166046816889084.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001643 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:45:23,305] Trial 19 finished with value: 0.6121198862393349 and parameters: {'num_leaves': 101, 'learning_rate': 0.007346106597298568, 'n_estimators': 200, 'subsample': 0.7567719207261199, 'colsample_bytree': 0.9092222641891619, 'min_child_samples': 21}. Best is trial 16 with value: 0.6166046816889084.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001748 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:45:55,308] Trial 20 finished with value: 0.6089477138481733 and parameters: {'num_leaves': 49, 'learning_rate': 0.036530781452577835, 'n_estimators': 1100, 'subsample': 0.9453450261009948, 'colsample_bytree': 0.9607021166592048, 'min_child_samples': 44}. Best is trial 16 with value: 0.6166046816889084.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002871 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:46:34,252] Trial 21 finished with value: 0.6166046816889084 and parameters: {'num_leaves': 132, 'learning_rate': 0.0022287245060435405, 'n_estimators': 700, 'subsample': 0.901712954794989, 'colsample_bytree': 0.9806915113921288, 'min_child_samples': 25}. Best is trial 16 with value: 0.6166046816889084.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001728 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:47:04,799] Trial 22 finished with value: 0.6161671406694378 and parameters: {'num_leaves': 115, 'learning_rate': 0.003256090463829816, 'n_estimators': 600, 'subsample': 0.9562038440742232, 'colsample_bytree': 0.9597361317812286, 'min_child_samples': 16}. Best is trial 16 with value: 0.6166046816889084.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002530 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:47:35,515] Trial 23 finished with value: 0.6160577554145701 and parameters: {'num_leaves': 115, 'learning_rate': 0.003004026189373518, 'n_estimators': 600, 'subsample': 0.8749645388322057, 'colsample_bytree': 0.9108850119830035, 'min_child_samples': 14}. Best is trial 16 with value: 0.6166046816889084.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001732 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:48:03,727] Trial 24 finished with value: 0.6179173047473201 and parameters: {'num_leaves': 155, 'learning_rate': 0.008253857763167505, 'n_estimators': 500, 'subsample': 0.9999601721627903, 'colsample_bytree': 0.8423214038617599, 'min_child_samples': 13}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001503 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:48:23,650] Trial 25 finished with value: 0.5870706628746445 and parameters: {'num_leaves': 156, 'learning_rate': 0.01655196363757244, 'n_estimators': 400, 'subsample': 0.9983911258507526, 'colsample_bytree': 0.8248682352227263, 'min_child_samples': 6}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001633 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:48:53,795] Trial 26 finished with value: 0.6150732881207613 and parameters: {'num_leaves': 172, 'learning_rate': 0.008678818939555992, 'n_estimators': 500, 'subsample': 0.9025408455022913, 'colsample_bytree': 0.878882539410909, 'min_child_samples': 26}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001494 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:49:31,045] Trial 27 finished with value: 0.5843360315029534 and parameters: {'num_leaves': 143, 'learning_rate': 0.005122014818383426, 'n_estimators': 700, 'subsample': 0.9381217722417762, 'colsample_bytree': 0.7823407519893845, 'min_child_samples': 39}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001516 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:50:23,165] Trial 28 finished with value: 0.5802887770728505 and parameters: {'num_leaves': 200, 'learning_rate': 0.011705607601496444, 'n_estimators': 900, 'subsample': 0.8533323710775088, 'colsample_bytree': 0.7725567152520794, 'min_child_samples': 12}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002722 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:50:34,624] Trial 29 finished with value: 0.6152920586304966 and parameters: {'num_leaves': 127, 'learning_rate': 0.049093474713328804, 'n_estimators': 200, 'subsample': 0.7974013094999663, 'colsample_bytree': 0.8476240328401733, 'min_child_samples': 80}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001664 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:50:58,890] Trial 30 finished with value: 0.607197549770291 and parameters: {'num_leaves': 178, 'learning_rate': 0.03745840652804335, 'n_estimators': 400, 'subsample': 0.6623474443311523, 'colsample_bytree': 0.9727184541084877, 'min_child_samples': 10}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001708 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:51:35,315] Trial 31 finished with value: 0.6176985342375848 and parameters: {'num_leaves': 113, 'learning_rate': 0.0034069701376207894, 'n_estimators': 700, 'subsample': 0.9510681737720466, 'colsample_bytree': 0.9385015618800977, 'min_child_samples': 15}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001679 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:52:05,143] Trial 32 finished with value: 0.6152920586304966 and parameters: {'num_leaves': 76, 'learning_rate': 0.006525913139881558, 'n_estimators': 700, 'subsample': 0.9330457126147372, 'colsample_bytree': 0.9219767082696269, 'min_child_samples': 26}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001648 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:53:04,610] Trial 33 finished with value: 0.6178079194924524 and parameters: {'num_leaves': 147, 'learning_rate': 0.004109994859863541, 'n_estimators': 1000, 'subsample': 0.9674668922350832, 'colsample_bytree': 0.8797848092224566, 'min_child_samples': 18}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003007 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:54:05,076] Trial 34 finished with value: 0.6161671406694378 and parameters: {'num_leaves': 149, 'learning_rate': 0.0060979577975839, 'n_estimators': 1000, 'subsample': 0.9970870322777434, 'colsample_bytree': 0.8782218966987291, 'min_child_samples': 19}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002796 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:55:04,710] Trial 35 finished with value: 0.6140888208269525 and parameters: {'num_leaves': 167, 'learning_rate': 0.008926280038337958, 'n_estimators': 1000, 'subsample': 0.9698329272174503, 'colsample_bytree': 0.8886521843483658, 'min_child_samples': 10}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001638 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:55:43,397] Trial 36 finished with value: 0.5835703347188799 and parameters: {'num_leaves': 108, 'learning_rate': 0.0043162088010922665, 'n_estimators': 800, 'subsample': 0.9529267623043389, 'colsample_bytree': 0.8178096005325141, 'min_child_samples': 54}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002908 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:56:05,186] Trial 37 finished with value: 0.6108072631809233 and parameters: {'num_leaves': 83, 'learning_rate': 0.003763819666942163, 'n_estimators': 500, 'subsample': 0.8240883442549825, 'colsample_bytree': 0.8560656677227307, 'min_child_samples': 17}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001756 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:56:40,451] Trial 38 finished with value: 0.5989936556552177 and parameters: {'num_leaves': 23, 'learning_rate': 0.002890489653779579, 'n_estimators': 1200, 'subsample': 0.9213489392847724, 'colsample_bytree': 0.9023956868974387, 'min_child_samples': 5}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001617 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:58:28,917] Trial 39 finished with value: 0.5658499234303216 and parameters: {'num_leaves': 185, 'learning_rate': 0.015165503220184647, 'n_estimators': 1600, 'subsample': 0.9737216472224509, 'colsample_bytree': 0.7533249811799312, 'min_child_samples': 39}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001708 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:58:55,402] Trial 40 finished with value: 0.581382629621527 and parameters: {'num_leaves': 147, 'learning_rate': 0.0059598040384757295, 'n_estimators': 500, 'subsample': 0.8870087860755881, 'colsample_bytree': 0.8311988685089182, 'min_child_samples': 22}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001679 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 05:59:33,464] Trial 41 finished with value: 0.6171516079632465 and parameters: {'num_leaves': 124, 'learning_rate': 0.002487293580526357, 'n_estimators': 700, 'subsample': 0.8978611232900202, 'colsample_bytree': 0.936707743949409, 'min_child_samples': 24}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:00:18,166] Trial 42 finished with value: 0.6169328374535112 and parameters: {'num_leaves': 124, 'learning_rate': 0.00825574892527436, 'n_estimators': 900, 'subsample': 0.9701032657993027, 'colsample_bytree': 0.8650257881153152, 'min_child_samples': 14}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001757 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:01:07,009] Trial 43 finished with value: 0.6147451323561584 and parameters: {'num_leaves': 122, 'learning_rate': 0.0016732772199134522, 'n_estimators': 900, 'subsample': 0.9731875634175482, 'colsample_bytree': 0.8607074964578963, 'min_child_samples': 9}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001672 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:02:08,040] Trial 44 finished with value: 0.5881645154233209 and parameters: {'num_leaves': 142, 'learning_rate': 0.003951175972832454, 'n_estimators': 1100, 'subsample': 0.9214669459112264, 'colsample_bytree': 0.8026916926467674, 'min_child_samples': 23}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001712 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:02:58,295] Trial 45 finished with value: 0.6174797637278495 and parameters: {'num_leaves': 165, 'learning_rate': 0.0014760943893918667, 'n_estimators': 800, 'subsample': 0.9802401471158415, 'colsample_bytree': 0.880425450872361, 'min_child_samples': 28}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001772 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:03:52,520] Trial 46 finished with value: 0.6164952964340407 and parameters: {'num_leaves': 186, 'learning_rate': 0.001610826575466455, 'n_estimators': 800, 'subsample': 0.9997193521507918, 'colsample_bytree': 0.8886278783082159, 'min_child_samples': 29}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001693 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:04:09,499] Trial 47 finished with value: 0.611463574710129 and parameters: {'num_leaves': 160, 'learning_rate': 0.0014133652864161388, 'n_estimators': 300, 'subsample': 0.9586254853250842, 'colsample_bytree': 0.9219992312656696, 'min_child_samples': 36}. Best is trial 24 with value: 0.6179173047473201.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001847 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:05:21,732] Trial 48 finished with value: 0.6181360752570554 and parameters: {'num_leaves': 149, 'learning_rate': 0.0027389900681531, 'n_estimators': 1100, 'subsample': 0.9836970361103197, 'colsample_bytree': 0.9219810502449365, 'min_child_samples': 63}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001809 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:06:50,911] Trial 49 finished with value: 0.6145263618464231 and parameters: {'num_leaves': 163, 'learning_rate': 0.0011982560052789916, 'n_estimators': 1300, 'subsample': 0.9834693690647699, 'colsample_bytree': 0.8739140047574069, 'min_child_samples': 63}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001463 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:07:57,652] Trial 50 finished with value: 0.5807263180923211 and parameters: {'num_leaves': 152, 'learning_rate': 0.0027209350601012892, 'n_estimators': 1100, 'subsample': 0.9861215157907897, 'colsample_bytree': 0.7052193514164828, 'min_child_samples': 73}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001685 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:08:44,059] Trial 51 finished with value: 0.6160577554145701 and parameters: {'num_leaves': 140, 'learning_rate': 0.0019002325254495725, 'n_estimators': 800, 'subsample': 0.9586072119961789, 'colsample_bytree': 0.925057210521148, 'min_child_samples': 60}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002984 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:09:49,768] Trial 52 finished with value: 0.6174797637278495 and parameters: {'num_leaves': 166, 'learning_rate': 0.002697737542548026, 'n_estimators': 1000, 'subsample': 0.9339933245455558, 'colsample_bytree': 0.9015292095769342, 'min_child_samples': 50}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001679 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:11:17,723] Trial 53 finished with value: 0.6169328374535112 and parameters: {'num_leaves': 168, 'learning_rate': 0.003524931973226019, 'n_estimators': 1200, 'subsample': 0.9257285513638596, 'colsample_bytree': 0.8907293388121291, 'min_child_samples': 47}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001683 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:12:32,473] Trial 54 finished with value: 0.6150732881207613 and parameters: {'num_leaves': 190, 'learning_rate': 0.004619772672570977, 'n_estimators': 1000, 'subsample': 0.9447364356673885, 'colsample_bytree': 0.8994142860038542, 'min_child_samples': 77}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001660 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:14:06,115] Trial 55 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 176, 'learning_rate': 0.002136179615372273, 'n_estimators': 1300, 'subsample': 0.9788186082573376, 'colsample_bytree': 0.8494083758703291, 'min_child_samples': 57}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001565 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:15:11,105] Trial 56 finished with value: 0.6150732881207613 and parameters: {'num_leaves': 164, 'learning_rate': 0.0015879279764985316, 'n_estimators': 1000, 'subsample': 0.7058364751597825, 'colsample_bytree': 0.9124322939605276, 'min_child_samples': 63}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001698 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:16:15,065] Trial 57 finished with value: 0.6161671406694378 and parameters: {'num_leaves': 179, 'learning_rate': 0.005371354836231277, 'n_estimators': 900, 'subsample': 0.9865511286910766, 'colsample_bytree': 0.955735124056741, 'min_child_samples': 68}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002841 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:17:22,033] Trial 58 finished with value: 0.5645373003719099 and parameters: {'num_leaves': 132, 'learning_rate': 0.001015865121538786, 'n_estimators': 1200, 'subsample': 0.9610385328215777, 'colsample_bytree': 0.832483162415636, 'min_child_samples': 50}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001805 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:18:28,896] Trial 59 finished with value: 0.582914023189674 and parameters: {'num_leaves': 148, 'learning_rate': 0.0030484543246346468, 'n_estimators': 1100, 'subsample': 0.9369910677324839, 'colsample_bytree': 0.6275066024540943, 'min_child_samples': 83}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002840 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:20:04,284] Trial 60 finished with value: 0.6172609932181142 and parameters: {'num_leaves': 173, 'learning_rate': 0.002517111892979466, 'n_estimators': 1400, 'subsample': 0.9098829738999592, 'colsample_bytree': 0.8677352300030291, 'min_child_samples': 28}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001698 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:21:52,887] Trial 61 finished with value: 0.6161671406694378 and parameters: {'num_leaves': 171, 'learning_rate': 0.002529695481932943, 'n_estimators': 1500, 'subsample': 0.9479815639461912, 'colsample_bytree': 0.8948898554536386, 'min_child_samples': 29}. Best is trial 48 with value: 0.6181360752570554.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:23:35,704] Trial 62 finished with value: 0.6183548457667907 and parameters: {'num_leaves': 159, 'learning_rate': 0.002104998379608275, 'n_estimators': 1500, 'subsample': 0.9163416638638208, 'colsample_bytree': 0.8668714198990523, 'min_child_samples': 35}. Best is trial 62 with value: 0.6183548457667907.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001637 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:25:40,857] Trial 63 finished with value: 0.6193393130605994 and parameters: {'num_leaves': 155, 'learning_rate': 0.0013048581297791474, 'n_estimators': 1800, 'subsample': 0.9652994744331221, 'colsample_bytree': 0.8479699962992974, 'min_child_samples': 35}. Best is trial 63 with value: 0.6193393130605994.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001775 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:27:51,769] Trial 64 finished with value: 0.6199956245898053 and parameters: {'num_leaves': 153, 'learning_rate': 0.0012647626382365506, 'n_estimators': 1900, 'subsample': 0.9843805853464391, 'colsample_bytree': 0.8430617078393272, 'min_child_samples': 35}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001598 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:30:01,063] Trial 65 finished with value: 0.587836359658718 and parameters: {'num_leaves': 156, 'learning_rate': 0.001869754819028678, 'n_estimators': 1900, 'subsample': 0.9646229356258067, 'colsample_bytree': 0.8110119035355288, 'min_child_samples': 35}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001753 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:32:04,866] Trial 66 finished with value: 0.6169328374535112 and parameters: {'num_leaves': 138, 'learning_rate': 0.00127074874856889, 'n_estimators': 1800, 'subsample': 0.9999032165043035, 'colsample_bytree': 0.84693154896633, 'min_child_samples': 45}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002855 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:33:50,031] Trial 67 finished with value: 0.5747101290746007 and parameters: {'num_leaves': 144, 'learning_rate': 0.0011323914405810293, 'n_estimators': 1700, 'subsample': 0.9867126444665592, 'colsample_bytree': 0.7909868142533518, 'min_child_samples': 32}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001526 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:36:13,147] Trial 68 finished with value: 0.618573616276526 and parameters: {'num_leaves': 154, 'learning_rate': 0.0020701493776966455, 'n_estimators': 2000, 'subsample': 0.9496969133420778, 'colsample_bytree': 0.8344372217868391, 'min_child_samples': 43}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001837 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:38:24,131] Trial 69 finished with value: 0.555895865237366 and parameters: {'num_leaves': 152, 'learning_rate': 0.09653256142986333, 'n_estimators': 2000, 'subsample': 0.8881618015948147, 'colsample_bytree': 0.8397924831849538, 'min_child_samples': 41}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001611 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:40:30,495] Trial 70 finished with value: 0.5855392693064975 and parameters: {'num_leaves': 158, 'learning_rate': 0.0017712636115110281, 'n_estimators': 1900, 'subsample': 0.9149085511726087, 'colsample_bytree': 0.8147942586996884, 'min_child_samples': 34}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001689 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:42:27,514] Trial 71 finished with value: 0.6184642310216583 and parameters: {'num_leaves': 131, 'learning_rate': 0.0021307686166393575, 'n_estimators': 1800, 'subsample': 0.9514438818059128, 'colsample_bytree': 0.8354758831016381, 'min_child_samples': 42}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:44:29,264] Trial 72 finished with value: 0.6176985342375848 and parameters: {'num_leaves': 130, 'learning_rate': 0.0021216862917252557, 'n_estimators': 1800, 'subsample': 0.9687390884357155, 'colsample_bytree': 0.8346747534655948, 'min_child_samples': 42}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001868 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:46:11,114] Trial 73 finished with value: 0.5775541457011595 and parameters: {'num_leaves': 136, 'learning_rate': 0.0013220059691637475, 'n_estimators': 1700, 'subsample': 0.9449619144653294, 'colsample_bytree': 0.821454522017871, 'min_child_samples': 45}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001831 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:48:32,556] Trial 74 finished with value: 0.618245460511923 and parameters: {'num_leaves': 151, 'learning_rate': 0.002069245773909703, 'n_estimators': 2000, 'subsample': 0.9923842910090706, 'colsample_bytree': 0.8557698752751924, 'min_child_samples': 38}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001539 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:50:46,396] Trial 75 finished with value: 0.5884926711879239 and parameters: {'num_leaves': 153, 'learning_rate': 0.002155975423563167, 'n_estimators': 2000, 'subsample': 0.9897953059928827, 'colsample_bytree': 0.7728092444610735, 'min_child_samples': 37}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002743 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:52:52,393] Trial 76 finished with value: 0.5535987748851455 and parameters: {'num_leaves': 161, 'learning_rate': 0.024570369202114892, 'n_estimators': 1900, 'subsample': 0.9760981637313686, 'colsample_bytree': 0.7990532705664707, 'min_child_samples': 48}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001679 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:54:41,411] Trial 77 finished with value: 0.6175891489827171 and parameters: {'num_leaves': 146, 'learning_rate': 0.0015533214635587427, 'n_estimators': 1600, 'subsample': 0.7643899959200308, 'colsample_bytree': 0.8509090954515869, 'min_child_samples': 53}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001883 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","[I 2025-03-14 06:56:45,615] Trial 78 finished with value: 0.6170422227083789 and parameters: {'num_leaves': 151, 'learning_rate': 0.001066796144887112, 'n_estimators': 1800, 'subsample': 0.9285864268576812, 'colsample_bytree': 0.8590103203783082, 'min_child_samples': 42}. Best is trial 64 with value: 0.6199956245898053.\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001732 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 51966, number of used features: 3\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n","[LightGBM] [Info] Start training from score -1.791759\n"]}],"source":["# Install missing libraries if necessary\n","# !pip install optuna lightgbm catboost xgboost imbalanced-learn joblib pandas numpy scikit-learn\n","!pip install optuna\n","!pip install catboost\n","import pandas as pd\n","import numpy as np\n","import optuna\n","import joblib\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler, LabelEncoder\n","from sklearn.impute import KNNImputer\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from imblearn.over_sampling import BorderlineSMOTE, ADASYN\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from catboost import CatBoostClassifier\n","from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","# ✅ Load Dataset\n","data = pd.read_csv('/balanced_fall_detection_dataset.csv')\n","X = data.drop('Label', axis=1)\n","y = data['Label']\n","\n","# ✅ Encode Labels\n","le = LabelEncoder()\n","y_encoded = le.fit_transform(y)\n","\n","# ✅ Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.15, random_state=42)\n","\n","# ✅ Feature Selection: Correlation + XGBoost Importance\n","xgb_temp = XGBClassifier(n_estimators=500, random_state=42)\n","xgb_temp.fit(X_train, y_train)\n","feature_importance = xgb_temp.feature_importances_\n","\n","important_features = X.columns[np.argsort(-feature_importance)[:30]]  # Increased to 30 features\n","X_train = X_train[important_features]\n","X_test = X_test[important_features]\n","\n","# ✅ Handle Missing Data with KNN Imputer\n","imputer = KNNImputer(n_neighbors=5)\n","X_train_imputed = imputer.fit_transform(X_train)\n","X_test_imputed = imputer.transform(X_test)\n","\n","# ✅ Robust Scaling\n","scaler = RobustScaler()\n","X_train_scaled = scaler.fit_transform(X_train_imputed)\n","X_test_scaled = scaler.transform(X_test_imputed)\n","\n","# ✅ Dimensionality Reduction (PCA Alternative)\n","max_pca_components = min(10, X_train_scaled.shape[1])\n","pca = PCA(n_components=max_pca_components)\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","# ✅ Hybrid Data Resampling (SMOTE + ADASYN)\n","smote = BorderlineSMOTE(random_state=42, kind='borderline-1')\n","adasyn = ADASYN(random_state=42, sampling_strategy='minority')\n","\n","X_resampled, y_resampled = smote.fit_resample(X_train_pca, y_train)\n","X_resampled, y_resampled = adasyn.fit_resample(X_resampled, y_resampled)\n","\n","# ✅ Optuna Hyperparameter Tuning (LGBM)\n","def lgbm_objective(trial):\n","    params = {\n","        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n","        'n_estimators': trial.suggest_int('n_estimators', 200, 2000, step=100),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100)\n","    }\n","    model = LGBMClassifier(**params, random_state=42)\n","    model.fit(X_resampled, y_resampled)\n","    y_pred = model.predict(X_test_pca)\n","    return accuracy_score(y_test, y_pred)\n","\n","lgbm_study = optuna.create_study(direction='maximize')\n","lgbm_study.optimize(lgbm_objective, n_trials=500)  # More trials for better tuning\n","best_lgbm_params = lgbm_study.best_params\n","\n","# ✅ Optuna Hyperparameter Tuning (XGBoost)\n","def xgb_objective(trial):\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 200, 2000, step=100),\n","        'max_depth': trial.suggest_int('max_depth', 3, 15),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'gamma': trial.suggest_float('gamma', 0, 0.3),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10)\n","    }\n","    model = XGBClassifier(**params, random_state=42, eval_metric='mlogloss')\n","    model.fit(X_resampled, y_resampled)\n","    y_pred = model.predict(X_test_pca)\n","    return accuracy_score(y_test, y_pred)\n","\n","xgb_study = optuna.create_study(direction='maximize')\n","xgb_study.optimize(xgb_objective, n_trials=500)\n","best_xgb_params = xgb_study.best_params\n","\n","# ✅ Train Final Models\n","rf_model = RandomForestClassifier(n_estimators=700, random_state=42)\n","xgb_model = XGBClassifier(**best_xgb_params, random_state=42, eval_metric='mlogloss')\n","lgbm_model = LGBMClassifier(**best_lgbm_params, random_state=42)\n","cat_model = CatBoostClassifier(n_estimators=700, depth=10, learning_rate=0.02, verbose=0)\n","\n","# ✅ Optimized Stacking Model\n","meta_learner = LogisticRegression(penalty='l1', solver='liblinear')\n","\n","stacking_clf = StackingClassifier(\n","    estimators=[\n","        ('rf', rf_model),\n","        ('xgb', xgb_model),\n","        ('lgbm', lgbm_model),\n","        ('cat', cat_model)\n","    ],\n","    final_estimator=meta_learner\n",")\n","\n","# ✅ Train Stacking Classifier\n","stacking_clf.fit(X_resampled, y_resampled)\n","\n","# ✅ Evaluate Model\n","y_pred = stacking_clf.predict(X_test_pca)\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n","\n","# ✅ Save Model and Preprocessing Objects\n","joblib.dump(stacking_clf, 'final_stacking_model.pkl')\n","joblib.dump(scaler, 'scaler.pkl')\n","joblib.dump(imputer, 'imputer.pkl')\n","joblib.dump(pca, 'pca.pkl')\n","joblib.dump(le, 'label_encoder.pkl')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":129842,"status":"ok","timestamp":1741953515534,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"},"user_tz":-330},"id":"QfHZBSCRftAN","outputId":"3767f2b1-391c-4218-e3c1-47d82ac86e00"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6624 - loss: 0.6536 - val_accuracy: 0.6880 - val_loss: 0.6210\n","Epoch 2/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6756 - loss: 0.6333 - val_accuracy: 0.6880 - val_loss: 0.6218\n","Epoch 3/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6798 - loss: 0.6287 - val_accuracy: 0.6880 - val_loss: 0.6217\n","Epoch 4/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6723 - loss: 0.6315 - val_accuracy: 0.6880 - val_loss: 0.6211\n","Epoch 5/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7093 - loss: 0.6058 - val_accuracy: 0.6880 - val_loss: 0.6229\n","Epoch 6/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6925 - loss: 0.6178 - val_accuracy: 0.6880 - val_loss: 0.6224\n","Epoch 7/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6885 - loss: 0.6165 - val_accuracy: 0.6880 - val_loss: 0.6229\n","Epoch 8/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6965 - loss: 0.6084 - val_accuracy: 0.6880 - val_loss: 0.6236\n","Epoch 9/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.7032 - loss: 0.6092 - val_accuracy: 0.6880 - val_loss: 0.6232\n","Epoch 10/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6882 - loss: 0.6178 - val_accuracy: 0.6870 - val_loss: 0.6246\n","Epoch 11/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6836 - loss: 0.6209 - val_accuracy: 0.6870 - val_loss: 0.6246\n","Epoch 12/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6969 - loss: 0.6110 - val_accuracy: 0.6890 - val_loss: 0.6247\n","Epoch 13/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6928 - loss: 0.6156 - val_accuracy: 0.6890 - val_loss: 0.6253\n","Epoch 14/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.6817 - loss: 0.6213 - val_accuracy: 0.6830 - val_loss: 0.6267\n","Epoch 15/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7049 - loss: 0.6034 - val_accuracy: 0.6860 - val_loss: 0.6248\n","Epoch 16/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6994 - loss: 0.6047 - val_accuracy: 0.6860 - val_loss: 0.6261\n","Epoch 17/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6837 - loss: 0.6141 - val_accuracy: 0.6870 - val_loss: 0.6252\n","Epoch 18/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6959 - loss: 0.6051 - val_accuracy: 0.6880 - val_loss: 0.6265\n","Epoch 19/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6950 - loss: 0.6048 - val_accuracy: 0.6900 - val_loss: 0.6278\n","Epoch 20/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6937 - loss: 0.6018 - val_accuracy: 0.6750 - val_loss: 0.6305\n","Epoch 21/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7078 - loss: 0.5963 - val_accuracy: 0.6860 - val_loss: 0.6282\n","Epoch 22/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6975 - loss: 0.6057 - val_accuracy: 0.6770 - val_loss: 0.6317\n","Epoch 23/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7049 - loss: 0.5971 - val_accuracy: 0.6880 - val_loss: 0.6296\n","Epoch 24/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7112 - loss: 0.5872 - val_accuracy: 0.6800 - val_loss: 0.6273\n","Epoch 25/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6990 - loss: 0.5988 - val_accuracy: 0.6780 - val_loss: 0.6363\n","Epoch 26/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6997 - loss: 0.5959 - val_accuracy: 0.6780 - val_loss: 0.6341\n","Epoch 27/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6991 - loss: 0.5960 - val_accuracy: 0.6790 - val_loss: 0.6314\n","Epoch 28/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6977 - loss: 0.6003 - val_accuracy: 0.6750 - val_loss: 0.6336\n","Epoch 29/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6963 - loss: 0.6014 - val_accuracy: 0.6790 - val_loss: 0.6335\n","Epoch 30/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7109 - loss: 0.5891 - val_accuracy: 0.6730 - val_loss: 0.6329\n","Epoch 31/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7001 - loss: 0.5891 - val_accuracy: 0.6650 - val_loss: 0.6398\n","Epoch 32/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7054 - loss: 0.5870 - val_accuracy: 0.6700 - val_loss: 0.6393\n","Epoch 33/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.7051 - loss: 0.5817 - val_accuracy: 0.6710 - val_loss: 0.6409\n","Epoch 34/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7077 - loss: 0.5810 - val_accuracy: 0.6670 - val_loss: 0.6429\n","Epoch 35/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6955 - loss: 0.5957 - val_accuracy: 0.6700 - val_loss: 0.6474\n","Epoch 36/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7217 - loss: 0.5755 - val_accuracy: 0.6570 - val_loss: 0.6489\n","Epoch 37/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7066 - loss: 0.5806 - val_accuracy: 0.6490 - val_loss: 0.6606\n","Epoch 38/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7141 - loss: 0.5740 - val_accuracy: 0.6560 - val_loss: 0.6544\n","Epoch 39/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7118 - loss: 0.5726 - val_accuracy: 0.6630 - val_loss: 0.6540\n","Epoch 40/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7131 - loss: 0.5780 - val_accuracy: 0.6550 - val_loss: 0.6650\n","Epoch 41/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7254 - loss: 0.5637 - val_accuracy: 0.6720 - val_loss: 0.6513\n","Epoch 42/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7106 - loss: 0.5787 - val_accuracy: 0.6660 - val_loss: 0.6589\n","Epoch 43/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7248 - loss: 0.5560 - val_accuracy: 0.6740 - val_loss: 0.6584\n","Epoch 44/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7193 - loss: 0.5653 - val_accuracy: 0.6690 - val_loss: 0.6594\n","Epoch 45/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7244 - loss: 0.5609 - val_accuracy: 0.6580 - val_loss: 0.6714\n","Epoch 46/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7239 - loss: 0.5557 - val_accuracy: 0.6720 - val_loss: 0.6622\n","Epoch 47/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7248 - loss: 0.5634 - val_accuracy: 0.6700 - val_loss: 0.6707\n","Epoch 48/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7252 - loss: 0.5559 - val_accuracy: 0.6670 - val_loss: 0.6786\n","Epoch 49/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7270 - loss: 0.5476 - val_accuracy: 0.6670 - val_loss: 0.6852\n","Epoch 50/50\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7214 - loss: 0.5590 - val_accuracy: 0.6700 - val_loss: 0.6738\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6411 - loss: 0.6926\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["Model Accuracy: 67.00%\n","Model saved successfully!\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","\n","# Load dataset\n","file_path = \"/content/stroke_detection_dataset (1).csv\"\n","df = pd.read_csv(file_path)\n","\n","# Split features and labels\n","X = df.drop(columns=['Label']).values  # Sensor data\n","y = df['Label'].values  # Stroke (1) or Fake Stroke (0)\n","\n","# Normalize sensor data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Reshape for LSTM (samples, timesteps, features)\n","X_reshaped = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n","\n","# Build LSTM Model\n","model = Sequential([\n","    LSTM(64, return_sequences=True, input_shape=(1, X_scaled.shape[1])),\n","    Dropout(0.2),\n","    LSTM(32, return_sequences=False),\n","    Dense(16, activation='relu'),\n","    Dropout(0.2),\n","    Dense(1, activation='sigmoid')  # Binary classification\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test))\n","\n","# Evaluate accuracy\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Save model for ESP32 TensorFlow Lite conversion\n","model.save(\"/mnt/data/stroke_detection_model.h5\")\n","print(\"Model saved successfully!\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":7178,"status":"error","timestamp":1741955778023,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"},"user_tz":-330},"id":"sCEQtYBjpdkt","outputId":"39bcef41-7c48-4ae6-c68a-bfe1d940c426"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"ename":"ValueError","evalue":"The total size of the tensor must be unchanged. Received: input_shape=(768,), target_shape=(10, 1)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-ea658cabb923>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# 🎯 LSTM Layers (Time-Series Analysis)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer, rebuild)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrebuild\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_rebuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36m_maybe_rebuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_shape\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m# We can build the Sequential model if the first layer has the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36mbuild_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0moriginal_build_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0;31m# Record build config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_build_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;31m# Can happen if shape inference is not implemented.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation_utils.py\u001b[0m in \u001b[0;36mcompute_reshape_output_shape\u001b[0;34m(input_shape, newshape, newshape_arg_name)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munknown_dim_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0;34m\"The total size of the tensor must be unchanged. Received: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;34mf\"input_shape={input_shape}, {newshape_arg_name}={newshape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The total size of the tensor must be unchanged. Received: input_shape=(768,), target_shape=(10, 1)"]}],"source":["!pip install tensorflow numpy pandas scikit-learn imbalanced-learn\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Flatten, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils.class_weight import compute_class_weight\n","from imblearn.over_sampling import SMOTE\n","\n","# ✅ Load dataset\n","df = pd.read_csv(\"/content/stroke_prediction_dataset_58000_real_fake.csv\")\n","\n","# ✅ Separate features (X) and labels (y)\n","X = df.iloc[:, :-1].values  # Sensor data (features)\n","y = df.iloc[:, -1].values   # Stroke classification (1: Real, -1: Fake, 0: No Stroke)\n","\n","# ✅ Convert labels (-1, 0, 1) to (0, 1, 2) for categorical cross-entropy\n","y = np.where(y == -1, 0, y)  # -1 → 0 (Fake), 0 → 1 (No Stroke), 1 → 2 (Real Stroke)\n","\n","# ✅ Normalize sensor data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Handle class imbalance using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# ✅ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# ✅ Reshape input for CNN-LSTM\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n","\n","# ✅ Define CNN-LSTM Model\n","model = Sequential()\n","\n","# 🎯 CNN Layers (Feature Extraction)\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","model.add(Flatten())\n","\n","# 🎯 LSTM Layers (Time-Series Analysis)\n","model.add(tf.keras.layers.Reshape((X_train.shape[1], 1)))\n","model.add(LSTM(units=64, return_sequences=True))\n","model.add(Dropout(0.3))\n","model.add(LSTM(units=32, return_sequences=False))\n","model.add(Dropout(0.3))\n","\n","# 🎯 Fully Connected Layers\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(3, activation='softmax'))  # 3 Classes: Fake Stroke (0), No Stroke (1), Real Stroke (2)\n","\n","# ✅ Compile Model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# ✅ Compute class weights for imbalanced data\n","class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n","class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n","\n","# ✅ Train model\n","history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test),\n","                    class_weight=class_weight_dict, verbose=1)\n","\n","# ✅ Evaluate Model\n","test_loss, test_acc = model.evaluate(X_test, y_test)\n","print(f\"🔥 Test Accuracy: {test_acc * 100:.2f}%\")\n","\n","# ✅ Save the Model\n","model.save(\"stroke_detection_model.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2581599,"status":"ok","timestamp":1741958481951,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"},"user_tz":-330},"id":"Rha1_qIjqUSL","outputId":"c46d11d9-af41-44bd-85d7-d1902355d914"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 21ms/step - accuracy: 0.5018 - loss: 0.7604 - val_accuracy: 0.5072 - val_loss: 0.6934\n","Epoch 2/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 20ms/step - accuracy: 0.5087 - loss: 0.6951 - val_accuracy: 0.5189 - val_loss: 0.6924\n","Epoch 3/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.5165 - loss: 0.6929 - val_accuracy: 0.5225 - val_loss: 0.6911\n","Epoch 4/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 20ms/step - accuracy: 0.5271 - loss: 0.6913 - val_accuracy: 0.5125 - val_loss: 0.6921\n","Epoch 5/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.5254 - loss: 0.6915 - val_accuracy: 0.5391 - val_loss: 0.6880\n","Epoch 6/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.5355 - loss: 0.6898 - val_accuracy: 0.5465 - val_loss: 0.6876\n","Epoch 7/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.5387 - loss: 0.6895 - val_accuracy: 0.5466 - val_loss: 0.6867\n","Epoch 8/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 20ms/step - accuracy: 0.5450 - loss: 0.6878 - val_accuracy: 0.5522 - val_loss: 0.6860\n","Epoch 9/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.5485 - loss: 0.6870 - val_accuracy: 0.5569 - val_loss: 0.6847\n","Epoch 10/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.5513 - loss: 0.6856 - val_accuracy: 0.5607 - val_loss: 0.6831\n","Epoch 11/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.5559 - loss: 0.6848 - val_accuracy: 0.5614 - val_loss: 0.6827\n","Epoch 12/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.5546 - loss: 0.6850 - val_accuracy: 0.5484 - val_loss: 0.6862\n","Epoch 13/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.5606 - loss: 0.6831 - val_accuracy: 0.5616 - val_loss: 0.6835\n","Epoch 14/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.5606 - loss: 0.6826 - val_accuracy: 0.5571 - val_loss: 0.6845\n","Epoch 15/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.5660 - loss: 0.6815 - val_accuracy: 0.5637 - val_loss: 0.6817\n","Epoch 16/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.5677 - loss: 0.6818 - val_accuracy: 0.5652 - val_loss: 0.6820\n","Epoch 17/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.5621 - loss: 0.6819 - val_accuracy: 0.5539 - val_loss: 0.6853\n","Epoch 18/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.5651 - loss: 0.6814 - val_accuracy: 0.5655 - val_loss: 0.6808\n","Epoch 19/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.5680 - loss: 0.6794 - val_accuracy: 0.5572 - val_loss: 0.6833\n","Epoch 20/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.5666 - loss: 0.6800 - val_accuracy: 0.5676 - val_loss: 0.6804\n","Epoch 21/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.5712 - loss: 0.6781 - val_accuracy: 0.5639 - val_loss: 0.6813\n","Epoch 22/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 22ms/step - accuracy: 0.5720 - loss: 0.6778 - val_accuracy: 0.5657 - val_loss: 0.6813\n","Epoch 23/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 19ms/step - accuracy: 0.5744 - loss: 0.6776 - val_accuracy: 0.5632 - val_loss: 0.6816\n","Epoch 24/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.5737 - loss: 0.6773 - val_accuracy: 0.5676 - val_loss: 0.6795\n","Epoch 25/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 0.5723 - loss: 0.6762 - val_accuracy: 0.5674 - val_loss: 0.6797\n","Epoch 26/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.5751 - loss: 0.6756 - val_accuracy: 0.5629 - val_loss: 0.6798\n","Epoch 27/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.5785 - loss: 0.6739 - val_accuracy: 0.5644 - val_loss: 0.6812\n","Epoch 28/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.5772 - loss: 0.6738 - val_accuracy: 0.5671 - val_loss: 0.6799\n","Epoch 29/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.5824 - loss: 0.6720 - val_accuracy: 0.5717 - val_loss: 0.6781\n","Epoch 30/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.5808 - loss: 0.6735 - val_accuracy: 0.5692 - val_loss: 0.6781\n","Epoch 31/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.5780 - loss: 0.6731 - val_accuracy: 0.5694 - val_loss: 0.6793\n","Epoch 32/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.5847 - loss: 0.6709 - val_accuracy: 0.5616 - val_loss: 0.6836\n","Epoch 33/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.5832 - loss: 0.6723 - val_accuracy: 0.5687 - val_loss: 0.6786\n","Epoch 34/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.5859 - loss: 0.6706 - val_accuracy: 0.5705 - val_loss: 0.6774\n","Epoch 35/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 20ms/step - accuracy: 0.5877 - loss: 0.6695 - val_accuracy: 0.5689 - val_loss: 0.6776\n","Epoch 36/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - accuracy: 0.5876 - loss: 0.6690 - val_accuracy: 0.5745 - val_loss: 0.6762\n","Epoch 37/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.5872 - loss: 0.6692 - val_accuracy: 0.5747 - val_loss: 0.6765\n","Epoch 38/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.5851 - loss: 0.6681 - val_accuracy: 0.5723 - val_loss: 0.6772\n","Epoch 39/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - accuracy: 0.5852 - loss: 0.6686 - val_accuracy: 0.5711 - val_loss: 0.6789\n","Epoch 40/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 20ms/step - accuracy: 0.5937 - loss: 0.6666 - val_accuracy: 0.5764 - val_loss: 0.6750\n","Epoch 41/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.5944 - loss: 0.6652 - val_accuracy: 0.5755 - val_loss: 0.6770\n","Epoch 42/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 20ms/step - accuracy: 0.5932 - loss: 0.6656 - val_accuracy: 0.5750 - val_loss: 0.6766\n","Epoch 43/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.5928 - loss: 0.6651 - val_accuracy: 0.5761 - val_loss: 0.6748\n","Epoch 44/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.5940 - loss: 0.6653 - val_accuracy: 0.5694 - val_loss: 0.6780\n","Epoch 45/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - accuracy: 0.5984 - loss: 0.6644 - val_accuracy: 0.5790 - val_loss: 0.6749\n","Epoch 46/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 19ms/step - accuracy: 0.5994 - loss: 0.6629 - val_accuracy: 0.5811 - val_loss: 0.6744\n","Epoch 47/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.5974 - loss: 0.6628 - val_accuracy: 0.5761 - val_loss: 0.6754\n","Epoch 48/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 0.5982 - loss: 0.6635 - val_accuracy: 0.5790 - val_loss: 0.6738\n","Epoch 49/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.5954 - loss: 0.6644 - val_accuracy: 0.5780 - val_loss: 0.6741\n","Epoch 50/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 0.5983 - loss: 0.6620 - val_accuracy: 0.5792 - val_loss: 0.6735\n","Epoch 51/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6038 - loss: 0.6608 - val_accuracy: 0.5798 - val_loss: 0.6745\n","Epoch 52/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 0.6031 - loss: 0.6596 - val_accuracy: 0.5807 - val_loss: 0.6738\n","Epoch 53/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.6041 - loss: 0.6596 - val_accuracy: 0.5794 - val_loss: 0.6748\n","Epoch 54/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.6006 - loss: 0.6597 - val_accuracy: 0.5829 - val_loss: 0.6725\n","Epoch 55/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.6030 - loss: 0.6592 - val_accuracy: 0.5865 - val_loss: 0.6729\n","Epoch 56/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 21ms/step - accuracy: 0.6046 - loss: 0.6585 - val_accuracy: 0.5803 - val_loss: 0.6740\n","Epoch 57/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 21ms/step - accuracy: 0.6076 - loss: 0.6572 - val_accuracy: 0.5820 - val_loss: 0.6736\n","Epoch 58/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 19ms/step - accuracy: 0.6035 - loss: 0.6586 - val_accuracy: 0.5861 - val_loss: 0.6722\n","Epoch 59/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.6079 - loss: 0.6544 - val_accuracy: 0.5826 - val_loss: 0.6723\n","Epoch 60/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 0.6077 - loss: 0.6561 - val_accuracy: 0.5849 - val_loss: 0.6710\n","Epoch 61/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.6119 - loss: 0.6514 - val_accuracy: 0.5844 - val_loss: 0.6709\n","Epoch 62/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 20ms/step - accuracy: 0.6140 - loss: 0.6531 - val_accuracy: 0.5801 - val_loss: 0.6732\n","Epoch 63/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - accuracy: 0.6117 - loss: 0.6547 - val_accuracy: 0.5854 - val_loss: 0.6726\n","Epoch 64/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 20ms/step - accuracy: 0.6114 - loss: 0.6524 - val_accuracy: 0.5894 - val_loss: 0.6706\n","Epoch 65/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6162 - loss: 0.6528 - val_accuracy: 0.5888 - val_loss: 0.6703\n","Epoch 66/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6186 - loss: 0.6495 - val_accuracy: 0.5919 - val_loss: 0.6690\n","Epoch 67/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.6166 - loss: 0.6507 - val_accuracy: 0.5894 - val_loss: 0.6685\n","Epoch 68/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6158 - loss: 0.6506 - val_accuracy: 0.5896 - val_loss: 0.6692\n","Epoch 69/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 21ms/step - accuracy: 0.6207 - loss: 0.6487 - val_accuracy: 0.5910 - val_loss: 0.6683\n","Epoch 70/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6171 - loss: 0.6495 - val_accuracy: 0.5933 - val_loss: 0.6702\n","Epoch 71/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 20ms/step - accuracy: 0.6154 - loss: 0.6496 - val_accuracy: 0.5948 - val_loss: 0.6681\n","Epoch 72/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.6164 - loss: 0.6491 - val_accuracy: 0.5956 - val_loss: 0.6669\n","Epoch 73/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.6200 - loss: 0.6461 - val_accuracy: 0.5919 - val_loss: 0.6712\n","Epoch 74/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.6185 - loss: 0.6475 - val_accuracy: 0.5911 - val_loss: 0.6696\n","Epoch 75/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6250 - loss: 0.6449 - val_accuracy: 0.5936 - val_loss: 0.6676\n","Epoch 76/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.6274 - loss: 0.6437 - val_accuracy: 0.5980 - val_loss: 0.6662\n","Epoch 77/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.6207 - loss: 0.6463 - val_accuracy: 0.5958 - val_loss: 0.6674\n","Epoch 78/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6240 - loss: 0.6448 - val_accuracy: 0.5956 - val_loss: 0.6662\n","Epoch 79/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.6237 - loss: 0.6448 - val_accuracy: 0.5957 - val_loss: 0.6662\n","Epoch 80/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.6287 - loss: 0.6423 - val_accuracy: 0.5909 - val_loss: 0.6664\n","Epoch 81/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.6287 - loss: 0.6411 - val_accuracy: 0.5943 - val_loss: 0.6687\n","Epoch 82/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.6288 - loss: 0.6408 - val_accuracy: 0.5943 - val_loss: 0.6685\n","Epoch 83/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6297 - loss: 0.6398 - val_accuracy: 0.6001 - val_loss: 0.6650\n","Epoch 84/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.6271 - loss: 0.6401 - val_accuracy: 0.5946 - val_loss: 0.6658\n","Epoch 85/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 0.6328 - loss: 0.6379 - val_accuracy: 0.5954 - val_loss: 0.6648\n","Epoch 86/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6297 - loss: 0.6402 - val_accuracy: 0.5984 - val_loss: 0.6679\n","Epoch 87/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.6292 - loss: 0.6401 - val_accuracy: 0.5994 - val_loss: 0.6639\n","Epoch 88/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.6334 - loss: 0.6374 - val_accuracy: 0.5985 - val_loss: 0.6660\n","Epoch 89/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.6369 - loss: 0.6372 - val_accuracy: 0.6069 - val_loss: 0.6626\n","Epoch 90/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6351 - loss: 0.6368 - val_accuracy: 0.5995 - val_loss: 0.6642\n","Epoch 91/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.6345 - loss: 0.6370 - val_accuracy: 0.6059 - val_loss: 0.6628\n","Epoch 92/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 20ms/step - accuracy: 0.6312 - loss: 0.6369 - val_accuracy: 0.6009 - val_loss: 0.6630\n","Epoch 93/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.6325 - loss: 0.6367 - val_accuracy: 0.6043 - val_loss: 0.6637\n","Epoch 94/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 20ms/step - accuracy: 0.6358 - loss: 0.6337 - val_accuracy: 0.6017 - val_loss: 0.6613\n","Epoch 95/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 0.6369 - loss: 0.6353 - val_accuracy: 0.5999 - val_loss: 0.6662\n","Epoch 96/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.6380 - loss: 0.6335 - val_accuracy: 0.6038 - val_loss: 0.6644\n","Epoch 97/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6380 - loss: 0.6333 - val_accuracy: 0.6014 - val_loss: 0.6619\n","Epoch 98/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6465 - loss: 0.6282 - val_accuracy: 0.6035 - val_loss: 0.6646\n","Epoch 99/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.6429 - loss: 0.6306 - val_accuracy: 0.6022 - val_loss: 0.6633\n","Epoch 100/100\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 0.6428 - loss: 0.6314 - val_accuracy: 0.6043 - val_loss: 0.6616\n","\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6106 - loss: 0.6591\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["🔥 Test Accuracy: 60.43%\n"]}],"source":["!pip install tensorflow numpy pandas scikit-learn imbalanced-learn\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Flatten, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils.class_weight import compute_class_weight\n","from imblearn.over_sampling import SMOTE\n","\n","# ✅ Load dataset\n","df = pd.read_csv(\"/content/stroke_prediction_dataset_58000_real_fake.csv\")\n","\n","# ✅ Separate features (X) and labels (y)\n","X = df.iloc[:, :-1].values  # Sensor data (features)\n","y = df.iloc[:, -1].values   # Stroke classification (1: Real, -1: Fake, 0: No Stroke)\n","\n","# ✅ Convert labels (-1, 0, 1) to (0, 1, 2) for categorical cross-entropy\n","y = np.where(y == -1, 0, y)  # -1 → 0 (Fake), 0 → 1 (No Stroke), 1 → 2 (Real Stroke)\n","\n","# ✅ Normalize sensor data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Handle class imbalance using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# ✅ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# ✅ Reshape input for CNN-LSTM\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # Reshape to (samples, timesteps, features)\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n","\n","# ✅ Define CNN-LSTM Model\n","model = Sequential()\n","\n","# 🎯 CNN Layers (Feature Extraction)\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","\n","# 🎯 LSTM Layers (Time-Series Analysis)\n","model.add(LSTM(units=64, return_sequences=True))  # Keep return_sequences=True for multiple LSTMs\n","model.add(Dropout(0.3))\n","model.add(LSTM(units=32, return_sequences=False))  # Last LSTM layer should have return_sequences=False\n","model.add(Dropout(0.3))\n","\n","# 🎯 Fully Connected Layers\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(3, activation='softmax'))  # 3 Classes: Fake Stroke (0), No Stroke (1), Real Stroke (2)\n","\n","# ✅ Compile Model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# ✅ Compute class weights for imbalanced data\n","class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n","class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n","\n","# ✅ Train model\n","history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test),\n","                    class_weight=class_weight_dict, verbose=1)\n","\n","# ✅ Evaluate Model\n","test_loss, test_acc = model.evaluate(X_test, y_test)\n","print(f\"🔥 Test Accuracy: {test_acc * 100:.2f}%\")\n","\n","# ✅ Save the Model\n","model.save(\"stroke_detection_model.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":559310,"status":"ok","timestamp":1741979460979,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"},"user_tz":-330},"id":"UKMt1a7P0SUr","outputId":"2fcb0c31-5441-4a28-ec36-13c23146ba11"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 54ms/step - accuracy: 0.4958 - loss: 0.1028 - val_accuracy: 0.4965 - val_loss: 0.0876\n","Epoch 2/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 54ms/step - accuracy: 0.5069 - loss: 0.0876 - val_accuracy: 0.4992 - val_loss: 0.0868\n","Epoch 3/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.5110 - loss: 0.0870 - val_accuracy: 0.5060 - val_loss: 0.0867\n","Epoch 4/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.5187 - loss: 0.0867 - val_accuracy: 0.4955 - val_loss: 0.0872\n","Epoch 5/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 54ms/step - accuracy: 0.5196 - loss: 0.0866 - val_accuracy: 0.5010 - val_loss: 0.0868\n","Epoch 6/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.5196 - loss: 0.0866 - val_accuracy: 0.5150 - val_loss: 0.0865\n","Epoch 7/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.5243 - loss: 0.0865 - val_accuracy: 0.5126 - val_loss: 0.0866\n","Epoch 8/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 55ms/step - accuracy: 0.5312 - loss: 0.0864 - val_accuracy: 0.5260 - val_loss: 0.0864\n","Epoch 9/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.5294 - loss: 0.0864 - val_accuracy: 0.5328 - val_loss: 0.0863\n","Epoch 10/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.5295 - loss: 0.0863 - val_accuracy: 0.5131 - val_loss: 0.0866\n","Epoch 11/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 53ms/step - accuracy: 0.5328 - loss: 0.0863 - val_accuracy: 0.5191 - val_loss: 0.0865\n","Epoch 12/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 55ms/step - accuracy: 0.5368 - loss: 0.0862 - val_accuracy: 0.5272 - val_loss: 0.0863\n","Epoch 13/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 54ms/step - accuracy: 0.5359 - loss: 0.0862 - val_accuracy: 0.5217 - val_loss: 0.0864\n","Epoch 14/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 54ms/step - accuracy: 0.5395 - loss: 0.0862 - val_accuracy: 0.5291 - val_loss: 0.0863\n","Epoch 15/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.5398 - loss: 0.0860 - val_accuracy: 0.5366 - val_loss: 0.0862\n","Epoch 16/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 54ms/step - accuracy: 0.5425 - loss: 0.0860 - val_accuracy: 0.5300 - val_loss: 0.0864\n","Epoch 17/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 53ms/step - accuracy: 0.5451 - loss: 0.0859 - val_accuracy: 0.5351 - val_loss: 0.0863\n","Epoch 18/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 53ms/step - accuracy: 0.5507 - loss: 0.0858 - val_accuracy: 0.5442 - val_loss: 0.0860\n","Epoch 19/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 56ms/step - accuracy: 0.5551 - loss: 0.0856 - val_accuracy: 0.5418 - val_loss: 0.0860\n","Epoch 20/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 55ms/step - accuracy: 0.5510 - loss: 0.0858 - val_accuracy: 0.5373 - val_loss: 0.0862\n","Epoch 21/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 54ms/step - accuracy: 0.5549 - loss: 0.0856 - val_accuracy: 0.5309 - val_loss: 0.0864\n","Epoch 22/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 56ms/step - accuracy: 0.5577 - loss: 0.0856 - val_accuracy: 0.5417 - val_loss: 0.0860\n","Epoch 23/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 53ms/step - accuracy: 0.5607 - loss: 0.0854 - val_accuracy: 0.5396 - val_loss: 0.0861\n","Epoch 24/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 55ms/step - accuracy: 0.5599 - loss: 0.0854 - val_accuracy: 0.5439 - val_loss: 0.0861\n","Epoch 25/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.5618 - loss: 0.0854 - val_accuracy: 0.5429 - val_loss: 0.0860\n","Epoch 26/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 54ms/step - accuracy: 0.5665 - loss: 0.0852 - val_accuracy: 0.5481 - val_loss: 0.0858\n","Epoch 27/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.5680 - loss: 0.0851 - val_accuracy: 0.5569 - val_loss: 0.0856\n","Epoch 28/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.5658 - loss: 0.0850 - val_accuracy: 0.5519 - val_loss: 0.0858\n","Epoch 29/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 53ms/step - accuracy: 0.5721 - loss: 0.0850 - val_accuracy: 0.5515 - val_loss: 0.0857\n","Epoch 30/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.5712 - loss: 0.0850 - val_accuracy: 0.5526 - val_loss: 0.0856\n","Epoch 31/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 55ms/step - accuracy: 0.5724 - loss: 0.0848 - val_accuracy: 0.5440 - val_loss: 0.0859\n","Epoch 32/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.5748 - loss: 0.0847 - val_accuracy: 0.5528 - val_loss: 0.0856\n","Epoch 33/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.5743 - loss: 0.0847 - val_accuracy: 0.5530 - val_loss: 0.0857\n","Epoch 34/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 54ms/step - accuracy: 0.5810 - loss: 0.0844 - val_accuracy: 0.5539 - val_loss: 0.0855\n","Epoch 35/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 53ms/step - accuracy: 0.5729 - loss: 0.0847 - val_accuracy: 0.5631 - val_loss: 0.0853\n","Epoch 36/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 54ms/step - accuracy: 0.5788 - loss: 0.0843 - val_accuracy: 0.5595 - val_loss: 0.0856\n","Epoch 37/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 54ms/step - accuracy: 0.5823 - loss: 0.0842 - val_accuracy: 0.5618 - val_loss: 0.0853\n","Epoch 38/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 55ms/step - accuracy: 0.5813 - loss: 0.0842 - val_accuracy: 0.5596 - val_loss: 0.0852\n","Epoch 39/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.5870 - loss: 0.0839 - val_accuracy: 0.5549 - val_loss: 0.0856\n","Epoch 40/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 54ms/step - accuracy: 0.5873 - loss: 0.0839 - val_accuracy: 0.5605 - val_loss: 0.0853\n","Epoch 41/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 52ms/step - accuracy: 0.5892 - loss: 0.0837 - val_accuracy: 0.5642 - val_loss: 0.0853\n","Epoch 42/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 55ms/step - accuracy: 0.5895 - loss: 0.0837 - val_accuracy: 0.5653 - val_loss: 0.0852\n","Epoch 43/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.5935 - loss: 0.0834 - val_accuracy: 0.5657 - val_loss: 0.0849\n","Epoch 44/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 55ms/step - accuracy: 0.5921 - loss: 0.0835 - val_accuracy: 0.5697 - val_loss: 0.0850\n","Epoch 45/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 53ms/step - accuracy: 0.5938 - loss: 0.0834 - val_accuracy: 0.5703 - val_loss: 0.0849\n","Epoch 46/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 54ms/step - accuracy: 0.5967 - loss: 0.0830 - val_accuracy: 0.5704 - val_loss: 0.0849\n","Epoch 47/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 52ms/step - accuracy: 0.5992 - loss: 0.0831 - val_accuracy: 0.5726 - val_loss: 0.0846\n","Epoch 48/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 54ms/step - accuracy: 0.5986 - loss: 0.0830 - val_accuracy: 0.5755 - val_loss: 0.0847\n","Epoch 49/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 52ms/step - accuracy: 0.6001 - loss: 0.0828 - val_accuracy: 0.5728 - val_loss: 0.0849\n","Epoch 50/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 54ms/step - accuracy: 0.6011 - loss: 0.0828 - val_accuracy: 0.5685 - val_loss: 0.0851\n","Epoch 51/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 52ms/step - accuracy: 0.6052 - loss: 0.0825 - val_accuracy: 0.5729 - val_loss: 0.0846\n","Epoch 52/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 54ms/step - accuracy: 0.6087 - loss: 0.0821 - val_accuracy: 0.5747 - val_loss: 0.0846\n","Epoch 53/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 54ms/step - accuracy: 0.6041 - loss: 0.0824 - val_accuracy: 0.5784 - val_loss: 0.0850\n","Epoch 54/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 54ms/step - accuracy: 0.6073 - loss: 0.0823 - val_accuracy: 0.5788 - val_loss: 0.0844\n","Epoch 55/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 53ms/step - accuracy: 0.6147 - loss: 0.0818 - val_accuracy: 0.5883 - val_loss: 0.0840\n","Epoch 56/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 55ms/step - accuracy: 0.6137 - loss: 0.0818 - val_accuracy: 0.5846 - val_loss: 0.0844\n","Epoch 57/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.6154 - loss: 0.0817 - val_accuracy: 0.5857 - val_loss: 0.0844\n","Epoch 58/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 53ms/step - accuracy: 0.6151 - loss: 0.0817 - val_accuracy: 0.5894 - val_loss: 0.0840\n","Epoch 59/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 53ms/step - accuracy: 0.6170 - loss: 0.0814 - val_accuracy: 0.5893 - val_loss: 0.0839\n","Epoch 60/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 53ms/step - accuracy: 0.6206 - loss: 0.0812 - val_accuracy: 0.5865 - val_loss: 0.0842\n","Epoch 61/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 54ms/step - accuracy: 0.6219 - loss: 0.0810 - val_accuracy: 0.5909 - val_loss: 0.0842\n","Epoch 62/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.6258 - loss: 0.0806 - val_accuracy: 0.5914 - val_loss: 0.0838\n","Epoch 63/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 56ms/step - accuracy: 0.6241 - loss: 0.0808 - val_accuracy: 0.5919 - val_loss: 0.0842\n","Epoch 64/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 54ms/step - accuracy: 0.6274 - loss: 0.0809 - val_accuracy: 0.6001 - val_loss: 0.0838\n","Epoch 65/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 53ms/step - accuracy: 0.6298 - loss: 0.0804 - val_accuracy: 0.5951 - val_loss: 0.0840\n","Epoch 66/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 52ms/step - accuracy: 0.6290 - loss: 0.0805 - val_accuracy: 0.5969 - val_loss: 0.0834\n","Epoch 67/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 53ms/step - accuracy: 0.6319 - loss: 0.0802 - val_accuracy: 0.5966 - val_loss: 0.0839\n","Epoch 68/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 54ms/step - accuracy: 0.6298 - loss: 0.0802 - val_accuracy: 0.6027 - val_loss: 0.0835\n","Epoch 69/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 54ms/step - accuracy: 0.6325 - loss: 0.0802 - val_accuracy: 0.6008 - val_loss: 0.0834\n","Epoch 70/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 54ms/step - accuracy: 0.6345 - loss: 0.0798 - val_accuracy: 0.6030 - val_loss: 0.0831\n","Epoch 71/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 52ms/step - accuracy: 0.6364 - loss: 0.0796 - val_accuracy: 0.6027 - val_loss: 0.0834\n","Epoch 72/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 52ms/step - accuracy: 0.6368 - loss: 0.0795 - val_accuracy: 0.5996 - val_loss: 0.0836\n","Epoch 73/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.6392 - loss: 0.0795 - val_accuracy: 0.6043 - val_loss: 0.0834\n","Epoch 74/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 53ms/step - accuracy: 0.6411 - loss: 0.0791 - val_accuracy: 0.6033 - val_loss: 0.0832\n","Epoch 75/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 54ms/step - accuracy: 0.6425 - loss: 0.0790 - val_accuracy: 0.6000 - val_loss: 0.0836\n","Epoch 76/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 52ms/step - accuracy: 0.6452 - loss: 0.0789 - val_accuracy: 0.6084 - val_loss: 0.0833\n","Epoch 77/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 52ms/step - accuracy: 0.6480 - loss: 0.0786 - val_accuracy: 0.6061 - val_loss: 0.0832\n","Epoch 78/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 52ms/step - accuracy: 0.6462 - loss: 0.0786 - val_accuracy: 0.6138 - val_loss: 0.0829\n","Epoch 79/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 54ms/step - accuracy: 0.6481 - loss: 0.0784 - val_accuracy: 0.6066 - val_loss: 0.0837\n","Epoch 80/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 52ms/step - accuracy: 0.6524 - loss: 0.0782 - val_accuracy: 0.6069 - val_loss: 0.0833\n","Epoch 81/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 52ms/step - accuracy: 0.6493 - loss: 0.0780 - val_accuracy: 0.6108 - val_loss: 0.0829\n","Epoch 82/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 54ms/step - accuracy: 0.6520 - loss: 0.0777 - val_accuracy: 0.6046 - val_loss: 0.0831\n","Epoch 83/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.6581 - loss: 0.0773 - val_accuracy: 0.6137 - val_loss: 0.0830\n","Epoch 84/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 52ms/step - accuracy: 0.6581 - loss: 0.0774 - val_accuracy: 0.6071 - val_loss: 0.0832\n","Epoch 85/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 54ms/step - accuracy: 0.6575 - loss: 0.0776 - val_accuracy: 0.6137 - val_loss: 0.0829\n","Epoch 86/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 52ms/step - accuracy: 0.6569 - loss: 0.0772 - val_accuracy: 0.6184 - val_loss: 0.0828\n","Epoch 87/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 54ms/step - accuracy: 0.6601 - loss: 0.0773 - val_accuracy: 0.6143 - val_loss: 0.0828\n","Epoch 88/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 53ms/step - accuracy: 0.6604 - loss: 0.0770 - val_accuracy: 0.6197 - val_loss: 0.0826\n","Epoch 89/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 52ms/step - accuracy: 0.6678 - loss: 0.0765 - val_accuracy: 0.6199 - val_loss: 0.0829\n","Epoch 90/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 54ms/step - accuracy: 0.6668 - loss: 0.0765 - val_accuracy: 0.6198 - val_loss: 0.0828\n","Epoch 91/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 53ms/step - accuracy: 0.6655 - loss: 0.0766 - val_accuracy: 0.6223 - val_loss: 0.0829\n","Epoch 92/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.6724 - loss: 0.0758 - val_accuracy: 0.6233 - val_loss: 0.0826\n","Epoch 93/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 52ms/step - accuracy: 0.6705 - loss: 0.0759 - val_accuracy: 0.6245 - val_loss: 0.0823\n","Epoch 94/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.6727 - loss: 0.0754 - val_accuracy: 0.6241 - val_loss: 0.0833\n","Epoch 95/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 54ms/step - accuracy: 0.6725 - loss: 0.0756 - val_accuracy: 0.6270 - val_loss: 0.0820\n","Epoch 96/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 54ms/step - accuracy: 0.6751 - loss: 0.0751 - val_accuracy: 0.6252 - val_loss: 0.0823\n","Epoch 97/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 54ms/step - accuracy: 0.6746 - loss: 0.0752 - val_accuracy: 0.6232 - val_loss: 0.0825\n","Epoch 98/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 53ms/step - accuracy: 0.6782 - loss: 0.0750 - val_accuracy: 0.6252 - val_loss: 0.0822\n","Epoch 99/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 53ms/step - accuracy: 0.6763 - loss: 0.0753 - val_accuracy: 0.6255 - val_loss: 0.0823\n","Epoch 100/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 55ms/step - accuracy: 0.6806 - loss: 0.0747 - val_accuracy: 0.6273 - val_loss: 0.0822\n","Epoch 101/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 52ms/step - accuracy: 0.6809 - loss: 0.0748 - val_accuracy: 0.6303 - val_loss: 0.0819\n","Epoch 102/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.6823 - loss: 0.0745 - val_accuracy: 0.6282 - val_loss: 0.0827\n","Epoch 103/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 52ms/step - accuracy: 0.6808 - loss: 0.0745 - val_accuracy: 0.6358 - val_loss: 0.0818\n","Epoch 104/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.6856 - loss: 0.0741 - val_accuracy: 0.6341 - val_loss: 0.0813\n","Epoch 105/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 55ms/step - accuracy: 0.6855 - loss: 0.0740 - val_accuracy: 0.6293 - val_loss: 0.0831\n","Epoch 106/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 52ms/step - accuracy: 0.6860 - loss: 0.0737 - val_accuracy: 0.6330 - val_loss: 0.0822\n","Epoch 107/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 55ms/step - accuracy: 0.6881 - loss: 0.0739 - val_accuracy: 0.6376 - val_loss: 0.0819\n","Epoch 108/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.6894 - loss: 0.0736 - val_accuracy: 0.6383 - val_loss: 0.0817\n","Epoch 109/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 54ms/step - accuracy: 0.6884 - loss: 0.0737 - val_accuracy: 0.6361 - val_loss: 0.0823\n","Epoch 110/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 53ms/step - accuracy: 0.6907 - loss: 0.0734 - val_accuracy: 0.6323 - val_loss: 0.0832\n","Epoch 111/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 53ms/step - accuracy: 0.6878 - loss: 0.0733 - val_accuracy: 0.6344 - val_loss: 0.0817\n","Epoch 112/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 54ms/step - accuracy: 0.6911 - loss: 0.0733 - val_accuracy: 0.6404 - val_loss: 0.0821\n","Epoch 113/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.6958 - loss: 0.0726 - val_accuracy: 0.6379 - val_loss: 0.0821\n","Epoch 114/200\n","\u001b[1m1012/1012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.6967 - loss: 0.0727 - val_accuracy: 0.6408 - val_loss: 0.0818\n","\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.6370 - loss: 0.0813\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["🔥 Test Accuracy: 63.41%\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils.class_weight import compute_class_weight\n","from imblearn.over_sampling import SMOTE\n","\n","# ✅ Load dataset\n","df = pd.read_csv(\"/content/stroke_prediction_dataset_58000_real_fake.csv\")\n","\n","# ✅ Separate features (X) and labels (y)\n","X = df.iloc[:, :-1].values  # Sensor data (features)\n","y = df.iloc[:, -1].values   # Stroke classification (1: Real, -1: Fake, 0: No Stroke)\n","\n","# ✅ Convert labels (-1, 0, 1) to (0, 1, 2) for categorical classification\n","y = np.where(y == -1, 0, y)  # -1 → 0 (Fake), 0 → 1 (No Stroke), 1 → 2 (Real Stroke)\n","\n","# ✅ Normalize sensor data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Handle class imbalance using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# ✅ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# ✅ Reshape input for CNN-LSTM\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n","\n","# ✅ Define High-Accuracy CNN-BiLSTM Model\n","model = Sequential()\n","\n","# 🎯 CNN Layers (Feature Extraction)\n","model.add(Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","\n","# 🎯 BiLSTM Layers (Time-Series Analysis)\n","model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n","model.add(Dropout(0.3))\n","model.add(Bidirectional(LSTM(units=64, return_sequences=False)))\n","model.add(Dropout(0.3))\n","\n","# 🎯 Fully Connected Layers\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(3, activation='softmax'))  # 3 Classes: Fake Stroke (0), No Stroke (1), Real Stroke (2)\n","\n","# ✅ Compile Model with Focal Loss (Better for Class Imbalance)\n","def focal_loss(alpha=0.25, gamma=2.0):\n","    def loss(y_true, y_pred):\n","        y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=3)\n","        epsilon = tf.keras.backend.epsilon()\n","        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n","        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n","        loss = -alpha * (1 - pt) ** gamma * tf.math.log(pt)\n","        return tf.reduce_sum(loss, axis=1)\n","    return loss\n","\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n","              loss=focal_loss(),\n","              metrics=['accuracy'])\n","\n","# ✅ Compute class weights for imbalanced data\n","class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n","class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n","\n","# ✅ Train model with Early Stopping\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_test, y_test),\n","                    class_weight=class_weight_dict, callbacks=[early_stopping], verbose=1)\n","\n","# ✅ Evaluate Model\n","test_loss, test_acc = model.evaluate(X_test, y_test)\n","print(f\"🔥 Test Accuracy: {test_acc * 100:.2f}%\")\n","\n","# ✅ Save the Model\n","model.save(\"stroke_detection_high_accuracy.h5\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"executionInfo":{"elapsed":257531,"status":"error","timestamp":1741983517095,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"},"user_tz":-330},"id":"QqwRfyM0SkHo","outputId":"43d75037-2390-472f-aa45-90d7fc313efc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","\u001b[1m1081/1081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 42ms/step - accuracy: 0.3350 - loss: -0.0651 - val_accuracy: 0.3312 - val_loss: -0.1160 - learning_rate: 0.0010\n","Epoch 2/50\n","\u001b[1m1081/1081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.3329 - loss: -0.6168 - val_accuracy: 0.3312 - val_loss: -8.3586 - learning_rate: 0.0010\n","Epoch 3/50\n","\u001b[1m1081/1081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 52ms/step - accuracy: 0.3355 - loss: -23.7684 - val_accuracy: 0.3312 - val_loss: -110.1920 - learning_rate: 0.0010\n","Epoch 4/50\n","\u001b[1m 757/1081\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.3324 - loss: -346.7178"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e6f3c508f41e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# ✅ Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","# ✅ Load dataset\n","file_path = \"/content/stroke_prediction_dataset_58000_real_fake.csv\"\n","df = pd.read_csv(file_path)\n","\n","# ✅ Split Features & Labels\n","X = df.drop(columns=[\"stroke_label\"]).values\n","y = df[\"stroke_label\"].values  # 0 = No Stroke, 1 = Stroke\n","\n","# ✅ Standardization\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Handle Class Imbalance using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# ✅ Reshape for LSTM (samples, time_steps, features)\n","X_resampled = X_resampled.reshape(X_resampled.shape[0], X_resampled.shape[1], 1)\n","\n","# ✅ Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# ✅ Build Model (BiLSTM + CNN)\n","model = Sequential()\n","\n","# 🔹 1D CNN for Feature Extraction\n","model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\", input_shape=(X_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(BatchNormalization())\n","\n","# 🔹 BiLSTM for Temporal Feature Learning\n","model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n","model.add(Dropout(0.4))\n","model.add(Bidirectional(LSTM(units=64, return_sequences=False)))\n","model.add(Dropout(0.3))\n","\n","# 🔹 Dense Layers for Classification\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dropout(0.3))\n","model.add(Dense(32, activation=\"relu\"))\n","model.add(Dense(1, activation=\"sigmoid\"))  # Output Layer\n","\n","# ✅ Compile Model\n","model.compile(optimizer=Adam(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","\n","# ✅ Callbacks (Early Stopping & Learning Rate Reduction)\n","callbacks = [\n","    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n","    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n","]\n","\n","# ✅ Train Model\n","history = model.fit(\n","    X_train, y_train,\n","    validation_data=(X_test, y_test),\n","    epochs=50,\n","    batch_size=64,\n","    callbacks=callbacks,\n","    verbose=1\n",")\n","\n","# ✅ Evaluate Model\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n","\n","# ✅ Save Model\n","model.save(\"/mnt/data/stroke_detection_model.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZTgXYo7UJbh"},"outputs":[],"source":["!pip install pandas numpy tensorflow imbalanced-learn scikit-learn\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import (\n","    Input, Dense, Dropout, BatchNormalization, Conv1D, LSTM, Bidirectional, Flatten,\n","    GlobalAveragePooling1D, Attention\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Load reduced dataset\n","file_path = \"\"\n","df = pd.read_csv(file_path)\n","\n","# 🔹 Drop irrelevant columns if any\n","df.dropna(inplace=True)  # Remove missing values\n","df.drop_duplicates(inplace=True)  # Remove duplicate rows\n","\n","# 🔹 Split features and labels\n","X = df.drop(columns=[\"stroke\"]).values  # Features\n","y = df[\"stroke\"].values  # Labels (0 = No Stroke, 1 = Real Stroke, 2 = Fake Stroke)\n","\n","# 🔹 Standardize features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Handle class imbalance using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# ✅ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",")\n","# 🔹 Define Input Shape\n","input_layer = Input(shape=(X_train.shape[1], 1))\n","\n","# 🔹 CNN Layers (Feature Extraction)\n","x = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(input_layer)\n","x = BatchNormalization()(x)\n","x = Dropout(0.3)(x)\n","\n","# 🔹 BiLSTM Layers (Bidirectional for Better Feature Extraction)\n","x = Bidirectional(LSTM(units=128, return_sequences=True))(x)\n","x = Dropout(0.4)(x)\n","\n","# 🔹 Attention Mechanism (Focus on Important Features)\n","attention = Attention()([x, x])\n","x = GlobalAveragePooling1D()(attention)\n","\n","# 🔹 Fully Connected Layers\n","x = Dense(128, activation=\"relu\")(x)\n","x = Dropout(0.3)(x)\n","x = Dense(64, activation=\"relu\")(x)\n","x = Dropout(0.2)(x)\n","\n","# 🔹 Output Layer (3 Classes: No Stroke, Real Stroke, Fake Stroke)\n","output_layer = Dense(3, activation=\"softmax\")(x)\n","\n","# ✅ Compile Model\n","model = Model(inputs=input_layer, outputs=output_layer)\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","              loss=\"sparse_categorical_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","# ✅ Model Summary\n","model.summary()\n","# Train the model\n","history = model.fit(\n","    X_train, y_train,\n","    validation_data=(X_test, y_test),\n","    epochs=100,  # Can increase if needed\n","    batch_size=64,\n","    verbose=1\n",")\n","# Predict on test data\n","y_pred = model.predict(X_test)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","# Print accuracy and classification report\n","accuracy = accuracy_score(y_test, y_pred_classes)\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_classes))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"OO_ona1YzeDS","executionInfo":{"status":"error","timestamp":1742031117508,"user_tz":-330,"elapsed":86676,"user":{"displayName":"Vikram Balaji","userId":"13340630066862876081"}},"outputId":"4822a583-e392-4154-a6cb-67956011841c"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Windowed data shape: (79, 500, 10) (79,)\n","Epoch 1/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 25s/step - accuracy: 0.3830 - loss: 1.1158 - val_accuracy: 0.9375 - val_loss: 0.9829 - learning_rate: 5.0000e-04\n","Epoch 2/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.7021 - loss: 0.8596 - val_accuracy: 0.9375 - val_loss: 0.8724 - learning_rate: 5.0000e-04\n","Epoch 3/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.8511 - loss: 0.9164 - val_accuracy: 0.9375 - val_loss: 0.8084 - learning_rate: 5.0000e-04\n","Epoch 4/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - accuracy: 0.7234 - loss: 1.0264 - val_accuracy: 0.9375 - val_loss: 0.7891 - learning_rate: 5.0000e-04\n","Epoch 5/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - accuracy: 0.7021 - loss: 0.8210 - val_accuracy: 0.9375 - val_loss: 0.7698 - learning_rate: 5.0000e-04\n","Epoch 6/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.6383 - loss: 0.9081 - val_accuracy: 0.9375 - val_loss: 0.7688 - learning_rate: 5.0000e-04\n","Epoch 7/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.5319 - loss: 0.9625 - val_accuracy: 0.9375 - val_loss: 0.7797 - learning_rate: 5.0000e-04\n","Epoch 8/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.5745 - loss: 0.8380 - val_accuracy: 0.0625 - val_loss: 0.8017 - learning_rate: 5.0000e-04\n","Epoch 9/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.4681 - loss: 0.7411 - val_accuracy: 0.0625 - val_loss: 0.8152 - learning_rate: 5.0000e-04\n","Epoch 10/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.4468 - loss: 0.7571 - val_accuracy: 0.0625 - val_loss: 0.8252 - learning_rate: 5.0000e-04\n","Epoch 11/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.4894 - loss: 0.6667 - val_accuracy: 0.0625 - val_loss: 0.8269 - learning_rate: 5.0000e-04\n","Epoch 12/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.4894 - loss: 0.8682 - val_accuracy: 0.0625 - val_loss: 0.8241 - learning_rate: 2.5000e-04\n","Epoch 13/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - accuracy: 0.4894 - loss: 0.8756 - val_accuracy: 0.0625 - val_loss: 0.8212 - learning_rate: 2.5000e-04\n","Epoch 14/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.4681 - loss: 0.6735 - val_accuracy: 0.0625 - val_loss: 0.8137 - learning_rate: 2.5000e-04\n","Epoch 15/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - accuracy: 0.5957 - loss: 1.2762 - val_accuracy: 0.0625 - val_loss: 0.8177 - learning_rate: 2.5000e-04\n","Epoch 16/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - accuracy: 0.4894 - loss: 0.9673 - val_accuracy: 0.0625 - val_loss: 0.8213 - learning_rate: 2.5000e-04\n","Epoch 17/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - accuracy: 0.4255 - loss: 0.9184 - val_accuracy: 0.0625 - val_loss: 0.8258 - learning_rate: 1.2500e-04\n","Epoch 18/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.4894 - loss: 0.7751 - val_accuracy: 0.0625 - val_loss: 0.8311 - learning_rate: 1.2500e-04\n","Epoch 19/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.4468 - loss: 0.7381 - val_accuracy: 0.0625 - val_loss: 0.8357 - learning_rate: 1.2500e-04\n","Epoch 20/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.4468 - loss: 0.6556 - val_accuracy: 0.0625 - val_loss: 0.8372 - learning_rate: 1.2500e-04\n","Epoch 21/100\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.4681 - loss: 0.8225 - val_accuracy: 0.0625 - val_loss: 0.8381 - learning_rate: 1.2500e-04\n","Test Accuracy on Simulated Data: 0.8750\n","Epoch 1/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 42s/step - loss: 1.0006 - val_loss: 1.0006\n","Epoch 2/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0005 - val_loss: 1.0005\n","Epoch 3/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 1.0003 - val_loss: 1.0005\n","Epoch 4/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0002 - val_loss: 1.0005\n","Epoch 5/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 6/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 7/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 8/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 9/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 10/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 11/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 12/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 13/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 14/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 15/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 16/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 1.0001 - val_loss: 1.0005\n","Epoch 17/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0000 - val_loss: 1.0005\n","Epoch 18/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 1.0000 - val_loss: 1.0005\n","Epoch 19/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0000 - val_loss: 1.0005\n","Epoch 20/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.0000 - val_loss: 1.0005\n","\n","=== Real-Time Stroke Detection ===\n","Enter sensor data (accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, heart_rate, spo2, emg_signal, ecg_signal)\n","Requires 500 samples at 100 Hz (5 seconds). Enter one sample per line, or 'done' to finish early.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b66589866c9b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;31m# Step 6: Test with User Input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mprocess_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0mcont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nContinue testing? (yes/no): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'yes'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-b66589866c9b>\u001b[0m in \u001b[0;36mprocess_user_input\u001b[0;34m(scaler, model, autoencoder, window_size)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sample {i+1}/{window_size}: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'done'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers, Model\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils import class_weight\n","import matplotlib.pyplot as plt\n","import time\n","\n","# Step 1: Simulate 100 Rows for Training\n","np.random.seed(42)\n","n_rows = 100\n","data = {\n","    'accel_x': np.random.uniform(-2, 2, n_rows),\n","    'accel_y': np.random.uniform(-2, 2, n_rows),\n","    'accel_z': np.random.uniform(-2, 2, n_rows),\n","    'gyro_x': np.random.uniform(-250, 250, n_rows),\n","    'gyro_y': np.random.uniform(-250, 250, n_rows),\n","    'gyro_z': np.random.uniform(-250, 250, n_rows),\n","    'heart_rate': np.random.uniform(60, 100, n_rows),\n","    'spo2': np.random.uniform(90, 100, n_rows),\n","    'emg_signal': np.random.uniform(0.5, 3, n_rows),\n","    'ecg_signal': np.random.uniform(-1, 1, n_rows),\n","    'stroke_label': np.concatenate([np.zeros(33), np.ones(33), -np.ones(34)])\n","}\n","df_sample = pd.DataFrame(data)\n","\n","for i in range(n_rows):\n","    label = df_sample.loc[i, 'stroke_label']\n","    if label == 1:  # True Stroke\n","        df_sample.loc[i, 'accel_x'] += np.random.uniform(1, 3)\n","        df_sample.loc[i, 'gyro_x'] += np.random.uniform(100, 200)\n","        df_sample.loc[i, 'heart_rate'] = np.random.uniform(100, 140)\n","        df_sample.loc[i, 'spo2'] = np.random.uniform(85, 92)\n","        df_sample.loc[i, 'emg_signal'] = np.random.uniform(3, 5)\n","        df_sample.loc[i, 'ecg_signal'] = np.random.uniform(-2, 2)\n","    elif label == -1:  # Fake Stroke\n","        df_sample.loc[i, 'accel_x'] += np.random.uniform(-0.5, 0.5)\n","        df_sample.loc[i, 'gyro_x'] += np.random.uniform(-50, 50)\n","        df_sample.loc[i, 'heart_rate'] = np.random.uniform(60, 120)\n","        df_sample.loc[i, 'spo2'] = np.random.uniform(88, 98)\n","        df_sample.loc[i, 'emg_signal'] = np.random.uniform(0.2, 1.5)\n","        df_sample.loc[i, 'ecg_signal'] = np.random.uniform(-0.5, 0.5)\n","\n","df_sample = df_sample.sample(frac=1).reset_index(drop=True)\n","df_sample['timestamp'] = np.arange(0, n_rows / 100, 1 / 100)\n","\n","# Scale to 20,000 samples\n","n_samples = 20000\n","repeat_factor = n_samples // len(df_sample) + 1\n","df = pd.concat([df_sample] * repeat_factor, ignore_index=True).iloc[:n_samples]\n","\n","for col in df.columns[:-1]:\n","    df[col] += np.random.normal(0, 0.01, n_samples)\n","\n","labels = np.array([0, 1, -1])\n","target_counts = n_samples // 3\n","for label in labels:\n","    current_count = (df['stroke_label'] == label).sum()\n","    if current_count < target_counts:\n","        to_add = target_counts - current_count\n","        indices = df[df['stroke_label'] != label].sample(to_add, replace=True).index\n","        df.loc[indices, 'stroke_label'] = label\n","\n","df['timestamp'] = np.arange(0, n_samples / 100, 1 / 100)\n","\n","# Step 2: Preprocessing\n","window_size = 500\n","step_size = 250\n","\n","scaler = StandardScaler()\n","sensor_cols = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z',\n","               'heart_rate', 'spo2', 'emg_signal', 'ecg_signal']\n","df[sensor_cols] = scaler.fit_transform(df[sensor_cols])\n","\n","def create_windows(df, window_size, step_size):\n","    X, y = [], []\n","    for i in range(0, len(df) - window_size + 1, step_size):\n","        window = df.iloc[i:i + window_size]\n","        X.append(window[sensor_cols].values)\n","        y.append(np.bincount(window['stroke_label'].astype(int) + 1).argmax() - 1)\n","    return np.array(X), np.array(y)\n","\n","X, y = create_windows(df, window_size, step_size)\n","print(\"Windowed data shape:\", X.shape, y.shape)\n","\n","y = tf.keras.utils.to_categorical(y + 1, num_classes=3)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n","\n","def augment_data(X):\n","    noise = np.random.normal(0, 0.05, X.shape)\n","    return X + noise\n","\n","X_train_aug = augment_data(X_train)\n","\n","# Step 3: Model Architecture\n","def build_model(window_size, num_features):\n","    mpu_input = layers.Input(shape=(window_size, 6))\n","    max_input = layers.Input(shape=(window_size, 2))\n","    emg_input = layers.Input(shape=(window_size, 1))\n","    ecg_input = layers.Input(shape=(window_size, 1))\n","\n","    def cnn_branch(x, filters):\n","        x = layers.Conv1D(filters, 5, activation='relu', padding='same')(x)\n","        x = layers.BatchNormalization()(x)\n","        x = layers.Conv1D(filters, 5, activation='relu', padding='same')(x)\n","        x = layers.MaxPooling1D(2)(x)\n","        return x\n","\n","    mpu_features = cnn_branch(mpu_input, 128)\n","    max_features = cnn_branch(max_input, 64)\n","    emg_features = cnn_branch(emg_input, 64)\n","    ecg_features = cnn_branch(ecg_input, 64)\n","\n","    combined = layers.Concatenate()([mpu_features, max_features, emg_features, ecg_features])\n","    lstm_out = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(combined)\n","    attention = layers.Attention()([lstm_out, lstm_out])\n","    lstm_out = layers.LSTM(128)(attention)\n","    lstm_out = layers.Dropout(0.5)(lstm_out)\n","\n","    dense_out = layers.Dense(256, activation='relu')(lstm_out)\n","    dense_out = layers.Dropout(0.4)(dense_out)\n","    output = layers.Dense(3, activation='softmax')(dense_out)\n","\n","    model = Model(inputs=[mpu_input, max_input, emg_input, ecg_input], outputs=output)\n","    return model\n","\n","# Prepare training inputs\n","X_train_mpu = X_train[:, :, :6]\n","X_train_max = X_train[:, :, 6:8]\n","X_train_emg = X_train[:, :, 8:9]\n","X_train_ecg = X_train[:, :, 9:10]\n","\n","X_val_mpu = X_val[:, :, :6]\n","X_val_max = X_val[:, :, 6:8]\n","X_val_emg = X_val[:, :, 8:9]\n","X_val_ecg = X_val[:, :, 9:10]\n","\n","X_test_mpu = X_test[:, :, :6]\n","X_test_max = X_test[:, :, 6:8]\n","X_test_emg = X_test[:, :, 8:9]\n","X_test_ecg = X_test[:, :, 9:10]\n","\n","# Train classifier\n","model = build_model(window_size, 10)\n","optimizer = Adam(learning_rate=0.0005)\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(np.argmax(y_train, axis=1)),\n","                                                  y=np.argmax(y_train, axis=1))\n","class_weights_dict = dict(enumerate(class_weights))\n","\n","lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n","\n","history = model.fit(\n","    [X_train_mpu, X_train_max, X_train_emg, X_train_ecg], y_train,\n","    validation_data=([X_val_mpu, X_val_max, X_val_emg, X_val_ecg], y_val),\n","    epochs=100,\n","    batch_size=64,\n","    class_weight=class_weights_dict,\n","    callbacks=[lr_scheduler, early_stopping],\n","    verbose=1\n",")\n","\n","test_loss, test_accuracy = model.evaluate(\n","    [X_test_mpu, X_test_max, X_test_emg, X_test_ecg], y_test, verbose=0\n",")\n","print(f\"Test Accuracy on Simulated Data: {test_accuracy:.4f}\")\n","\n","# Step 4: Autoencoder for Anomaly Detection\n","def build_autoencoder(window_size, num_features):\n","    input_layer = layers.Input(shape=(window_size, num_features))\n","    x = layers.LSTM(128, return_sequences=True)(input_layer)\n","    x = layers.LSTM(64)(x)\n","    x = layers.RepeatVector(window_size)(x)\n","    x = layers.LSTM(64, return_sequences=True)(x)\n","    x = layers.LSTM(128, return_sequences=True)(x)\n","    output_layer = layers.TimeDistributed(layers.Dense(num_features))(x)\n","    return Model(input_layer, output_layer)\n","\n","X_train_normal = X_train[np.argmax(y_train, axis=1) == 0]\n","autoencoder = build_autoencoder(window_size, 10)\n","autoencoder.compile(optimizer='adam', loss='mse')\n","autoencoder.fit(X_train_normal, X_train_normal, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n","\n","# Step 5: Function to Process User Sensor Input\n","def process_user_input(scaler, model, autoencoder, window_size):\n","    print(\"\\n=== Real-Time Stroke Detection ===\")\n","    print(\"Enter sensor data (accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, heart_rate, spo2, emg_signal, ecg_signal)\")\n","    print(f\"Requires {window_size} samples at 100 Hz (5 seconds). Enter one sample per line, or 'done' to finish early.\")\n","\n","    sensor_data = []\n","    sensor_cols = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z',\n","                   'heart_rate', 'spo2', 'emg_signal', 'ecg_signal']\n","\n","    for i in range(window_size):\n","        user_input = input(f\"Sample {i+1}/{window_size}: \")\n","        if user_input.lower() == 'done':\n","            break\n","        try:\n","            values = [float(x) for x in user_input.split(',')]\n","            if len(values) != 10:\n","                print(\"Error: Enter exactly 10 values separated by commas.\")\n","                continue\n","            sensor_data.append(values)\n","        except ValueError:\n","            print(\"Error: Invalid input. Use numbers separated by commas (e.g., 1.2, -0.5, ...)\")\n","            continue\n","\n","    # Pad with zeros or truncate to window_size\n","    if len(sensor_data) < window_size:\n","        sensor_data.extend([[0] * 10 for _ in range(window_size - len(sensor_data))])\n","    sensor_data = sensor_data[:window_size]\n","\n","    # Convert to DataFrame and normalize\n","    df_user = pd.DataFrame(sensor_data, columns=sensor_cols)\n","    X_user = scaler.transform(df_user[sensor_cols])\n","    X_user = np.array([X_user])  # Shape: (1, window_size, 10)\n","\n","    # Split for multi-input model\n","    X_user_mpu = X_user[:, :, :6]\n","    X_user_max = X_user[:, :, 6:8]\n","    X_user_emg = X_user[:, :, 8:9]\n","    X_user_ecg = X_user[:, :, 9:10]\n","\n","    # Classifier prediction\n","    pred = model.predict([X_user_mpu, X_user_max, X_user_emg, X_user_ecg])\n","    class_pred = np.argmax(pred, axis=1)[0]\n","    class_labels = {0: \"Normal\", 1: \"True Stroke\", 2: \"Fake Stroke\"}\n","    print(f\"Classification: {class_labels[class_pred]} (Confidence: {pred[0][class_pred]:.4f})\")\n","\n","    # Autoencoder anomaly detection\n","    reconstruction = autoencoder.predict(X_user)\n","    mse = np.mean(np.square(X_user - reconstruction))\n","    threshold = np.percentile(autoencoder.predict(X_train_normal).mean(axis=(1, 2)), 95)  # Precomputed threshold\n","    if mse > threshold:\n","        if class_pred == 1:\n","            print(\"Pre-Stroke Warning: True Stroke Likely\")\n","        elif class_pred == 2:\n","            print(\"Pre-Stroke Warning: Possible Fake Stroke\")\n","        else:\n","            print(\"Pre-Stroke Warning: Anomaly Detected\")\n","    else:\n","        print(\"No Pre-Stroke Warning\")\n","\n","    return class_pred, mse\n","\n","# Step 6: Test with User Input\n","while True:\n","    process_user_input(scaler, model, autoencoder, window_size)\n","    cont = input(\"\\nContinue testing? (yes/no): \")\n","    if cont.lower() != 'yes':\n","        break\n","\n","print(\"Testing complete.\")"]},{"cell_type":"code","source":["!pip install numpy pandas scikit-learn tensorflow imbalanced-learn\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/fall_detection_dataset_10000.csv\")\n","\n","# Define features and target\n","X = df.drop(columns=[\"fall_detected\"]).values  # Features (sensor data)\n","y = df[\"fall_detected\"].values  # Target (1 = Real Fall, 0 = False Detection)\n","\n","# Normalize features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Handle imbalanced data using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# Reshape for LSTM input\n","X_resampled = X_resampled.reshape(X_resampled.shape[0], X_resampled.shape[1], 1)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","# Create LSTM-based model\n","model = Sequential()\n","\n","# Input Layer\n","model.add(Bidirectional(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], 1))))\n","model.add(Dropout(0.3))\n","\n","# Hidden Layers\n","model.add(Bidirectional(LSTM(64, return_sequences=True)))\n","model.add(Dropout(0.3))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dropout(0.2))\n","\n","# Output Layer\n","model.add(Dense(1, activation=\"sigmoid\"))\n","\n","# Compile Model\n","optimizer = Adam(learning_rate=0.0005)\n","model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YeqLTHCxJ4-I","executionInfo":{"status":"ok","timestamp":1742039774099,"user_tz":-330,"elapsed":943809,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"}},"outputId":"92212beb-6783-4e19-8982-16e71f832d1f"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 76ms/step - accuracy: 0.4996 - loss: 0.6936 - val_accuracy: 0.5106 - val_loss: 0.6930\n","Epoch 2/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 83ms/step - accuracy: 0.5064 - loss: 0.6935 - val_accuracy: 0.5052 - val_loss: 0.6917\n","Epoch 3/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 78ms/step - accuracy: 0.5242 - loss: 0.6912 - val_accuracy: 0.5361 - val_loss: 0.6896\n","Epoch 4/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 80ms/step - accuracy: 0.5367 - loss: 0.6895 - val_accuracy: 0.5239 - val_loss: 0.6902\n","Epoch 5/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 76ms/step - accuracy: 0.5374 - loss: 0.6887 - val_accuracy: 0.5332 - val_loss: 0.6907\n","Epoch 6/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.5516 - loss: 0.6865 - val_accuracy: 0.5389 - val_loss: 0.6886\n","Epoch 7/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 73ms/step - accuracy: 0.5486 - loss: 0.6871 - val_accuracy: 0.5274 - val_loss: 0.6894\n","Epoch 8/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 84ms/step - accuracy: 0.5483 - loss: 0.6856 - val_accuracy: 0.5303 - val_loss: 0.6885\n","Epoch 9/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 78ms/step - accuracy: 0.5528 - loss: 0.6845 - val_accuracy: 0.5296 - val_loss: 0.6887\n","Epoch 10/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.5557 - loss: 0.6841 - val_accuracy: 0.5325 - val_loss: 0.6891\n","Epoch 11/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.5520 - loss: 0.6844 - val_accuracy: 0.5188 - val_loss: 0.6921\n","Epoch 12/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 73ms/step - accuracy: 0.5589 - loss: 0.6829 - val_accuracy: 0.5346 - val_loss: 0.6889\n","Epoch 13/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 73ms/step - accuracy: 0.5528 - loss: 0.6852 - val_accuracy: 0.5364 - val_loss: 0.6893\n","Epoch 14/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 73ms/step - accuracy: 0.5642 - loss: 0.6823 - val_accuracy: 0.5314 - val_loss: 0.6893\n","Epoch 15/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 76ms/step - accuracy: 0.5598 - loss: 0.6844 - val_accuracy: 0.5343 - val_loss: 0.6898\n","Epoch 16/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 73ms/step - accuracy: 0.5591 - loss: 0.6840 - val_accuracy: 0.5328 - val_loss: 0.6968\n","Epoch 17/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 75ms/step - accuracy: 0.5675 - loss: 0.6813 - val_accuracy: 0.5357 - val_loss: 0.6887\n","Epoch 18/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 73ms/step - accuracy: 0.5582 - loss: 0.6823 - val_accuracy: 0.5386 - val_loss: 0.6906\n","Epoch 19/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 73ms/step - accuracy: 0.5618 - loss: 0.6824 - val_accuracy: 0.5379 - val_loss: 0.6902\n","Epoch 20/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 72ms/step - accuracy: 0.5564 - loss: 0.6834 - val_accuracy: 0.5300 - val_loss: 0.6908\n","Epoch 21/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 76ms/step - accuracy: 0.5642 - loss: 0.6813 - val_accuracy: 0.5375 - val_loss: 0.6895\n","Epoch 22/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - accuracy: 0.5658 - loss: 0.6791 - val_accuracy: 0.5364 - val_loss: 0.6892\n","Epoch 23/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - accuracy: 0.5687 - loss: 0.6787 - val_accuracy: 0.5386 - val_loss: 0.6921\n","Epoch 24/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 80ms/step - accuracy: 0.5659 - loss: 0.6794 - val_accuracy: 0.5346 - val_loss: 0.6888\n","Epoch 25/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 76ms/step - accuracy: 0.5735 - loss: 0.6770 - val_accuracy: 0.5418 - val_loss: 0.6897\n","Epoch 26/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 74ms/step - accuracy: 0.5604 - loss: 0.6788 - val_accuracy: 0.5364 - val_loss: 0.6935\n","Epoch 27/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 77ms/step - accuracy: 0.5700 - loss: 0.6763 - val_accuracy: 0.5393 - val_loss: 0.6928\n","Epoch 28/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 75ms/step - accuracy: 0.5791 - loss: 0.6730 - val_accuracy: 0.5429 - val_loss: 0.6869\n","Epoch 29/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.5821 - loss: 0.6703 - val_accuracy: 0.5447 - val_loss: 0.6908\n","Epoch 30/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 74ms/step - accuracy: 0.5773 - loss: 0.6710 - val_accuracy: 0.5504 - val_loss: 0.6871\n","Epoch 31/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 74ms/step - accuracy: 0.5769 - loss: 0.6732 - val_accuracy: 0.5411 - val_loss: 0.6954\n","Epoch 32/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 78ms/step - accuracy: 0.5860 - loss: 0.6692 - val_accuracy: 0.5522 - val_loss: 0.6870\n","Epoch 33/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 83ms/step - accuracy: 0.5983 - loss: 0.6618 - val_accuracy: 0.5461 - val_loss: 0.6864\n","Epoch 34/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 78ms/step - accuracy: 0.5980 - loss: 0.6643 - val_accuracy: 0.5590 - val_loss: 0.6861\n","Epoch 35/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 75ms/step - accuracy: 0.6072 - loss: 0.6585 - val_accuracy: 0.5597 - val_loss: 0.6877\n","Epoch 36/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 78ms/step - accuracy: 0.6063 - loss: 0.6592 - val_accuracy: 0.5623 - val_loss: 0.6939\n","Epoch 37/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 74ms/step - accuracy: 0.6253 - loss: 0.6447 - val_accuracy: 0.5762 - val_loss: 0.6817\n","Epoch 38/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 78ms/step - accuracy: 0.6191 - loss: 0.6478 - val_accuracy: 0.5723 - val_loss: 0.6883\n","Epoch 39/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 76ms/step - accuracy: 0.6321 - loss: 0.6383 - val_accuracy: 0.5727 - val_loss: 0.6901\n","Epoch 40/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.6564 - loss: 0.6248 - val_accuracy: 0.5773 - val_loss: 0.6889\n","Epoch 41/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.6520 - loss: 0.6188 - val_accuracy: 0.5784 - val_loss: 0.6884\n","Epoch 42/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.6679 - loss: 0.6117 - val_accuracy: 0.5967 - val_loss: 0.6777\n","Epoch 43/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 74ms/step - accuracy: 0.6756 - loss: 0.6024 - val_accuracy: 0.5892 - val_loss: 0.6873\n","Epoch 44/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 74ms/step - accuracy: 0.6844 - loss: 0.5933 - val_accuracy: 0.6010 - val_loss: 0.6901\n","Epoch 45/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 77ms/step - accuracy: 0.6905 - loss: 0.5903 - val_accuracy: 0.6093 - val_loss: 0.6901\n","Epoch 46/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 75ms/step - accuracy: 0.6992 - loss: 0.5716 - val_accuracy: 0.6143 - val_loss: 0.6761\n","Epoch 47/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 75ms/step - accuracy: 0.7068 - loss: 0.5642 - val_accuracy: 0.6075 - val_loss: 0.6864\n","Epoch 48/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 78ms/step - accuracy: 0.7081 - loss: 0.5613 - val_accuracy: 0.6164 - val_loss: 0.7022\n","Epoch 49/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 75ms/step - accuracy: 0.7240 - loss: 0.5388 - val_accuracy: 0.6157 - val_loss: 0.6980\n","Epoch 50/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - accuracy: 0.7398 - loss: 0.5256 - val_accuracy: 0.6168 - val_loss: 0.6975\n","\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6174 - loss: 0.6928\n","Test Accuracy: 61.68%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Conv1D, Flatten, Dropout, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","# ✅ Load dataset\n","df = pd.read_csv(\"/content/fall_detection_dataset_10000.csv\")\n","\n","# ✅ Ensure correct feature selection\n","features = [\"acc_x\", \"acc_y\", \"acc_z\", \"emg_signal\", \"heart_rate_ecg\", \"spo2\"]\n","target = \"fall_detected\"  # 1 = Fall, 0 = No Fall, 2 = False Fall\n","\n","# Check if all required features exist\n","missing_features = [col for col in features if col not in df.columns]\n","if missing_features:\n","    raise ValueError(f\"Missing features: {missing_features}\")\n","\n","# ✅ Split features and labels\n","X = df[features].values\n","y = df[target].values\n","\n","# ✅ Normalize data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Handle class imbalance using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# ✅ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# ✅ Reshape data for CNN/LSTM\n","X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n","X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n","\n","# ✅ Define Model: CNN + LSTM for Temporal & Spatial Features\n","model = Sequential([\n","    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    LSTM(128, return_sequences=True),\n","    Dropout(0.4),\n","\n","    LSTM(64),\n","    Dropout(0.3),\n","\n","    Dense(32, activation='relu'),\n","    Dense(3, activation='softmax')  # 3 classes: Fall, No Fall, False Fall\n","])\n","\n","# ✅ Compile Model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","              loss=\"sparse_categorical_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","# ✅ Train Model\n","history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n","\n","# ✅ Evaluate Model\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"✅ Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g8u7OeLIxkYv","executionInfo":{"status":"ok","timestamp":1742042133601,"user_tz":-330,"elapsed":355218,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"}},"outputId":"67812ed5-6ef2-4f8f-dff5-60a7cd70a50e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.5029 - loss: 0.7810 - val_accuracy: 0.5091 - val_loss: 0.6954\n","Epoch 2/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.5117 - loss: 0.6977 - val_accuracy: 0.5221 - val_loss: 0.6922\n","Epoch 3/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.5227 - loss: 0.6937 - val_accuracy: 0.5077 - val_loss: 0.6921\n","Epoch 4/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.5128 - loss: 0.6940 - val_accuracy: 0.5142 - val_loss: 0.6929\n","Epoch 5/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5205 - loss: 0.6923 - val_accuracy: 0.5389 - val_loss: 0.6906\n","Epoch 6/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.5243 - loss: 0.6918 - val_accuracy: 0.5149 - val_loss: 0.6933\n","Epoch 7/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.5275 - loss: 0.6916 - val_accuracy: 0.5307 - val_loss: 0.6904\n","Epoch 8/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.5408 - loss: 0.6873 - val_accuracy: 0.5393 - val_loss: 0.6910\n","Epoch 9/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.5492 - loss: 0.6868 - val_accuracy: 0.5235 - val_loss: 0.6882\n","Epoch 10/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.5453 - loss: 0.6861 - val_accuracy: 0.5511 - val_loss: 0.6896\n","Epoch 11/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5577 - loss: 0.6816 - val_accuracy: 0.5483 - val_loss: 0.6877\n","Epoch 12/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5799 - loss: 0.6741 - val_accuracy: 0.5257 - val_loss: 0.6975\n","Epoch 13/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5873 - loss: 0.6714 - val_accuracy: 0.5540 - val_loss: 0.6880\n","Epoch 14/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.5833 - loss: 0.6703 - val_accuracy: 0.5608 - val_loss: 0.6894\n","Epoch 15/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.5979 - loss: 0.6628 - val_accuracy: 0.5694 - val_loss: 0.6838\n","Epoch 16/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6117 - loss: 0.6576 - val_accuracy: 0.5648 - val_loss: 0.6877\n","Epoch 17/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.6043 - loss: 0.6582 - val_accuracy: 0.5745 - val_loss: 0.6820\n","Epoch 18/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.6163 - loss: 0.6506 - val_accuracy: 0.5684 - val_loss: 0.6800\n","Epoch 19/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.6151 - loss: 0.6508 - val_accuracy: 0.5788 - val_loss: 0.6777\n","Epoch 20/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.6280 - loss: 0.6401 - val_accuracy: 0.5806 - val_loss: 0.6803\n","Epoch 21/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.6452 - loss: 0.6299 - val_accuracy: 0.5841 - val_loss: 0.6788\n","Epoch 22/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.6363 - loss: 0.6314 - val_accuracy: 0.5938 - val_loss: 0.6711\n","Epoch 23/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.6494 - loss: 0.6211 - val_accuracy: 0.6107 - val_loss: 0.6675\n","Epoch 24/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.6465 - loss: 0.6176 - val_accuracy: 0.6017 - val_loss: 0.6685\n","Epoch 25/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.6551 - loss: 0.6149 - val_accuracy: 0.6014 - val_loss: 0.6741\n","Epoch 26/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.6688 - loss: 0.5968 - val_accuracy: 0.6021 - val_loss: 0.6713\n","Epoch 27/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - accuracy: 0.6734 - loss: 0.6036 - val_accuracy: 0.5981 - val_loss: 0.6715\n","Epoch 28/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.6734 - loss: 0.5971 - val_accuracy: 0.6017 - val_loss: 0.6738\n","Epoch 29/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.6873 - loss: 0.5852 - val_accuracy: 0.6125 - val_loss: 0.6651\n","Epoch 30/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6831 - loss: 0.5834 - val_accuracy: 0.6050 - val_loss: 0.6741\n","Epoch 31/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.6891 - loss: 0.5799 - val_accuracy: 0.6175 - val_loss: 0.6659\n","Epoch 32/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.6974 - loss: 0.5711 - val_accuracy: 0.6279 - val_loss: 0.6642\n","Epoch 33/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.6901 - loss: 0.5719 - val_accuracy: 0.6276 - val_loss: 0.6670\n","Epoch 34/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.7009 - loss: 0.5609 - val_accuracy: 0.6207 - val_loss: 0.6682\n","Epoch 35/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.6958 - loss: 0.5697 - val_accuracy: 0.6340 - val_loss: 0.6646\n","Epoch 36/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.6986 - loss: 0.5615 - val_accuracy: 0.6416 - val_loss: 0.6632\n","Epoch 37/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.7121 - loss: 0.5523 - val_accuracy: 0.6430 - val_loss: 0.6571\n","Epoch 38/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.7135 - loss: 0.5465 - val_accuracy: 0.6430 - val_loss: 0.6572\n","Epoch 39/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.7220 - loss: 0.5451 - val_accuracy: 0.6426 - val_loss: 0.6617\n","Epoch 40/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.7213 - loss: 0.5402 - val_accuracy: 0.6376 - val_loss: 0.6680\n","Epoch 41/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.7214 - loss: 0.5376 - val_accuracy: 0.6491 - val_loss: 0.6615\n","Epoch 42/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.7204 - loss: 0.5411 - val_accuracy: 0.6473 - val_loss: 0.6614\n","Epoch 43/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7341 - loss: 0.5295 - val_accuracy: 0.6563 - val_loss: 0.6650\n","Epoch 44/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.7431 - loss: 0.5192 - val_accuracy: 0.6509 - val_loss: 0.6646\n","Epoch 45/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.7309 - loss: 0.5218 - val_accuracy: 0.6548 - val_loss: 0.6478\n","Epoch 46/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.7313 - loss: 0.5280 - val_accuracy: 0.6606 - val_loss: 0.6561\n","Epoch 47/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.7478 - loss: 0.5077 - val_accuracy: 0.6642 - val_loss: 0.6521\n","Epoch 48/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.7491 - loss: 0.5045 - val_accuracy: 0.6627 - val_loss: 0.6531\n","Epoch 49/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.7413 - loss: 0.5105 - val_accuracy: 0.6631 - val_loss: 0.6627\n","Epoch 50/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7499 - loss: 0.4984 - val_accuracy: 0.6591 - val_loss: 0.6618\n","\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6631 - loss: 0.6595\n","✅ Final Test Accuracy: 65.91%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Conv1D, Flatten, Dropout, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","# ✅ Load dataset\n","df = pd.read_csv(\"/content/fall_detection_dataset_10000.csv\")\n","\n","# ✅ Ensure correct feature selection\n","features = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\",\"gyro_y\",\"gyro_z\",\"emg_signal\", \"heart_rate_ecg\", \"spo2\",\"ecg_variability\"]\n","target = \"fall_detected\"  # 1 = Fall, 0 = No Fall, 2 = False Fall\n","\n","# Check if all required features exist\n","missing_features = [col for col in features if col not in df.columns]\n","if missing_features:\n","    raise ValueError(f\"Missing features: {missing_features}\")\n","\n","# ✅ Split features and labels\n","X = df[features].values\n","y = df[target].values\n","\n","# ✅ Normalize data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Handle class imbalance using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# ✅ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# ✅ Reshape data for CNN/LSTM\n","X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n","X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n","\n","# ✅ Define Model: CNN + LSTM for Temporal & Spatial Features\n","model = Sequential([\n","    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    LSTM(128, return_sequences=True),\n","    Dropout(0.4),\n","\n","    LSTM(64),\n","    Dropout(0.3),\n","\n","    Dense(32, activation='relu'),\n","    Dense(3, activation='softmax')  # 3 classes: Fall, No Fall, False Fall\n","])\n","\n","# ✅ Compile Model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","              loss=\"sparse_categorical_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","# ✅ Train Model\n","history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n","\n","# ✅ Evaluate Model\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"✅ Final Test Accuracy: {test_accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"476OAxIlzq7d","executionInfo":{"status":"ok","timestamp":1742042835581,"user_tz":-330,"elapsed":518894,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"}},"outputId":"02ad7072-ef28-4bcb-c6d9-f1f0aa519a7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 28ms/step - accuracy: 0.5087 - loss: 0.7809 - val_accuracy: 0.5045 - val_loss: 0.6964\n","Epoch 2/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.5193 - loss: 0.6965 - val_accuracy: 0.5242 - val_loss: 0.6903\n","Epoch 3/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.5205 - loss: 0.6936 - val_accuracy: 0.5260 - val_loss: 0.6902\n","Epoch 4/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 60ms/step - accuracy: 0.5244 - loss: 0.6925 - val_accuracy: 0.5278 - val_loss: 0.6962\n","Epoch 5/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 28ms/step - accuracy: 0.5530 - loss: 0.6880 - val_accuracy: 0.5414 - val_loss: 0.6901\n","Epoch 6/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.5444 - loss: 0.6881 - val_accuracy: 0.5274 - val_loss: 0.6896\n","Epoch 7/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.5333 - loss: 0.6893 - val_accuracy: 0.5501 - val_loss: 0.6848\n","Epoch 8/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.5539 - loss: 0.6848 - val_accuracy: 0.5544 - val_loss: 0.6827\n","Epoch 9/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.5635 - loss: 0.6825 - val_accuracy: 0.5526 - val_loss: 0.6842\n","Epoch 10/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.5625 - loss: 0.6811 - val_accuracy: 0.5572 - val_loss: 0.6886\n","Epoch 11/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.5737 - loss: 0.6744 - val_accuracy: 0.5784 - val_loss: 0.6795\n","Epoch 12/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.6023 - loss: 0.6673 - val_accuracy: 0.5809 - val_loss: 0.6750\n","Epoch 13/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.5987 - loss: 0.6604 - val_accuracy: 0.5892 - val_loss: 0.6694\n","Epoch 14/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.6278 - loss: 0.6436 - val_accuracy: 0.5935 - val_loss: 0.6638\n","Epoch 15/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.6346 - loss: 0.6399 - val_accuracy: 0.5924 - val_loss: 0.6664\n","Epoch 16/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.6270 - loss: 0.6370 - val_accuracy: 0.6046 - val_loss: 0.6630\n","Epoch 17/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.6583 - loss: 0.6169 - val_accuracy: 0.6078 - val_loss: 0.6608\n","Epoch 18/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.6678 - loss: 0.6078 - val_accuracy: 0.6107 - val_loss: 0.6627\n","Epoch 19/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.6941 - loss: 0.5879 - val_accuracy: 0.6125 - val_loss: 0.6549\n","Epoch 20/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.6883 - loss: 0.5877 - val_accuracy: 0.6315 - val_loss: 0.6564\n","Epoch 21/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.7074 - loss: 0.5641 - val_accuracy: 0.6225 - val_loss: 0.6509\n","Epoch 22/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.7191 - loss: 0.5560 - val_accuracy: 0.6355 - val_loss: 0.6542\n","Epoch 23/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.7276 - loss: 0.5432 - val_accuracy: 0.6365 - val_loss: 0.6520\n","Epoch 24/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - accuracy: 0.7382 - loss: 0.5243 - val_accuracy: 0.6509 - val_loss: 0.6531\n","Epoch 25/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.7580 - loss: 0.4994 - val_accuracy: 0.6534 - val_loss: 0.6445\n","Epoch 26/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.7582 - loss: 0.5075 - val_accuracy: 0.6487 - val_loss: 0.6705\n","Epoch 27/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.7531 - loss: 0.5006 - val_accuracy: 0.6652 - val_loss: 0.6534\n","Epoch 28/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.7702 - loss: 0.4758 - val_accuracy: 0.6645 - val_loss: 0.6433\n","Epoch 29/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.7831 - loss: 0.4638 - val_accuracy: 0.6688 - val_loss: 0.6504\n","Epoch 30/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.7874 - loss: 0.4534 - val_accuracy: 0.6724 - val_loss: 0.6582\n","Epoch 31/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.7899 - loss: 0.4518 - val_accuracy: 0.6642 - val_loss: 0.6696\n","Epoch 32/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.8003 - loss: 0.4247 - val_accuracy: 0.6771 - val_loss: 0.6547\n","Epoch 33/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.8074 - loss: 0.4200 - val_accuracy: 0.6825 - val_loss: 0.6291\n","Epoch 34/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.8180 - loss: 0.4012 - val_accuracy: 0.6943 - val_loss: 0.6406\n","Epoch 35/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.8256 - loss: 0.3921 - val_accuracy: 0.6850 - val_loss: 0.6583\n","Epoch 36/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.8252 - loss: 0.3824 - val_accuracy: 0.6936 - val_loss: 0.6572\n","Epoch 37/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8376 - loss: 0.3742 - val_accuracy: 0.6929 - val_loss: 0.6707\n","Epoch 38/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.8358 - loss: 0.3706 - val_accuracy: 0.6853 - val_loss: 0.6762\n","Epoch 39/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.8369 - loss: 0.3673 - val_accuracy: 0.6982 - val_loss: 0.6492\n","Epoch 40/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.8465 - loss: 0.3553 - val_accuracy: 0.7079 - val_loss: 0.6675\n","Epoch 41/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.8514 - loss: 0.3462 - val_accuracy: 0.7036 - val_loss: 0.6683\n","Epoch 42/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.8669 - loss: 0.3228 - val_accuracy: 0.6986 - val_loss: 0.6982\n","Epoch 43/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.8623 - loss: 0.3213 - val_accuracy: 0.7029 - val_loss: 0.6901\n","Epoch 44/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.8632 - loss: 0.3197 - val_accuracy: 0.7043 - val_loss: 0.6937\n","Epoch 45/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.8708 - loss: 0.3073 - val_accuracy: 0.7230 - val_loss: 0.6689\n","Epoch 46/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.8651 - loss: 0.3135 - val_accuracy: 0.7072 - val_loss: 0.6863\n","Epoch 47/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.8720 - loss: 0.3059 - val_accuracy: 0.7090 - val_loss: 0.6827\n","Epoch 48/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.8750 - loss: 0.2912 - val_accuracy: 0.7094 - val_loss: 0.6884\n","Epoch 49/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.8842 - loss: 0.2715 - val_accuracy: 0.7169 - val_loss: 0.6778\n","Epoch 50/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.8873 - loss: 0.2715 - val_accuracy: 0.7234 - val_loss: 0.6952\n","\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7344 - loss: 0.6807\n","✅ Final Test Accuracy: 72.34%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Conv1D, Flatten, Dropout, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","# ✅ Load dataset\n","df = pd.read_csv(\"/content/fall_detection_dataset_10000.csv\")\n","\n","# ✅ Ensure correct feature selection\n","features = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\",\"gyro_y\",\"gyro_z\",\"emg_signal\", \"heart_rate_ecg\", \"spo2\",\"ecg_variability\",\"heart_rate_spo2\"]\n","target = \"fall_detected\"  # 1 = Fall, 0 = No Fall, 2 = False Fall\n","\n","# Check if all required features exist\n","missing_features = [col for col in features if col not in df.columns]\n","if missing_features:\n","    raise ValueError(f\"Missing features: {missing_features}\")\n","\n","# ✅ Split features and labels\n","X = df[features].values\n","y = df[target].values\n","\n","# ✅ Normalize data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Handle class imbalance using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# ✅ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# ✅ Reshape data for CNN/LSTM\n","X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n","X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n","\n","# ✅ Define Model: CNN + LSTM for Temporal & Spatial Features\n","model = Sequential([\n","    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    LSTM(128, return_sequences=True),\n","    Dropout(0.4),\n","\n","    LSTM(64),\n","    Dropout(0.3),\n","\n","    Dense(32, activation='relu'),\n","    Dense(3, activation='softmax')  # 3 classes: Fall, No Fall, False Fall\n","])\n","\n","# ✅ Compile Model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","              loss=\"sparse_categorical_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","# ✅ Train Model\n","history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n","\n","# ✅ Evaluate Model\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"✅ Final Test Accuracy: {test_accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_6cCTU22LQ5","executionInfo":{"status":"ok","timestamp":1742043623118,"user_tz":-330,"elapsed":671555,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"}},"outputId":"538242b5-9e74-47bf-f5e9-af7892aa2d5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.5001 - loss: 0.7806 - val_accuracy: 0.5303 - val_loss: 0.6932\n","Epoch 2/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5241 - loss: 0.6952 - val_accuracy: 0.5289 - val_loss: 0.6919\n","Epoch 3/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.5314 - loss: 0.6918 - val_accuracy: 0.5307 - val_loss: 0.6888\n","Epoch 4/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.5417 - loss: 0.6875 - val_accuracy: 0.5396 - val_loss: 0.6873\n","Epoch 5/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.5462 - loss: 0.6864 - val_accuracy: 0.5346 - val_loss: 0.6904\n","Epoch 6/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.5506 - loss: 0.6852 - val_accuracy: 0.5411 - val_loss: 0.6874\n","Epoch 7/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.5669 - loss: 0.6833 - val_accuracy: 0.5536 - val_loss: 0.6858\n","Epoch 8/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.5717 - loss: 0.6776 - val_accuracy: 0.5583 - val_loss: 0.6824\n","Epoch 9/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.5854 - loss: 0.6726 - val_accuracy: 0.5644 - val_loss: 0.6824\n","Epoch 10/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.5919 - loss: 0.6669 - val_accuracy: 0.5719 - val_loss: 0.6784\n","Epoch 11/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.6016 - loss: 0.6628 - val_accuracy: 0.5806 - val_loss: 0.6761\n","Epoch 12/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.6147 - loss: 0.6569 - val_accuracy: 0.5899 - val_loss: 0.6712\n","Epoch 13/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.6228 - loss: 0.6460 - val_accuracy: 0.5945 - val_loss: 0.6690\n","Epoch 14/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.6416 - loss: 0.6376 - val_accuracy: 0.6060 - val_loss: 0.6625\n","Epoch 15/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.6473 - loss: 0.6269 - val_accuracy: 0.6103 - val_loss: 0.6605\n","Epoch 16/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.6645 - loss: 0.6161 - val_accuracy: 0.6172 - val_loss: 0.6615\n","Epoch 17/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.6749 - loss: 0.5980 - val_accuracy: 0.6236 - val_loss: 0.6535\n","Epoch 18/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.6888 - loss: 0.5908 - val_accuracy: 0.6197 - val_loss: 0.6630\n","Epoch 19/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.7080 - loss: 0.5649 - val_accuracy: 0.6265 - val_loss: 0.6660\n","Epoch 20/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.7075 - loss: 0.5668 - val_accuracy: 0.6401 - val_loss: 0.6547\n","Epoch 21/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.7201 - loss: 0.5438 - val_accuracy: 0.6369 - val_loss: 0.6634\n","Epoch 22/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.7321 - loss: 0.5335 - val_accuracy: 0.6609 - val_loss: 0.6400\n","Epoch 23/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.7358 - loss: 0.5184 - val_accuracy: 0.6613 - val_loss: 0.6378\n","Epoch 24/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.7445 - loss: 0.5167 - val_accuracy: 0.6588 - val_loss: 0.6475\n","Epoch 25/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.7579 - loss: 0.4977 - val_accuracy: 0.6534 - val_loss: 0.6733\n","Epoch 26/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.7720 - loss: 0.4812 - val_accuracy: 0.6541 - val_loss: 0.6704\n","Epoch 27/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.7674 - loss: 0.4697 - val_accuracy: 0.6706 - val_loss: 0.6702\n","Epoch 28/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.7751 - loss: 0.4678 - val_accuracy: 0.6749 - val_loss: 0.6616\n","Epoch 29/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.7881 - loss: 0.4486 - val_accuracy: 0.6703 - val_loss: 0.6653\n","Epoch 30/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.7929 - loss: 0.4364 - val_accuracy: 0.6814 - val_loss: 0.6804\n","Epoch 31/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.8036 - loss: 0.4208 - val_accuracy: 0.6803 - val_loss: 0.6822\n","Epoch 32/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.8155 - loss: 0.4152 - val_accuracy: 0.6749 - val_loss: 0.6575\n","Epoch 33/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.8229 - loss: 0.3913 - val_accuracy: 0.6720 - val_loss: 0.6826\n","Epoch 34/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.8183 - loss: 0.3922 - val_accuracy: 0.6903 - val_loss: 0.6717\n","Epoch 35/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.8367 - loss: 0.3661 - val_accuracy: 0.6875 - val_loss: 0.6813\n","Epoch 36/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.8415 - loss: 0.3576 - val_accuracy: 0.6886 - val_loss: 0.6932\n","Epoch 37/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.8362 - loss: 0.3564 - val_accuracy: 0.6900 - val_loss: 0.7106\n","Epoch 38/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.8368 - loss: 0.3546 - val_accuracy: 0.6918 - val_loss: 0.6863\n","Epoch 39/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.8421 - loss: 0.3517 - val_accuracy: 0.7008 - val_loss: 0.7252\n","Epoch 40/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.8565 - loss: 0.3249 - val_accuracy: 0.7033 - val_loss: 0.6998\n","Epoch 41/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.8667 - loss: 0.3147 - val_accuracy: 0.7011 - val_loss: 0.7143\n","Epoch 42/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.8682 - loss: 0.3033 - val_accuracy: 0.7051 - val_loss: 0.7154\n","Epoch 43/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.8670 - loss: 0.3059 - val_accuracy: 0.6964 - val_loss: 0.6928\n","Epoch 44/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.8695 - loss: 0.2957 - val_accuracy: 0.7015 - val_loss: 0.7084\n","Epoch 45/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.8722 - loss: 0.2983 - val_accuracy: 0.6993 - val_loss: 0.7570\n","Epoch 46/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.8746 - loss: 0.2957 - val_accuracy: 0.6918 - val_loss: 0.7600\n","Epoch 47/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.8808 - loss: 0.2728 - val_accuracy: 0.7094 - val_loss: 0.7361\n","Epoch 48/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.8826 - loss: 0.2796 - val_accuracy: 0.7183 - val_loss: 0.7437\n","Epoch 49/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.8925 - loss: 0.2636 - val_accuracy: 0.7072 - val_loss: 0.7554\n","Epoch 50/50\n","\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.8891 - loss: 0.2703 - val_accuracy: 0.7076 - val_loss: 0.7783\n","\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6965 - loss: 0.8014\n","✅ Final Test Accuracy: 70.76%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, Flatten, Dropout, BatchNormalization, Bidirectional, Attention, Permute, Multiply\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","# ✅ Load dataset\n","df = pd.read_csv(\"/content/fall_detection_dataset_10000.csv\")\n","\n","# ✅ Ensure correct feature selection\n","features = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\",\"gyro_y\",\"gyro_z\",\"emg_signal\", \"heart_rate_ecg\", \"spo2\",\"ecg_variability\",\"heart_rate_spo2\"]\n","target = \"fall_detected\"  # 1 = Fall, 0 = No Fall, 2 = False Fall\n","\n","# ✅ Check if features exist\n","missing_features = [col for col in features if col not in df.columns]\n","if missing_features:\n","    raise ValueError(f\"Missing features: {missing_features}\")\n","\n","# ✅ Prepare data\n","X = df[features].values\n","y = df[target].values\n","\n","# ✅ Normalize data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Handle class imbalance using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# ✅ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# ✅ Reshape for CNN-LSTM\n","X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n","X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n","\n","# ✅ Define Model: CNN + BiLSTM + Fixed Attention Layer\n","input_layer = Input(shape=(X_train.shape[1], 1))\n","\n","# 🟢 Convolutional Layers\n","x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_layer)\n","x = BatchNormalization()(x)\n","x = Dropout(0.3)(x)\n","\n","# 🔵 BiLSTM for capturing temporal patterns\n","x = Bidirectional(LSTM(128, return_sequences=True))(x)\n","x = Dropout(0.4)(x)\n","\n","# 🟠 Fixed Attention Mechanism\n","attention_scores = Dense(1, activation=\"tanh\")(x)  # Compute attention scores\n","attention_scores = Flatten()(attention_scores)\n","attention_scores = tf.keras.activations.softmax(attention_scores)  # Normalize scores\n","attention_scores = tf.keras.layers.RepeatVector(128)(attention_scores)  # Match LSTM output shape\n","attention_scores = Permute((2, 1))(attention_scores)\n","x = Multiply()([x, attention_scores])  # Apply attention weights\n","x = Flatten()(x)  # Flatten for fully connected layer\n","\n","# 🔴 Fully Connected Layers\n","x = Dense(64, activation='relu')(x)\n","x = Dropout(0.3)(x)\n","output_layer = Dense(3, activation='softmax')(x)  # 3 classes: Fall, No Fall, False Fall\n","\n","# ✅ Compile Model\n","model = Model(inputs=input_layer, outputs=output_layer)\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","              loss=\"sparse_categorical_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","# ✅ Train Model\n","history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))\n","\n","# ✅ Evaluate Model\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"✅ Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"DiKJrqeI6XYZ","executionInfo":{"status":"error","timestamp":1742044096112,"user_tz":-330,"elapsed":636,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"}},"outputId":"c36149a8-3dca-46f0-ea23-8ae1247da764"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Inputs have incompatible shapes. Received shapes (11, 256) and (11, 128)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-5abbd3c85952>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Match LSTM output shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten for fully connected layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/merging/base_merge.py\u001b[0m in \u001b[0;36m_compute_elemwise_op_output_shape\u001b[0;34m(self, shape1, shape2)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     94\u001b[0m                         \u001b[0;34m\"Inputs have incompatible shapes. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                         \u001b[0;34mf\"Received shapes {shape1} and {shape2}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Inputs have incompatible shapes. Received shapes (11, 256) and (11, 128)"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, Flatten, Dropout, BatchNormalization, Bidirectional, Attention, GlobalAveragePooling1D\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","# ✅ Load dataset\n","df = pd.read_csv(\"/content/fall_detection_dataset_10000.csv\")\n","\n","# ✅ Ensure correct feature selection\n","features = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\",\"gyro_y\",\"gyro_z\",\"emg_signal\", \"heart_rate_ecg\", \"spo2\",\"ecg_variability\",\"heart_rate_spo2\"]\n","target = \"fall_detected\"  # 1 = Fall, 0 = No Fall, 2 = False Fall\n","\n","# ✅ Check if features exist\n","missing_features = [col for col in features if col not in df.columns]\n","if missing_features:\n","    raise ValueError(f\"Missing features: {missing_features}\")\n","\n","# ✅ Prepare data\n","X = df[features].values\n","y = df[target].values\n","\n","# ✅ Normalize data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Handle class imbalance using SMOTE\n","smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","# ✅ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# ✅ Reshape for CNN-LSTM\n","X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n","X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n","\n","# ✅ Define Model: CNN + BiLSTM + Fixed Attention\n","input_layer = Input(shape=(X_train.shape[1], 1))\n","\n","# 🟢 Convolutional Layers\n","x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_layer)\n","x = BatchNormalization()(x)\n","x = Dropout(0.3)(x)\n","\n","# 🔵 BiLSTM for capturing temporal patterns\n","x = Bidirectional(LSTM(128, return_sequences=True))(x)\n","x = Dropout(0.4)(x)\n","\n","# 🟠 Fixed Attention Mechanism\n","attention_layer = Attention()([x, x])  # Self-attention\n","x = GlobalAveragePooling1D()(attention_layer)  # Compress features\n","\n","# 🔴 Fully Connected Layers\n","x = Dense(64, activation='relu')(x)\n","x = Dropout(0.3)(x)\n","output_layer = Dense(3, activation='softmax')(x)  # 3 classes: Fall, No Fall, False Fall\n","\n","# ✅ Compile Model\n","model = Model(inputs=input_layer, outputs=output_layer)\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","              loss=\"sparse_categorical_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","# ✅ Train Model\n","history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))\n","\n","# ✅ Evaluate Model\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"✅ Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cR2cEeAR7POi","executionInfo":{"status":"ok","timestamp":1742046068637,"user_tz":-330,"elapsed":1058932,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"}},"outputId":"827369de-aac6-4f00-8e51-b4713be85eb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 74ms/step - accuracy: 0.5062 - loss: 0.8279 - val_accuracy: 0.5181 - val_loss: 0.8243\n","Epoch 2/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 74ms/step - accuracy: 0.5204 - loss: 0.7077 - val_accuracy: 0.5267 - val_loss: 0.7077\n","Epoch 3/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - accuracy: 0.5410 - loss: 0.6942 - val_accuracy: 0.5350 - val_loss: 0.6888\n","Epoch 4/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 68ms/step - accuracy: 0.5432 - loss: 0.6893 - val_accuracy: 0.5389 - val_loss: 0.6880\n","Epoch 5/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.5286 - loss: 0.6902 - val_accuracy: 0.5310 - val_loss: 0.6902\n","Epoch 6/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 69ms/step - accuracy: 0.5498 - loss: 0.6876 - val_accuracy: 0.5414 - val_loss: 0.6880\n","Epoch 7/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.5511 - loss: 0.6869 - val_accuracy: 0.5260 - val_loss: 0.6869\n","Epoch 8/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - accuracy: 0.5469 - loss: 0.6855 - val_accuracy: 0.5357 - val_loss: 0.6889\n","Epoch 9/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.5581 - loss: 0.6827 - val_accuracy: 0.5425 - val_loss: 0.6872\n","Epoch 10/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - accuracy: 0.5483 - loss: 0.6855 - val_accuracy: 0.5375 - val_loss: 0.6887\n","Epoch 11/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 74ms/step - accuracy: 0.5518 - loss: 0.6854 - val_accuracy: 0.5389 - val_loss: 0.6866\n","Epoch 12/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - accuracy: 0.5523 - loss: 0.6828 - val_accuracy: 0.5379 - val_loss: 0.6883\n","Epoch 13/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 72ms/step - accuracy: 0.5605 - loss: 0.6820 - val_accuracy: 0.5465 - val_loss: 0.6866\n","Epoch 14/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 67ms/step - accuracy: 0.5603 - loss: 0.6823 - val_accuracy: 0.5400 - val_loss: 0.6875\n","Epoch 15/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - accuracy: 0.5678 - loss: 0.6798 - val_accuracy: 0.5450 - val_loss: 0.6873\n","Epoch 16/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.5567 - loss: 0.6813 - val_accuracy: 0.5425 - val_loss: 0.6868\n","Epoch 17/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 68ms/step - accuracy: 0.5651 - loss: 0.6788 - val_accuracy: 0.5411 - val_loss: 0.6867\n","Epoch 18/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 70ms/step - accuracy: 0.5700 - loss: 0.6766 - val_accuracy: 0.5461 - val_loss: 0.6870\n","Epoch 19/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 69ms/step - accuracy: 0.5825 - loss: 0.6705 - val_accuracy: 0.5400 - val_loss: 0.6836\n","Epoch 20/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.5798 - loss: 0.6711 - val_accuracy: 0.5540 - val_loss: 0.6885\n","Epoch 21/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 64ms/step - accuracy: 0.5925 - loss: 0.6662 - val_accuracy: 0.5540 - val_loss: 0.6855\n","Epoch 22/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 68ms/step - accuracy: 0.5930 - loss: 0.6678 - val_accuracy: 0.5497 - val_loss: 0.6838\n","Epoch 23/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.5905 - loss: 0.6633 - val_accuracy: 0.5497 - val_loss: 0.6805\n","Epoch 24/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - accuracy: 0.5921 - loss: 0.6630 - val_accuracy: 0.5658 - val_loss: 0.6886\n","Epoch 25/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 66ms/step - accuracy: 0.5929 - loss: 0.6599 - val_accuracy: 0.5605 - val_loss: 0.6853\n","Epoch 26/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 68ms/step - accuracy: 0.6112 - loss: 0.6585 - val_accuracy: 0.5655 - val_loss: 0.6822\n","Epoch 27/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 69ms/step - accuracy: 0.6021 - loss: 0.6566 - val_accuracy: 0.5669 - val_loss: 0.6803\n","Epoch 28/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.6146 - loss: 0.6470 - val_accuracy: 0.5673 - val_loss: 0.6824\n","Epoch 29/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - accuracy: 0.6116 - loss: 0.6491 - val_accuracy: 0.5709 - val_loss: 0.6759\n","Epoch 30/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 74ms/step - accuracy: 0.6379 - loss: 0.6347 - val_accuracy: 0.5838 - val_loss: 0.6758\n","Epoch 31/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.6179 - loss: 0.6467 - val_accuracy: 0.5770 - val_loss: 0.6736\n","Epoch 32/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.6227 - loss: 0.6354 - val_accuracy: 0.5766 - val_loss: 0.6769\n","Epoch 33/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - accuracy: 0.6442 - loss: 0.6301 - val_accuracy: 0.5841 - val_loss: 0.6760\n","Epoch 34/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 71ms/step - accuracy: 0.6366 - loss: 0.6294 - val_accuracy: 0.5816 - val_loss: 0.6827\n","Epoch 35/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - accuracy: 0.6438 - loss: 0.6208 - val_accuracy: 0.5917 - val_loss: 0.6764\n","Epoch 36/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 72ms/step - accuracy: 0.6446 - loss: 0.6207 - val_accuracy: 0.5949 - val_loss: 0.6703\n","Epoch 37/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.6574 - loss: 0.6136 - val_accuracy: 0.6032 - val_loss: 0.6710\n","Epoch 38/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 67ms/step - accuracy: 0.6583 - loss: 0.6098 - val_accuracy: 0.6021 - val_loss: 0.6668\n","Epoch 39/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.6679 - loss: 0.6073 - val_accuracy: 0.6028 - val_loss: 0.6718\n","Epoch 40/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 72ms/step - accuracy: 0.6720 - loss: 0.5968 - val_accuracy: 0.6150 - val_loss: 0.6720\n","Epoch 41/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 72ms/step - accuracy: 0.6912 - loss: 0.5808 - val_accuracy: 0.6125 - val_loss: 0.6598\n","Epoch 42/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.6753 - loss: 0.5837 - val_accuracy: 0.6182 - val_loss: 0.6657\n","Epoch 43/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.6891 - loss: 0.5803 - val_accuracy: 0.6326 - val_loss: 0.6643\n","Epoch 44/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.6979 - loss: 0.5679 - val_accuracy: 0.6308 - val_loss: 0.6542\n","Epoch 45/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.6957 - loss: 0.5629 - val_accuracy: 0.6283 - val_loss: 0.6510\n","Epoch 46/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.7053 - loss: 0.5598 - val_accuracy: 0.6398 - val_loss: 0.6787\n","Epoch 47/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.7135 - loss: 0.5501 - val_accuracy: 0.6279 - val_loss: 0.6623\n","Epoch 48/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 66ms/step - accuracy: 0.7192 - loss: 0.5379 - val_accuracy: 0.6383 - val_loss: 0.6574\n","Epoch 49/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 68ms/step - accuracy: 0.7197 - loss: 0.5377 - val_accuracy: 0.6376 - val_loss: 0.6594\n","Epoch 50/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 70ms/step - accuracy: 0.7159 - loss: 0.5407 - val_accuracy: 0.6512 - val_loss: 0.6528\n","Epoch 51/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 69ms/step - accuracy: 0.7261 - loss: 0.5230 - val_accuracy: 0.6419 - val_loss: 0.6607\n","Epoch 52/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 73ms/step - accuracy: 0.7336 - loss: 0.5159 - val_accuracy: 0.6581 - val_loss: 0.6518\n","Epoch 53/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.7340 - loss: 0.5102 - val_accuracy: 0.6408 - val_loss: 0.6799\n","Epoch 54/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 68ms/step - accuracy: 0.7530 - loss: 0.4966 - val_accuracy: 0.6448 - val_loss: 0.6654\n","Epoch 55/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.7623 - loss: 0.4849 - val_accuracy: 0.6523 - val_loss: 0.6752\n","Epoch 56/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.7579 - loss: 0.4871 - val_accuracy: 0.6581 - val_loss: 0.6511\n","Epoch 57/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - accuracy: 0.7634 - loss: 0.4803 - val_accuracy: 0.6649 - val_loss: 0.6686\n","Epoch 58/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - accuracy: 0.7717 - loss: 0.4659 - val_accuracy: 0.6652 - val_loss: 0.6687\n","Epoch 59/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - accuracy: 0.7701 - loss: 0.4621 - val_accuracy: 0.6699 - val_loss: 0.6780\n","Epoch 60/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - accuracy: 0.7842 - loss: 0.4417 - val_accuracy: 0.6581 - val_loss: 0.7100\n","Epoch 61/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 67ms/step - accuracy: 0.7773 - loss: 0.4542 - val_accuracy: 0.6688 - val_loss: 0.6672\n","Epoch 62/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.7905 - loss: 0.4403 - val_accuracy: 0.6656 - val_loss: 0.7005\n","Epoch 63/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - accuracy: 0.7937 - loss: 0.4290 - val_accuracy: 0.6785 - val_loss: 0.6771\n","Epoch 64/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.7975 - loss: 0.4285 - val_accuracy: 0.6817 - val_loss: 0.6768\n","Epoch 65/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.8032 - loss: 0.4155 - val_accuracy: 0.6792 - val_loss: 0.6667\n","Epoch 66/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.8047 - loss: 0.4054 - val_accuracy: 0.6764 - val_loss: 0.7158\n","Epoch 67/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.8168 - loss: 0.4011 - val_accuracy: 0.6792 - val_loss: 0.6855\n","Epoch 68/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - accuracy: 0.8144 - loss: 0.3991 - val_accuracy: 0.6864 - val_loss: 0.6906\n","Epoch 69/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - accuracy: 0.8215 - loss: 0.3910 - val_accuracy: 0.6817 - val_loss: 0.7097\n","Epoch 70/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.8257 - loss: 0.3822 - val_accuracy: 0.6900 - val_loss: 0.6864\n","Epoch 71/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.8197 - loss: 0.3821 - val_accuracy: 0.6929 - val_loss: 0.6960\n","Epoch 72/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.8222 - loss: 0.3754 - val_accuracy: 0.6975 - val_loss: 0.6987\n","Epoch 73/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 65ms/step - accuracy: 0.8374 - loss: 0.3636 - val_accuracy: 0.6975 - val_loss: 0.7014\n","Epoch 74/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.8404 - loss: 0.3512 - val_accuracy: 0.6825 - val_loss: 0.7390\n","Epoch 75/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 69ms/step - accuracy: 0.8440 - loss: 0.3484 - val_accuracy: 0.6932 - val_loss: 0.7269\n","Epoch 76/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.8504 - loss: 0.3343 - val_accuracy: 0.7022 - val_loss: 0.7318\n","Epoch 77/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.8525 - loss: 0.3358 - val_accuracy: 0.7090 - val_loss: 0.7284\n","Epoch 78/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 72ms/step - accuracy: 0.8543 - loss: 0.3343 - val_accuracy: 0.7033 - val_loss: 0.7176\n","Epoch 79/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 69ms/step - accuracy: 0.8572 - loss: 0.3251 - val_accuracy: 0.7033 - val_loss: 0.7335\n","Epoch 80/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.8531 - loss: 0.3401 - val_accuracy: 0.7040 - val_loss: 0.7449\n","Epoch 81/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.8579 - loss: 0.3167 - val_accuracy: 0.7144 - val_loss: 0.7339\n","Epoch 82/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 72ms/step - accuracy: 0.8602 - loss: 0.3098 - val_accuracy: 0.7069 - val_loss: 0.7502\n","Epoch 83/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 69ms/step - accuracy: 0.8711 - loss: 0.3042 - val_accuracy: 0.7069 - val_loss: 0.7418\n","Epoch 84/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.8677 - loss: 0.2971 - val_accuracy: 0.6932 - val_loss: 0.7694\n","Epoch 85/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.8759 - loss: 0.2892 - val_accuracy: 0.7069 - val_loss: 0.8212\n","Epoch 86/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - accuracy: 0.8832 - loss: 0.2804 - val_accuracy: 0.7244 - val_loss: 0.7539\n","Epoch 87/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 64ms/step - accuracy: 0.8815 - loss: 0.2761 - val_accuracy: 0.7140 - val_loss: 0.7649\n","Epoch 88/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 71ms/step - accuracy: 0.8822 - loss: 0.2796 - val_accuracy: 0.7018 - val_loss: 0.8162\n","Epoch 89/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 69ms/step - accuracy: 0.8855 - loss: 0.2590 - val_accuracy: 0.7058 - val_loss: 0.7926\n","Epoch 90/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - accuracy: 0.8879 - loss: 0.2645 - val_accuracy: 0.7033 - val_loss: 0.8128\n","Epoch 91/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.8847 - loss: 0.2700 - val_accuracy: 0.7176 - val_loss: 0.7561\n","Epoch 92/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.8881 - loss: 0.2684 - val_accuracy: 0.7187 - val_loss: 0.8031\n","Epoch 93/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 66ms/step - accuracy: 0.8974 - loss: 0.2474 - val_accuracy: 0.7162 - val_loss: 0.8210\n","Epoch 94/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.8895 - loss: 0.2603 - val_accuracy: 0.7176 - val_loss: 0.8226\n","Epoch 95/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - accuracy: 0.9023 - loss: 0.2433 - val_accuracy: 0.7162 - val_loss: 0.8310\n","Epoch 96/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9043 - loss: 0.2431 - val_accuracy: 0.7205 - val_loss: 0.8528\n","Epoch 97/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 74ms/step - accuracy: 0.8993 - loss: 0.2370 - val_accuracy: 0.7198 - val_loss: 0.8767\n","Epoch 98/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.9031 - loss: 0.2371 - val_accuracy: 0.7248 - val_loss: 0.8290\n","Epoch 99/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9063 - loss: 0.2395 - val_accuracy: 0.7151 - val_loss: 0.8336\n","Epoch 100/100\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9079 - loss: 0.2252 - val_accuracy: 0.7180 - val_loss: 0.8725\n","\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7146 - loss: 0.8949\n","✅ Final Test Accuracy: 71.80%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# ✅ Load dataset\n","file_path = \"fall_detection_dataset_10000.csv\"  # Update path if needed\n","df = pd.read_csv(file_path)\n","\n","# ✅ Check for missing values\n","df.dropna(inplace=True)\n","\n","# ✅ Split features and labels\n","X = df.drop(columns=[\"fall_type\"]).values  # All sensor readings\n","y = df[\"fall_type\"].values  # 0 = False Fall, 1 = Real Fall\n","\n","# ✅ Normalize Data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# ✅ Reshape for LSTM (samples, timesteps, features)\n","X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n","X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n","\n","# ✅ Build CNN + LSTM Model\n","model = Sequential([\n","    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n","    MaxPooling1D(pool_size=2),\n","    BatchNormalization(),\n","\n","    LSTM(128, return_sequences=True),\n","    Dropout(0.3),\n","\n","    LSTM(64, return_sequences=False),\n","    Dropout(0.3),\n","\n","    Dense(32, activation='relu'),\n","    Dropout(0.2),\n","\n","    Dense(1, activation='sigmoid')  # Binary Classification (Fall vs False Fall)\n","])\n","\n","# ✅ Compile Model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# ✅ Train Model\n","history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n","\n","# ✅ Evaluate Model\n","test_loss, test_acc = model.evaluate(X_test, y_test)\n","print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n","\n","# ✅ Save Model\n","model.save(\"fall_detection_model.h5\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"id":"b-Chv8bbq8pg","executionInfo":{"status":"error","timestamp":1742107610178,"user_tz":-330,"elapsed":75,"user":{"displayName":"kkt Boss","userId":"15973758270427207434"}},"outputId":"c8575d9e-bc98-4fcb-96fd-4f4601ae72f7"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"\"['fall_type'] not found in axis\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e918bd52cc3a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# ✅ Split features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fall_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m  \u001b[0;31m# All sensor readings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fall_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m  \u001b[0;31m# 0 = False Fall, 1 = Real Fall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['fall_type'] not found in axis\""]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}